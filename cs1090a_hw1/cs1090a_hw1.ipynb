{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6eb141",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs1090a_hw1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f31b6-e021-4842-b0ac-5fca2c0b4e92",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS1090A Introduction to Data Science\n",
    "## Homework 1: Webscraping, Pandas, and EDA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2025**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Gumb\n",
    "\n",
    "<hr style=\"height:2.4pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6edac1-4b76-49be-8c7b-c67fc4dc959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa # optional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import io\n",
    "import os.path\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import asyncio # optional\n",
    "import aiohttp # optional\n",
    "from IPython.core.display import HTML\n",
    "from IPython import display\n",
    "\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3ec48-8dcd-4da4-b996-e2142edabd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating total notebook runtime\n",
    "notebook_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf897cf-d166-44a9-b2cb-f25dfaf0ed46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h3, h4 {\n",
       "    background-color: #7efcf5;\n",
       "    border-left: 5px solid #7ec4fc;\n",
       "    border-right: 5px solid #7ec4fc;\n",
       "    padding: 0em;\n",
       "}\n",
       "h3 {\n",
       "    background-color: #7efcf5;\n",
       "    border-top: 5px solid #7ec4fc;\n",
       "    border-left: 5px solid #7ec4fc;\n",
       "    border-right: 5px solid #7ec4fc;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "p {\n",
       "    padding: 0.5em;\n",
       "    max-width: 34em;\n",
       "    font-weight:400;\n",
       "}\n",
       ".md {\n",
       "    max-width: 80ch;\n",
       "\n",
       "}\n",
       ".prompt {    \n",
       "    background-color: lightgreen;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #f57efc;\n",
       "    padding: 0.5em;\n",
       "    font-weight:500;\n",
       "    }\n",
       " </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style = '''<style>\n",
    "h3, h4 {\n",
    "    background-color: #7efcf5;\n",
    "    border-left: 5px solid #7ec4fc;\n",
    "    border-right: 5px solid #7ec4fc;\n",
    "    padding: 0em;\n",
    "}\n",
    "h3 {\n",
    "    background-color: #7efcf5;\n",
    "    border-top: 5px solid #7ec4fc;\n",
    "    border-left: 5px solid #7ec4fc;\n",
    "    border-right: 5px solid #7ec4fc;\n",
    "    padding: 0.5em;\n",
    "}\n",
    "p {\n",
    "    padding: 0.5em;\n",
    "    max-width: 34em;\n",
    "    font-weight:400;\n",
    "}\n",
    ".md {\n",
    "    max-width: 80ch;\n",
    "\n",
    "}\n",
    ".prompt {    \n",
    "    background-color: lightgreen;\n",
    "    border-color: #dFb5b4;\n",
    "    border-left: 5px solid #f57efc;\n",
    "    padding: 0.5em;\n",
    "    font-weight:500;\n",
    "    }\n",
    " </style>'''\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ef44b-e503-42a2-8dee-44e1d8bbbbf3",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "\n",
    "You'll be working with data from screenboston.com, a website that aggregates movie screenings from various theaters in the Boston area. Your task is to collect this data, enrich it with information from Wikipedia, and then analyze and visualize the results.\n",
    "\n",
    "## Assignment Structure\n",
    "\n",
    "The assignment is divided into 8 main questions, each building upon the previous ones, followed by a wrap-up:\n",
    "\n",
    "1. Fetching HTML from screenboston.com (10 pts)\n",
    "2. Parsing HTML into structured data (15 pts)\n",
    "3. Augmenting data with historical snapshots (15 pts)\n",
    "4. Creating and manipulating a Pandas DataFrame (15 pts)\n",
    "5. Initial exploratory data analysis and visualization (15 pts)\n",
    "6. Finding Wikipedia pages for each screened film (7 pts)\n",
    "7. Storing Wikipedia HTML in the DataFrame (3 pts)\n",
    "8. Extracting and analyzing data from Wikipedia pages (15 pts)\n",
    "9. Wrap-up (5 pts)\n",
    "\n",
    "As you progress through these steps, you'll be transforming raw web data into a rich dataset ready for analysis. By the end, you'll have created a comprehensive overview of Boston's movie screening landscape, potentially uncovering interesting trends and patterns in the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85b37d-d1b3-4575-971c-06c6006578eb",
   "metadata": {},
   "source": [
    "<div style = \"background: lightgreen; border: thin solid black; border-radius: 2px; padding: 5px\">\n",
    "\n",
    "### Instructions\n",
    "- To submit your notebook, follow the instructions given in on the Canvas assignment page.\n",
    "- Plots should be legible and interpretable *without having to refer to the code that generated them*. They should include labels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you believe the plot *means*.\n",
    "- Autograding tests are mostly to help you debug. The tests are not exhaustive so simply passing all tests may not be sufficient for full credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output. Or better yet, avoid overly long output altogether.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Don't forget to include a written response when one is requested by a question prompt.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells (note that this can take a few minutes). \n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06254d-bdbc-44e3-8c98-cdda42b0aab5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<div class=md>\n",
    "<h3>1. Getting HTML from screenboston.com</h3>\n",
    "\n",
    "<p>Our first step is to get the HTML content from the page at screenboston.com for analysis. To avoid having to make unnecessary future requests, you should write the content to disk.</p>\n",
    "\n",
    "\n",
    "\n",
    "<div class=prompt>\n",
    "    \n",
    "1. Use the `requests` library to get the html located at screenboston.com.\n",
    "1. Save it in the file `data/html/screenboston.html`.\n",
    "\n",
    "**Your code should only make an HTTP request if the file does not already exist.** \n",
    "\n",
    "**Hint:** the `os.path.isfile()` function can be used check if a file exists.\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2f172-b4f3-49c7-8544-cdf6ae0e33b9",
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ea31b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2b4b1-3614-4d82-8f58-4e4a49168b31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=md>\n",
    "<h3>2. Parsing HTML into an Efficient Data Structure</h3>\n",
    "\n",
    "<p>Were you to open this HTML file in an editor, you'd find it wouldn't make for light reading.\n",
    "There's a lot of text in there, but we're currently only interested in a small portion of it. Thankfully, this small portion's constituant occurances appear in a mostly predictable way.\n",
    "</p>\n",
    "\n",
    "<div class='prompt'>\n",
    "1. Parse the HTML page to produce an iterable of dictionaries called <code>movies</code>. Here is one possible example:\n",
    "<div style=\"max-width:30em;padding:1em\">\n",
    "    \n",
    "```python\n",
    "{'title': 'Monty Python and the Holy Grail',\n",
    " 'directors': 'Terry Jones, Terry Gilliam',\n",
    " 'year': 1975,\n",
    " 'genre': 'Adventure',\n",
    " 'runtime': '1h 31m',\n",
    " 'theater': 'Coolidge Corner Theater',\n",
    " 'screen_date': '2024-08-17'\n",
    " 'screen_times': '11:59 PM'}\n",
    "```\n",
    "</div>\n",
    "\n",
    "2. Report the number of dictionaries in your iterable.\n",
    "3. Display an example dictionary.\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "<b>One possible approach:</b> leveraging the structure of the page, use BeautifulSoup to divide it into disjoint peices of the HTML tree, each containing the set of information outlined above for each title's screening date at a given theater. Each peice is processed by a helper function(s) which extracts the desired fields and assemples them into a dictionary. These dictionaries are then accumulated in some iterable such as a list.\n",
    "</p>\n",
    "\n",
    "**Hints:**\n",
    "- Your browser's 'inspect' tool is very useful for exploring a page's structure.\n",
    "- To maintain a uniform set of keys across all movie dictionaries, use the plural 'directors' even for movies which attribute only a single director. \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd237053-2e4d-4081-9d70-6281d7b73e97",
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c28617",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcacef4-c29f-4743-8811-45064c7a0357",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=md>\n",
    "    <h3>3. Adding Historical Data: Snapshots from the Internet Archive's Wayback Machine</h3>\n",
    "<p>\n",
    "The screenboston.com page only shows screenings scheduled from the current day until about 2-4 weeks into the future.\n",
    "Using the <a href=\"http://web.archive.org/\">Wayback Machine</a>, we can augment our dataset with past snapshots of the page  \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Rather than clicking around to download each snapshot manually, we should again acquire our pages programmatically using <code>requests</code>. The Internet Archive offers an <a href='https://archive.org/help/wayback_api.php'>API</a> where you can specify a URL and timestamp and receive a JSON object which contains a new URL to a snapshot of the desired page nearest to the provided timestamp. You can then use <code>requests</code> again to acquire the HTML of this snapshot.\n",
    "</p>\n",
    "    \n",
    "<div class=prompt>\n",
    "\n",
    "1. Acquire enough snapshots to cover all days from Jan through September 2024 for which there exists data. \n",
    "For simplicity we'll assume that any well-formed screenboston.com page will cover at least through the end of that month and that a day's schedule won't change once displayed.\n",
    "2. Write each snapshot's HTML content to disk in the `/data/html` directory using filenames of the form `'snapshot_YYYYMMDD.html'`. Your code should only make requests if no local snapshot files already exist.\n",
    "3. Using your parsing function from the previous section, add movie dictionaries extracted from all the snapshots to your list, `movies`. Save this list of dictionaries as 'data/movies.json'. You should only perform the parsing if the JSON file does not already exist. Don't worry about any duplicate movie screenings in your list at this point.\n",
    "\n",
    "</div>\n",
    "\n",
    "**Hints:**\n",
    "- When retrieving JSON content with <code>requests</code>, us the `json()` method rather than the `text` attribute to extract the content from the returned request object into a nested dictionary rather than a string.\n",
    "- APIs often take parameters as key-value pairs occuring after a '?' character in the URL. For example 'http://archive.org/wayback/available?url=example.com&timestamp=20060101' includes a 'url' and a 'timestamp' parameter. When using <code>requests.get()</code> you can exclude these parameters from the URL itself and instead pass them as a dictionary to an argument called 'params'.\n",
    "- Use <code>json.dump()</code> and <code>json.load()</code> from the imported <code>json</code> module for writing and reading JSON content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cba772-70ea-40c6-abaa-ff20025c7cf3",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0cfad-0b4a-49de-9660-c5b3cff54e69",
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c000d1-37c3-4b59-b557-4ce6ffb8a8f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display info about snapshots and results of parsing\n",
    "print(f\"Snapshots:\")\n",
    "print([f.split('/')[-1] for f in glob.glob('data/html/snapshot_*')])\n",
    "print(f\"Found {len(movies)} movie entries across all HTML files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7541671",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e227fb9-5645-4086-b89e-fda1afa80d19",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=md>\n",
    "    \n",
    "<h3>4. DataFrame Creation & Manipulation</h3>\n",
    "\n",
    "Your variable `movies` is a list of dictionaries which you can use to create a Pandas DataFrame. Have our data in a DataFrame will make it much easier to analyze and manipulate.\n",
    "\n",
    "<div class=prompt>\n",
    "    \n",
    "1. Use `movies` to create a DataFrame and call it `df`\n",
    "2. Drop any **duplicate rows** and report the number of remaining rows. Here we consider duplicates to be rows in which *all* values match. The same movie screening on different days, even at the same theater, will not be considered a duplicate.\n",
    "3. Check for **nans**. If you find any nans you should report the number of nans in each column and display the rows that contain at least one nan. If there are no nans then you should display output to confirm this fact.\n",
    "4. Convert the **'screen_date'** to a Pandas `datetime` dtype and **'runtime'** to a Pandas `timedelta` dtype.\n",
    "5. Sort your `df` by ascending 'screen_date'\n",
    "</div>\n",
    "\n",
    "Your result should look something like:\n",
    "\n",
    "| title                            | directors       | year | genre  | runtime         | theater                   | screen_date | screen_times         |\n",
    "|-----------------------------------|-----------------|------|--------|-----------------|---------------------------|-------------|----------------------|\n",
    "| Ghost Dog: The Way of the Samurai | Jim Jarmusch    | 1999 | Crime  | 0 days 01:56:00 | The Brattle               | 2024-05-24  | 4:00 PM, 9:00 PM     |\n",
    "| Stray Dog                        | Akira Kurosawa  | 1949 | Crime  | 0 days 02:02:00 | The Brattle               | 2024-05-24  | 6:30 PM              |\n",
    "| Big Trouble in Little China       | John Carpenter  | 1986 | Comedy | 0 days 01:40:00 | Coolidge Corner Theatre    | 2024-05-24  | 11:59 PM             |\n",
    "| Young Frankenstein               | Mel Brooks      | 1974 | Comedy | 0 days 01:46:00 | The Brattle               | 2024-05-25  | 12:00 PM             |\n",
    "| Poetry                           | Lee Chang-dong  | 2010 | Drama  | 0 days 02:19:00 | Museum of Fine Arts        | 2024-05-25  | 2:30 PM              |\n",
    "\n",
    "**Hints:**\n",
    "- You can use the `dt.date` attribute on a datetime column to extract just the day (and ignore the time)\n",
    "- You may need to to do some string parsing on 'runtime' before coverting it to a `timedelta`\n",
    "- Pandas now supports [PyArrow](https://arrow.apache.org/docs/python/index.html) as a backend alternative to Numpy. This gives access to new datatypes including native strings (as opposed to the catch-all 'object'), ints that support NaN values, and others. Using these datatypes can often speed up computations so you may choose to experiment with converting some or all of your columns to these new datatypes. You can read more [here](https://pandas.pydata.org/docs/user_guide/pyarrow.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6cd265-e541-4f93-abb4-ea00287f7b08",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e474a9-10b2-4376-9ce1-620883241788",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad88a84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f235ae-92d0-49b6-83aa-eb0a4021b1eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=md>\n",
    "    \n",
    "<h3>5. Initial Exploratory Data Analysis (EDA)</h3>\n",
    "\n",
    "Using your DataFrame, perform the following exploratory data analysis tasks. Note that we'll use the term \"screening\" to refer to rows in the DataFrame example provided above. For each visualization component of this question you should interpret the results and draw conclusions.\n",
    "\n",
    "<div class=prompt>\n",
    "\n",
    "1. **Longest Films**:\n",
    "   Display the rows corresponding to the top 5 longest reported runtimes in descending order.\n",
    "\n",
    "2. **Screening Counts**:\n",
    "   Create two plots as side-by-side subplots with a shared y-axis range:\\\n",
    "   a. Number of screenings by theater\\\n",
    "   b. Number of screenings by genre\n",
    "\n",
    "3. **Movie Year Analysis**:\n",
    "   Create two separate plots:\\\n",
    "   a. Distribution of 'year' for all movies\\\n",
    "   b. Distribution of 'year' grouped by theater\n",
    "\n",
    "4. **Custom Analysis**:\n",
    "   Ask two questions of your choice regarding the screening data and use plots to attempt to find some answers. For each question:\\\n",
    "   a. Clearly state your question\\\n",
    "   b. Explain your approach to answering the question\\\n",
    "   c. Create an appropriate visualization\n",
    "   \n",
    "   At least one of these new plots must be a different type from those used in parts 2 and 3.\n",
    "\n",
    "Example questions you might consider:\n",
    "- Is there a relationship between a movie's age (current year - release year) and the number of screenings it receives?\n",
    "- Do certain genres tend to have longer runtimes?\n",
    "- Are there patterns in screening times for different theaters or genres?\n",
    "- How does the distribution of genres vary across different theaters?\n",
    "\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6435e-bc72-4d9c-9b55-bc8d31362318",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Longest films\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfab035-93a1-4d20-8a8c-5d288f07bd15",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 2. Screenings by theater & screenings by genre\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e315e-5b10-4fa0-bda1-8067dd8bb5ee",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 3. Year distribution & distribution of year by theater\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167374a0-bb68-4188-b766-0919e94c261c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 4.1 Custom question 1 - Runtime Distributions by Year (Scatter with Trend Line, Outliers Removed but Included in Regression)\n",
    "# your code here\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa02d2a-c38e-480c-8c81-08107b1eaa54",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 4.2 Custom question 2 - Top 10 Directors by Number of Screenings (Aesthetic Improvements)\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6174f-88a3-4e95-b111-f9b24ab16c34",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<div class=md>\n",
    "<h3>6. Get each filmâ€™s Wikipedia page identifier</h3>\n",
    "\n",
    "Use the Wikipedia Python library to find a stable identifier for each filmâ€™s Wikipedia page and store it in a new `wiki_id` column. Prefer the numeric page ID (`page.pageid`); a canonical article URL is acceptable. Display the head of your updated DataFrame and briefly describe your approach. Aim for at least 50% coverage.\n",
    "\n",
    "<strong>Hints:</strong>\n",
    "- Set up the library: `wikipedia.set_lang(\"en\")` and `wikipedia.set_rate_limiting(True)`.\n",
    "- Build your search query from known fields (e.g., title + year + \"film\" [+ director if helpful]).\n",
    "- Use `wikipedia.search(...)`, then fetch a page with `wikipedia.page(..., auto_suggest=True)`.\n",
    "- Extract a stable ID: `page.pageid` (preferred) or `page.url`.\n",
    "- Handle `DisambiguationError` and `PageError`; return `None` when youâ€™re not confident.\n",
    "- Cache results (e.g., write `wiki_id` to a CSV and only query rows where itâ€™s missing).\n",
    "- Optional: use a small thread pool for speed, but keep concurrency low to be polite to Wikipedia.\n",
    "\n",
    "\n",
    "<strong>Optional performance hints (polite concurrency):</strong>\n",
    "- The <code>wikipedia</code> calls are blocking I/O. If you want to speed things up, use a small <code>ThreadPoolExecutor</code> (threads are appropriate for I/O). Keep <code>max_workers</code> low (2â€“4; at most ~8) and enable <code>wikipedia.set_rate_limiting(True)</code>.\n",
    "- Simple threaded map (no asyncio):\n",
    "  \n",
    "  ```python\n",
    "  from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "  wikipedia.set_rate_limiting(True)\n",
    "\n",
    "  def get_wiki_id_for_movie(row):\n",
    "      # your blocking lookup; return pageid or None\n",
    "      ...\n",
    "\n",
    "  rows = df.to_dict(\"records\")\n",
    "  with ThreadPoolExecutor(max_workers=3) as ex:\n",
    "      wiki_ids = list(ex.map(get_wiki_id_for_movie, rows))\n",
    "  df[\"wiki_id\"] = wiki_ids\n",
    "  ```\n",
    "- If you prefer <code>asyncio</code>, you can run the blocking function in a thread pool and gather results:\n",
    "  \n",
    "  ```python\n",
    "  import asyncio\n",
    "  from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "  async def get_wiki_ids_async(rows, max_workers=3):\n",
    "      loop = asyncio.get_running_loop()\n",
    "      with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "          tasks = [loop.run_in_executor(ex, get_wiki_id_for_movie, r) for r in rows]\n",
    "          return await asyncio.gather(*tasks)\n",
    "\n",
    "  wiki_ids = await get_wiki_ids_async(df.to_dict(\"records\"), max_workers=3)\n",
    "  df[\"wiki_id\"] = wiki_ids\n",
    "  ```\n",
    "- Cache results to disk so you donâ€™t repeat requests. Handle <code>DisambiguationError</code>/<code>PageError</code> and return <code>None</code> gracefully.\n",
    "\n",
    "\n",
    "<strong>Ensure that :</strong>\n",
    "- `df` has a `wiki_id` column.\n",
    "- â‰¥50% of `wiki_id` are non-null.\n",
    "- You show the head of the updated DataFrame and briefly describe your approach.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715a479",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Custom question 2 - Retrieving Wikipedia page IDs for movies\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764473d-6052-4ed0-82db-ac6e7b34a7d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display results of updated df\n",
    "missing_rows = df[df.wiki_id.isna()]\n",
    "num_wiki_id_na = missing_rows.shape[0]\n",
    "print(\"Number of screenings missing a wiki ID:\", num_wiki_id_na)\n",
    "if num_wiki_id_na > 0:\n",
    "    print(\"Example rows with missing wiki ID\")\n",
    "    display.display(missing_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b8849-38eb-4687-90ab-349237d0d6eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*your answer here*\n",
    "\n",
    "With these missed pages the above approach is still suboptimal.\n",
    "The code makes aysnyc wikipedia searches of the form '\\<title> \\<director> film'. It then goes through the search result pages until it finds one in which half of its 'categories' (listed at the bottom of a Wikipedia page) are contain 'film'. It probably would have been better to check for certain fields in the infobox (the section on the top right of most wikipedia pages) rather than the looking at the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded9d53",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6b0b6-93a9-4604-a218-bdefcc6bf484",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='md'>\n",
    "    \n",
    "<h3>7. Acquire HTML from wiki page and store in DataFrame</h3>\n",
    "\n",
    "<div class=prompt>\n",
    "    \n",
    "Use `'wiki_id'` to acquire the HTML content of each film's Wikipedia page and store it in a new column called `'wiki_html'`.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c591d79",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444931d1-6a06-4971-a0af-5b89d3ccd676",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display results of updated df\n",
    "missing_rows = df[df.wiki_html.isna()]\n",
    "num_wiki_html_na = missing_rows.shape[0]\n",
    "print(\"Number of screenings missing wiki HTML:\", num_wiki_html_na)\n",
    "if num_wiki_html_na > 0:\n",
    "    print(\"Example rows with missing wiki HTML\")\n",
    "    display.display(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d49e25",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a9317-d9b5-4215-9f27-9df08d7b2f69",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=md>\n",
    "\n",
    "<h3>8. Adding Data from Wikipedia and Asking New Questions</h3>\n",
    "\n",
    "Now that we have Wikipedia pages for most or all of our movies, we can add supplemental data from those pages to our DataFrame, ask new questions, and use Pandas and visualizations to try and get some answers. This final section is very open-ended, and you are encouraged to experiment. We want to see what you can do!\n",
    "\n",
    "<div class='prompt'>\n",
    "\n",
    "1. **Data Extraction and Integration**\n",
    "   - Extract at least one new piece of data from each movie page you found.\n",
    "   - Add this data as a new column(s) to your DataFrame.\n",
    "   - Display the head of your updated DataFrame.\n",
    "   - Discuss your strategy for extracting the new data, including any challenges you faced and how you overcame them.\n",
    "\n",
    "2. **Data Analysis and Visualization**\n",
    "    - Formulate at least two new questions that involve the Wikipedia data you've added.\n",
    "    - For each question:\n",
    "      - Clearly state your question and why it's interesting or relevant.\n",
    "      - Describe your approach to answering the question.\n",
    "      - Use Pandas to manipulate and analyze the data as needed.\n",
    "      - Create at least one meaningful visualization that helps answer your question.\n",
    "      - Ensure your plots are well-labeled and easy to interpret.\n",
    "      - Interpret the results of your analysis, discussing any insights, patterns, or surprises you discovered.\n",
    "\n",
    "</div>\n",
    "\n",
    "**Hints:**\n",
    "- The 'info box' on the right side of most Wikipedia pages is fairly standardized across film pages. This might be the easiest place to start.\n",
    "- Some potential pieces of data to extract include the language of the film, its box office revenue or budget, the starring cast, etc.\n",
    "- Feel free to extract more than just a single new piece of data to use in your analysis. If you're feeling particularly adventurous, you can even jump from the film page to other linked pages to acquire more data!\n",
    "- Consider how the new data you've extracted might relate to the screening information you already have. Are there interesting connections or patterns to explore?\n",
    "\n",
    "Remember, the goal is to demonstrate your ability to ask insightful questions, use appropriate data science techniques to answer them, and effectively communicate your findings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce124146-e7ee-421e-b249-2b421ad3d57b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Data Extraction and Integration\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce4c29-8c1f-4939-859b-632506dc636f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 2. Data Analysis and Visualization\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37720d-10ea-4c90-95bf-d8ebb6a89e3d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d87af6",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d899db-0e7d-4d80-87e4-1e100b9c0aa8",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dce54e-0e1d-495c-9a9f-f9c3d59268a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\" style=\"color: #4a4a4a; background-color: #fbe8ff; border-color: #eed4db; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "**Wrap-up**\n",
    "\n",
    "* Please describe the aspect(s) of the assignment you found most challenging. This could be conceptual and/or related to coding and implementation. This is also a good place to ask any remaining questions you may have.\n",
    "\n",
    "* How many hours did you spend working on this assignment? Store this as an int or float in `hours_spent_on_hw`. If you worked on the project in a group, report the *average* time spent per person.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea51b8-4184-486b-9073-465e6f724ffc",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "hours_spent_on_hw = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217aaf0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"wrapup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc0855-6ed3-42fa-a89a-0ea7f63956bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_end = time.time()\n",
    "print(f\"It took {(notebook_end - notebook_start)/60:.2f} minutes for this notebook to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e4ed6-d3d9-44aa-88f6-61f712d915d9",
   "metadata": {},
   "source": [
    "ðŸŒˆ **This concludes HW1. Thank you!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> assert os.path.isfile(expected_url), f'Expected local file {expected_url}'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> with open(expected_url, 'r') as f:\n...     content = f.read()\n...     s = BeautifulSoup(content)\n...     assert s.select_one('p').text == 'Screen Boston', f'Content of file saved from {expected_url} should contain a <p> tag with the page name.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n = 50\n>>> assert len(movies) > n, f'`movies` should contain more than {n} elements; you have {len(movies)}.'\n>>> assert all((isinstance(m, dict) for m in movies)), 'Elements of `movies` should all be dictionaries.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((set(keys).issubset(m.keys()) for m in movies)), f\"Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.\"\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> assert all((isinstance(m['year'], int) for m in movies)), \"The 'year' value should be an integer in all dictionaries.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert len(snapshots) >= 4, f'You should have at least 4 snapshots, but found {len(snapshots)} files with paths like ./data/html/snapshot_*.html'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert all((re.match('snapshot_\\\\d{8}\\\\.html', os.path.basename(f)) for f in snapshots)), \"All snapshot files should be named in the format 'snapshot_YYYYMMDD.html'\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert os.path.isfile('data/movies.json'), \"The file 'data/movies.json' should exist.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> with open('data/movies.json', 'r') as f:\n...     movies = json.load(f)\n>>> n = 300\n>>> assert len(movies) > n, f'`movies` should now contain more than {n} elements; you have {len(movies)}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((isinstance(m, dict) and set(keys).issubset(m.keys()) for m in movies)), f\"Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.\"\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(df, pd.DataFrame), \"You should have stored your DataFrame in a variable called 'df'\"\n>>> assert df.shape[1] == 8, 'Your DataFrame, df, should have 8 columns'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert df.shape[0] > 300, 'You should have found at least 300 non-duplicate rows'\n>>> assert df.shape[0] == df.drop_duplicates().shape[0], 'There are still duplicate rows in your DataFrame'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['screen_date']), \"The 'screen_date' column must be either a Pandas or PyArrow date/datetime type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['runtime']), \"The 'runtime' column must be either a Pandas or PyArrow timedelta type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert df['screen_date'].is_monotonic_increasing, \"The 'screen_date' column is not sorted in ascending order\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 7,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_id' in df.columns, \"`df` should now have a column called 'wiki_id'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_id'].notna().mean()) >= p, f'You should have been able to find wiki IDs for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_html' in df.columns, \"`df` should now have a column called 'wiki_html'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_html'].notna().mean()) >= p, f'You should have been able to acquire wiki page HTML content for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "wrapup": {
     "name": "wrapup",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
