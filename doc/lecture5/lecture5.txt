109 day 6 - YouTube
https://www.youtube.com/watch?v=ob2QGY6qlOY

Transcript:
(00:01) What would be really funny? I think that's what Yeah. But then that means you s me too. So that means I'll pay attention. All right. Good morning. Good morning everyone. Good morning. Can you hear me? Okay. Good morning. Can you hear me? Okay. Hello. Good morning. All right. All right. So now 10:30 sound like a great time.
(01:01) Uh, everybody seems energized. I like that you're chatting. It's fantastic. That means you're awake and I guess the weekend was fun. You have a lot of sun and everybody seems to be in good mood. Uh, just to start in the regular welcome welcome students here, there over there, here, there, over there online and online. I'm going to I'm your instructor. My name is Pablo Proto just in case.
(01:27) you all of a sudden decide to watch the videos from lecture six forward. Okay. So, uh this is uh today we're going to be talking about interaction terms and polomial regression. Today we're going to extend beyond simple linearity simple linearity. Now we're going to start some more complex. Now we're going to start doing some more complex models.
(01:51) And I did say the first couple A and B A 109 A and B start with something is we start with something simple we fully understand it. We say can we do better and then we keep going and then we keep going. I want actually that to be part of your the way you think about projects when you do any project you do for this any project you do for this class or overall.
(02:09) Start with something simple fully understand with something simple fully understand it interpreted it and jumping into jumping into transformers right off the bat. Visual transformers right off the batus modus diffusion models out of the bat because you found a tutorial that works. I don't recommend it. Actually I dislike it with passion. So I want you to get into the habit.
(02:33) Okay, we're going to start with something simple. Habit. Okay, we're going to start with something simple. Actually, we're going to start with EDA. We're going to start with something simple model. We understand linear regression. Yes, it's not going to work. But I want to try I want to see how much I can understand and then keep building. So today we're going to get to the next level.
(02:50) We're going to be talking about going to go beyond linearity. We're going to be talking about interaction terms and polomial regression. But I think before I do that, I think last one last week I didn't finish. There was one slide left all of you two three minutes.
(03:08) I'm going to be finishing late all of you two three minutes. I'm going to three minutes late because I have a lot to talk today. Let me get I'm going to share on Zoom. Don't worry. And I'm going to share on Zoom. Don't worry. Okay. And zoom. And the zoom. I'm becoming good at this. While you all settling down happily. Good at this. While you down happily.
(03:48) Where the hell is here? Okay. Yeah. Not for Zoom people. There should be a button there. Not for Zoom people. There should be a button there. Say Microsoft PowerPoint. Group them all together. Chris is a different thing. Yeah. Okay. All right. So, we left somewhere here. Okay. All right. So, we left and I want to bring it back.
(04:17) Not more coins. No, I think already someone got that. Let's move to the next thing which is uh um so so if we have to predict if we have two predictors you talk about dummy variables or one hot encoding is that what is that? I hear a chatter be quiet. All right. So if I have a a predictor a predictor that is categorical and it has two values.
(04:58) What should we do? What should we do? What do we do for Yes. Yes. 0 and one. Okay. Zero and one. Okay. Great. So do that. So we're going to do that. So we talk about that. And in this case for example the data we gave uh one for what was it now? I forgot. Uh yeah it was yes biological sex we gave zero and one and we even interpreted the coefficients we found.
(05:25) Now what if we have more than two categories? What do we do? Shall we give 01 and two? Okay. Why not 012 or one two three? you are assigned in more ways one the categorical variables uh I'm going to rephrase it a little bit or like the relationship between variables are being assigned to zero on the two the gap between the one zero and two for some order first and second the the difference between zero and one is one between zero and two is two. So you inherited put some important some weight. I think you both said the same
(06:15) thing. Uh so that's not good. So we go into your name Zoe. Zoe said uh we do we go 0 0 1 0 1 0 0. So let's explain that in a little bit more uh detail. So let's say in this case we have let's say we have ethnicity and I have three categories Asian Casian and African American. So I'm going to have to have three dummy variables and each one of them tells me if that person is Asian or not African American in this case this case would consider exclusive.
(06:54) Not not usually the case but nevertheless uh so in this case so we have so in this case so we have three variables. So each one one will tell us if that person is in that category or not. Do we need three? Can we do two? If I know there's only three categories and I know the person is not Asian or Caucasian, that means he's African-American, right? So in principle, we only need two. Okay? So we can drop one of the predictors.
(07:31) So we don't have many predictors flying around. So we don't have many predictions that you can do. So that's the idea is that you can do uh just one less so you don't have to worry about so in this case we have so in this case we have beta 0 plus beta 1 person beta 0 plus beta 2 the person is caucasian and beta 0 the person is Africaname good so what is the interpretation beta 0 beta 1 and beta 2 I don't have the answer into the slides but I want you to think the same way we did last time what is the interpretation of beta 0 if the only predictor we have is ethnicity. If
(08:08) everything is zero, beta 0 will be everything is zero, beta 0 would be the average balance for African-American. Beta 0 plus beta 1 is the average for Asian and beta 0 plus beta 2 is the average for okay is that clear this idea it's going to be in the quizzes and in the midterms. So I want you to understand it and Kevin loves this question so watch out.
(08:32) Uh so let's now think about so far we have assumed uh linear relation between X and Y. Uh the residuals uncorrelated and these assumptions need to be verified and these assumptions need to be verified with the data. So I'm going to stop here go to the other lecture because I have it all started from the beginning with all the assumptions we need to worry in the meantime.
(08:58) This is the time to talk to your friend what happened the weekend until I get it uh figured out here. It will only take a second. Let's start with the zoom here and then let's do the other one. Change All right, here we are where we started 5 minutes ago, 10 minutes ago. So we're going to go now talk more precisely about nonlinearity. So let me get with the road map, the outline.
(09:47) First we're going to be talking interaction effects in regression models. This is the first kind of nonlinearity we're going to throw into our models. And this is when two predictors depend on each other. So we're going to talk about that. Then we're going to go to polomial regression extending the linear models.
(10:05) And then we're going to go to the fun parts model selection techniques. And we're going to introduce today one of my favorite things which is cross validation. Okay. So let's start. And now you have to repeat after me. Too many predictors and colinear lead to overfitting. Good. All right. So, game time. We start with a fun game.
(10:30) Actually, I have two fun game times today. Uh, okay. So, the first one is just to warm up. It is for a student. Any student who wants remember if you get it right. Wait, didn't you do Jacob already one? You really don't like this class, do you? Um, let's give it and you you remember your name, the school you are. All right, go ahead. Hi, I'm Jason. Go ahead. I'm Jason. I'm an Adam. All right.
(11:01) I'm actually what? I'm Jacob Rubin. Oh, come on. All right. So, all right. Wondering what is the discussion in your room at night. All right. All right. Not go to that class. All right. All right. So, here's the question. Okay. Any fun facts? Ah, the fun fact is that your roommate to Jacob, right? All right.
(11:24) So the question if your mother was a student what would overfeeding be like let me explain. I haven't defined overfeeding yet in some ways but I think most of us have an idea but I'm going to define it of course right so let's start by asking what is overfeeding imagine of is a student right who will be option number one studying just the night before the test memorizing every lecture love and officer word for word only studying one chapter for all subjects and I think I have the taking extensive notes but forgetting to actually understand the concepts. What is overfeitting the way you understand it
(12:02) for students? If you're a student, I think either like B or C like sound like pretty close to the definition, but I think B or C memorize your C. I think I would probably go for B. All right. I think like C would also be like a decent death. All right. This time let's use the audience. All right. So, your name again? I'm Jason. Jason's is between B and C.
(12:29) Uh, if you think it's Bclap and if you think Ccl. All right, Jason. Okay. So, I think it's B. All right. He believes he's the audience and the audience correct. Thank you, Jason. I need the microphone. Now, we have one more, but not. So in this case, this goes for one of the TFs. Where are my TFS? Is it only you here, Chris? All right. You're the only TF here.
(13:04) So you're going to answer that and you get it right, you can skip coming to class once. All right. Your name? Uh my name is Teddy. I'm a junior and I took the class last year. You're a junior in which house? Uh in career house. Okay. Okay. All right. So, yes in the quad. All right. Best place on camp. All right. Let's cannot get it is in fact the best deal. Okay. So, here's a question to you.
(13:36) Uh in your model was a TM. What will overfeit be like? Okay. So, number eight, option A. Grading papers while wearing 3D glasses to see the errors in a new dimension. No, wait, wait, hold on. See all the options here. Number B. Using a magic grade ball to decide students grades.
(14:01) See, give everyone the first letter that comes on their name. Sorry, Frank. Is Frank here? Any Frank? Okay. And subtract points for every answer that does not include the word overfeeding. It could be D if TF learned to Well, we said this is an overfeed TF, not you, of course. Yes. Um, for um over the one that comes to mind, I think it's D. Okay, Theo, this is your final answer. Final answer D.
(14:33) If you get it right, you skip one lecture. The correct answer. Thank you, Theo. Excellent. All right, so this is the idea. We getting into what overfeitting is slowly slowly of course eventually we have to define it right so correct answer is D. So back to too many predictors and colinearity links to overfeitting but we heard that the question is what what is overfeitting in some more formal way.
(15:00) So I think most of you have some idea. So I think most of let's just talk about so occurs when a learns the training data too well including his noise and outliers resulting in poor performance including his noise and outliers in data. Okay. So in the case of the student over just word by word. So understand so when he gets a new question cannot answer the question because he has not answer the question because he hasn't learned the concept. You just learn to memorize. Okay. So that's the idea of fitting. So the model
(15:32) learning nonsensical learning nonsensical pattern only the data all the things that in the data but not learn. Is that clear to everyone? Yep. Okay. Is that clear to everyone? Now let's start with now let's start with keep in mind because it's going to come and bite. Keep the overfeed in mind because it's going to come and bite. We know there is a problem over.
(15:54) So we know there is a problem over feeding but when does it occur is when I have too many predictors why because the model has the possibility the option has a lot of parameters to adjust in order to fit the training data exactly right but it does in general okay so we'll see more of these examples okay so let's now extend so okay so let's now extend the linear regression the linear assumption by doing one thing that's called before we do Actually before we do let's actually talk about the assumptions of linear regression. The first one instead of
(16:29) define let me show you what we have done and just going to pull it out of there and then the next slide will have them nicely listed. So in linear regression what we have done I was bored last night. Okay. Uh so the first thing we said is that the relation between x and this function of x is linear. Okay. That's an assumption of linear regression. There's no doubt about that.
(16:56) Right? And the next thing we did, you remember we we defined the data. Then next what we define the model. What's the next step in machine learning? Before we train before we train on what? Train on what? You train means minimizing what the error the loss function. So we define the model. The next thing we have to do is to set the loss function. So we set the loss function to be the MSE.
(17:25) Right? Okay. MSE these are the only two things. Okay. Now these are the only two things we've done. And then of course we train we find the betas that minimize the loss function. But the two things but the two things that we did and their assumptions is the linear function and MS function and MS minimizing. There's no assumption.
(17:42) We say exact as a matter of fact, right? The data the data we don't have any assumptions with it. the data given to us. So two things we did define the function to be linear and the second one was the MSE. Okay. Now let's look at that.
(18:02) So the the first assumption for linear regression is the linear relationship dot. Okay. Obviously right cool one the second one is a little trickier. We ass we sum every square error every square we aggregate them. We just s them every aggregation we just sum them or average them that that that that means that every error is independent of each other. Okay.
(18:28) So that assumes independency because we just sum the square of the errors. Okay. The third one we put no weight on anyone. Every point error was treated equally. that assume homoskeetasticity. I said it right. Okay. So that means the error that we expect from every point is equal for every point. Right? If there were not, what would you do? I leave that there.
(19:08) We'll come back to and finally we square the errors square the errors assume that's normal assume normally normal distribution of that if that last point confuses you and actually the the first the first the second one the independence confuses you in lecture nine I believe Kevin will show you all these assumptions are they come to play to come to this loss okay u right now is the fact that we sum without doing any coariance or anything. It means independence. The fact that we just take the square of the errors that assume
(19:46) normality and the fact that we don't weight the errors at all we just keep them all the same that assume homoskeasticity. Okay. All right. So that's the thing and I wrote them down nicely for you so you have a reference slide. constant variance of residuals, normality of the residual and normality distribution.
(20:10) Okay. Uh this is the same thing I said. But there are two other things to consider that they don't violate the linear uh assumptions, the linear regression assumption. Uh one thing actually I think it's fine. I'll ask Kevin in a second. So we assume that measurement of X is constant. There's no error in the measurement of X.
(20:35) So if you do have error in X you need to go into more basian approach. The second one is no multiolinearity. Low correlation between predictors. Low correlation between predictor multiolinary will not affect your linear regression assumption mess things up. Okay. Things are just so the last two things are just warnings. The first four that we have there.
(21:00) But the first four that we have there are four assumptions that okay. Now how do we know that? Now how do we know that our data linear relationship described by linear relation? I give you data you run so I give you data you run your linear regression. You give it back to me. How do I know? You give it back to me. How do I know that my assumption for linearity was good? The first one actually we check all the how do we know these assumptions? It depends on the data of course right? How will I know that? Well, our main diagnostics uh tool we have is
(21:36) called residual analysis. Residual is the difference between my prediction and the true value. Okay, so what are we going to do? So on the left here you see here this is my x and the y nicely right and then on the top here let's see what I plotting here x on the x axis and r which is the residual on the y axis. Okay.
(22:02) Axis now. Okay. Now, and on the bottom and on the bottom plot here, I'm doing the histogram of those residual values. Okay. Now, if assumption number one is true, these residuals here will be dist will be scattered around zero equally. And if assumption number three and four if the normality and independence that means that this is going to look more like a normal distribution.
(22:30) So I have a question um actually let's yeah let's look at another this is you see the data obviously are not linearly cor related okay this is my data and now if you look at the residuals they are not nicely scattered randomly around not nicely scattered around zero you see that there is kind of a U- shape here let me put this way if you see any let me put this way if you see any patter in your residual plot even if you imagine dragons flying over castles any pattern that you can see that means your assumptions are violent.
(23:09) Now what it is is another story. Um so this is a so then usually this histogram may not look like a nice bell shape. Now let's think about one thing. What if I see residuals and I don't have it on slide so I'm going to draw it and tell me if you can see up in the legs up there.
(23:42) So let's say I have residuals everybody can see over there. Yeah. Let's say the residuals are like that. So they are small here and they become bigger on there. What assumption doesn't hold. Okay, good. That means the variance is not the same. Yeah. And which diagnostic tool you think is going to work? Will the histogram show us that it's like that? Yes. I mean once I see that I know that it's not homosastic.
(24:24) Okay, do you follow me on this? So if my residuals have that shape it means it's not homo. Is it normally distributed? Why not? Actually let me draw another one to confuse you even more. I like to confuse you. I want you to think this. This is my residual plot. X residual nicely symmetric.
(25:04) Is it nor is it nor is it normal? Is it normal? Okay. histogram. The histogram will look like something like that. Do is the normality holds? Does the normality hold? Anyone to say yes or no? You say yes. Yes. You said yes because the histogram look like normal, right? It's a it's symmetric because at least his program. Okay. Can you still hear me? Can you still hear me back there? Can you hear me? Can you hear me up there?
(26:11) Yes or no? Thank you. Um, so the argument is the argument is there looks like to have the histogram there looks like to have a bell shape. Okay. All right. I'm going to leave it there because I want you to think about it and it's part of your homework. But start thinking about is that enough to test normality? Do we need something else? Kevin, you smile. Kevin, you smile. That's a hint. Yes, that's a hint. Yes.
(26:44) Okay. So, let me actually So, let me actually example. Let me actually make an example. Let's see. Let's say these guys here are Let's say these guys here are nice outliers, but here have big outliers. By doing a histogram is like by doing a histogram it's like I'm doing histograms of all of them together marginalized over so that may not be good enough to test something else and I'm going to tell you how and what and I'm going to tell because I'm going to put it as a homework question and you have fun
(27:19) and you have fun think so the residual plot is our diagnostics and you can deise other diagnostic tools to do to test the assumptions but in general If you see something like this, meaning your residual um are scattered around zero randomly, it look like a wide noise and your histograms look like a bell-shaped normal. I think we're good.
(27:49) Okay, there are other diagnostics and we're going to talk about actually we're not you're going to do it for me. Okay, you're going to do it for me. So, any questions about that? So, any questions about that? So, so we start by saying we know what is a linear regression assumption.
(28:08) Now we have a diagnostic to say is good or not. What if it's not? Let's say the data like that. What are we going to do? We're going to go beyond linearity. So let's start beyond linearity. So let's beond linearity. The first thing we're going to do is the synergy effect or interaction terms effect. So what is that effect? So what is what do we assume? What we assumed before so far is that if I increase the budget for TV by 1,000, the sales will increase at some fixed value which we call beta Y.
(28:45) Right? Is that clear? But we then consider what happens to all the other budgets. Maybe if I increase my TV budget by 1,000 and at the same time increase the newspaper or the radio or the other media by 1,000 maybe that would not have the same effect. Okay, you follow me on this? So this means interaction terms means that the effect of one predictor on the overall um response variable may be affected but what's happening on the other predictors. So this is what synergy effects we call.
(29:25) So to model that remember the first thing we do is model it. What do we do with linear regression? We said beta 0 plus beta 1x plus beta 2x2 blah blah blah. Now I'm modeling it. And here is my model down here. I think it's going to give me a nice here. That thing. Now notice I multiply x1 with x2. Is that linear? Actually, what is linear? Linear means every term I have for predictors is to the power one. Here I have two terms. I multiply them.
(29:58) That is not linear. Okay. So x * x1 * x2 is a nonlinear term. Yes, we going beyond linearity. You ready for that? Okay, that's our first nonlinear terms we have. So to demonstrate that, I'm going to go back to this data set that we showed last time, which is the uh credit card balance data. It has a bunch of predictors and we're trying to predict the the balance of the credit for every individual the 700 or so individuals here. Uh and we're trying to predict there's a bunch of predictors.
(30:34) One is income limit rating etc. But I'm going to focus on only two predictors here because I want to talk about interaction term. The first one is income and the second if it's a student or not. So before I even show you something, let's look at the intuition. The intuition is the following.
(30:57) If I want to predict the balance and I ask the question, what if I increase the income of the individual by one unit which is $1,000, right? What is the expected increase in the credit card balance? Okay, that's balance. Okay, that's the question. Now there is let's say two population students and non students. So, so without interaction term, we say that if I increase the income of an individual by one unit, which is $1,000, I'm expecting the credit card balance to increase by so much, which is the better one, right? But now I'm saying, well, students behave different from non- students, right? The students get $1,000 more,
(31:35) say, hey, let's go party. Right? That means more balance, right? I'm making this up, of course. Maybe it's not the case, but it depends what population one which is a student may react differently if you increase the income by 1,000 where the population two which is not a student may react differently. Okay.
(32:01) Is that intuition gives you a good ground to understand what I'm going to get right? All right. So let's do that. So first I'm going to say I'm going to show you without interaction terms. So here is my my model is balance equal to beta 0 plus beta 1 income plus beta 2 students. Remember student is going to take two values is categorical.
(32:20) What are the values? Zero non- studentent one student. Yep. Good. All right. Y so good. So so if it's a student so if it's a student is Yeah. Yeah. Uh if student is zero means he's not a student. my balance beta 0 plus beta 1 is not a student if it's a student I have beta 0 plus beta 1 income plus beta 2 student but the value of student plus beta 2 okay it's one it's getting away it's getting away you like this all right all right so so if I rearrange the terms meaning I take the beta 2 and put it with beta 0 now I have balance to beta 0 plus beta 2 plus beta 1 * what is this
(33:06) is without interaction term does the slope change? No. If I increase the the income by one unit if I increase the will the balance change will the balance change for students and no students yet because it's one income by one unit. If I change the income by one unit my balance will change by beta one. That's the interpretation we have for the coefficient.
(33:32) So that means if I don't have interaction turn what we're going to have is this. This is the two models for bed as poor students for beta poor students and no student right they just shifted slowly not changed that the way we react to the income is the same that's we don't like so let's go with interaction term um with interaction term now we have the third term beta 3 comes student so if it's zero we still have beta 0 plus beta 1 income but if it's a student now I have this term here.
(34:09) You can see here beta 3 times income. Let's rearrange the terms again. The slope changed to beta 0 plus beta 1. But the the bias the intercept changed to beta 0 plus uh beta 2. But the slope has changed too. Beta 1 plus beta 3. Therefore, the way we're going to response to the income will be different for student or non- studentent. Boom.
(34:32) See, now they have two different slopes. That means if a student gain uh increases income by 1,000, he will spend more. I mean sense to me makes sense to me. Maybe the data are not. Okay. When I was a student start making a little money, I spend them that plus more. Uh okay. So that's the idea of interaction term. I hope it's clear.
(35:01) Any questions from up there? Oh yeah, go right ahead. Actually I didn't quite well like how is it very different to the line like how the interaction is actually making the difference. See because the error response only because of this term here so if you remember before so if you remember before it was just beta 1 times in times income. Now I have now I have beta 1 plus beta 3 income.
(35:32) So if three is zero means we're saying the model is the data was going to say that I fit the data I have beta 3 to be zero that means it doesn't matter if you're a student or not your balance will always be proportional to your income but if the data tells me otherwise that means if the data tells me that the beta 3 is not zero could be negative or positive that means the the response that you get from changing the income will be different and you can see clearly here this intercept, this slope has changed.
(36:03) Now, where do I find I'm going to ask you the question because that's what I do. Where do I find beta 1, beta 2, beta 3, beta 0? You said it earlier. How do I find it? You said it. We fit. We use the data to determine the values to determine the values of the coefficients. The data will tell us if beta 3 for example is not zero.
(36:34) If beta is zero the data there's no difference between students and non students but if negative it tells us hey students are more saving more right and if beta 3 is positive it says oh students do like to spend more. The data will tell us. We don't say the data will tell us give the option to the model to do that.
(36:59) Remember these coefficients will be found will be found by fitting means we determining them from the data. Yeah. Stop becoming like a Netflix show. You get the overall picture. It's entertaining but you miss the details. That's how I feel sometimes. All right. So that's the thing. the compassion digestion time. Um, the next thing we're going to do while you're digesting the previous one, it has to go twice.
(37:29) What? One more. Okay. You digest it? Good. Let's move on. You know, I have the theory that students are like, you know, I have the theory that cows or sheep will correct even will correct me. You eat it. eat. He goes down, comes up, cross again, he goes down. How many times, Kevin? You're a farmer. Three, four times. Yeah, it's the same.
(38:00) You hear it once from me, you get into the section, you talk to your section, you talk to your hear it again, you read quizzes, you do, you do quizzes, you do homeworks, you just it takes time. Okay. All right. Okay. Again. Okay. Again. Too many predictors, colinearity, and too many interactions. to very good right now let's make it even now let's make it even more complicated all right so I interm relationship is of the input and the output the predictor st variable is not a straight line as I showed before we may have the curvy curve what should I do so here is the example so if I have this
(38:42) data The blue dots are the data. This is my best linear model. Does it look good? Right? The residuals will tell me I'm going to have residuals that they have some p. Now, what to do? We may try interaction terms, but may not work.
(39:04) What's the next thing to do? Well, we want a model that predicts something like the red line. Okay, that would be cool. So, what does this look like? What is what does this look like? What is the mathematical term that will give me this line on the right the curvy line? Polomial, right? So what is polomial? So what do we actually want is a function.
(39:32) What do we actually function f that will capture that and that function f is always parameterized by some beta. So those are the coefficients that we're going to find that we're going to find on the data by training on the data. Now let's take that and say which is the model that will give me something like that. It's a polomial right? So it's a polomial right? So we adding instead of x we add x² x cub all the way to x of m is just some number 10 10 20 whatever okay that kind of function will be able to create curvy lines of that form. Okay, cool. Now just as in the case of multilinear
(40:11) regression, I'm going to make the claim that polomial regression is a special case is a special case of linear regression. How? How Jacob go? Hold on. Let me finish this for a sec. Let me finish. What I'm claiming is that poling is that the polomial regression case of linear it is a special case of linear regression. Easy peasy. It's very easy. I put from G.
(40:54) I put from G. So how? All right. So how remember the design matrix? Remember the design matrix. Think about the design matrix we have for multi linear regress. X it was a matrix. It was a matrix. First column was first column was once you remember once first column was once you remember and then X1 you remember and then X3 X2 X3 X4. Here it is. Right.
(41:23) Here it is right which is and we have the y and a vector 2 and a vector 2 case that y should be lower case but now let's look the polomial now let's look the polomial regression so now the polomial so now the polomial put x1 I'm going to put x1 one to the power one and then I'm going to put the x to the^ of two as a column x to the R3 as a column etc etc.
(41:54) So I take my data, I take X and I'm out square it cub it square it. Now I have a matrix. First column is one. Now I have a matrix. First column is one. Second is X. Second is X square blah blah blah blah blah blah. Here we are. Here we are. So if you compare that with the multi multilinear regression, it's the same matrix. Okay. It has the same form.
(42:17) It has the same form. Now I'm going to do even now I'm going to do even the trick. the trick. So, so how do we train this? Okay, so I have my x's and I have x1 x² cube all to the xm. Here is a trick. Are you ready? Okay, easy peasy. I'm going to just call x to the power of whatever I'm going to call it predictor number k.
(42:45) You remember in the multilinear regression we have one predictor, two predictors, three predictors, four predictor. Now I have one predictor but I'm going to call X cube a different name and XQ for a different name. Now if you think about that, how do we find the coefficients? How did we find the coefficients on the multilinear regression? on the multiar regression that side anyone how can you find better multiar regression I did some remember moving things around and then I end up with a formula you remember that does anyone remember the
(43:26) formula it's imprinted into your regular right it is that formula okay so it was x transpose x inverse * X transpose Y. Doesn't matter if you remember or not the formula. I like you to remember, but you don't have to remember it. Okay. Well, take that back. You do have to remember that. Chris is looking at me.
(43:53) Don't say things like that because then they're going to come back in the midterm and say, "No, you told back in the midterm say no, you told." So, this is the formula. So, this is the formula. But remember, the only thing I have is that the big design metrics has the terms. Every column represent every column represents one polinomial term. That's it. End of the story. That's it. I square. I take my data. I square them. I put them in a column.
(44:17) I give you to skarn and everything works. So if you want to actually create that design, if you want to actually create that design matrix, skarn will give you this function we call polomial features and you give the degree degree. It's going to create that design matrix for you. So you don't have to do number two.
(44:40) Number two, once I have once I have the design matrix, I'm calling linear regression fit because it's the same model. It's the same thing that will find you the for every polomial term. There's more term. There's more one warning that some may forget. Yes. Okay. I call I call XQ. I call it X3 three tilted. I call it a new predictor and it's just one call. Yes. Easy peasy. So, and you don't even have to do it. Skarn will do it for you.
(45:25) Okay. You just say polomial features. It's going to create the design matrix. is going to have the first column is going to be one, second column is going to be x, third column will be x squar, fourth column x cub, etc. Zoe, we say whatever you feel. Actually, the next part of this lecture is to decide what that end is, right? But you just have to say because if you say three, it's going to create four columns.
(45:57) If you say five, it's going to create six column. Remember the first column is one. Okay. And wait there was a warning here. Important if you do if you ask skarn to do polomial features remember it's going to create all the interaction terms.
(46:18) So if I have let's say terms so if I have let's say m is equal to three he's going to do he's going to do the interaction actually let me back up because the statement the same if what I talk so far we have x x² x cub x but what if I have two predictors it's going to be x1 x1 squar x1 cub etc and then xy x2 x2 y x2 x2 x2 square x2 cub if I have three ones you get the picture and if I 31.
(46:44) Now if you have more than one if you have more than one predictor and you ask skarn to create polinomial features keep in mind that it's going to create interaction terms. Jacob Jacob you had a question earlier on and I forgot to come back. Okay. Yeah. So um so just keep that in mind. If you don't want interaction terms good luck is not so straightforward. You have to remove the columns by hand.
(47:12) Okay, there's as far as I know it's clear doesn't say doesn't have a flag say no interaction say interaction only but you can't say you only interact so keep that in mind uh and Kevin says in R you can do it yeah uh the second warning which is very super important which usually we make make a mistake all of us some I'm sure you do uh then pay attention to that these are little nuggets that you if you pay attention you save you some time the once you design the the matrix is going to create a matrix with the ones that means you don't need an intercept
(47:58) because your first coefficient will be the intercept okay now if you use polinomial you have the one and then you fit it with the intercept is not catastrophic. Don't worry. It's just your interpretation of the intercept. It's going to have two intercept. The one that comes from your vector beta zero and the other is going to be the intercept.
(48:21) So those two together is going to be the intercept together is the true intercept. Just put a note just put a note and I'm going to say and I'm going to say pld too long did not read. If you use polomial feature when you fit your model just say intercept false. I think I got the right keyword right. Okay. So write that down is important.
(48:45) Write that down is important to remember. All right. Now the other now the other final thing policression is the linear in terms of the coefficients not the relation between the independent and dependent variable. This is a question we usually get.
(49:03) How can you call a linear regression? Well, it is not a linear regression, but it fits into the linear regression because it's linear in the coefficients and therefore we can find those coefficients right of the bat. Now, so fitting a polomial model requires choosing a degree. Zoe asked earlier said what is M? Well, it's something we choose right now. What is a good choice of M? And how do we go about choosing M? Once we introduce a free parameter, we need to understand what it means, right? How do we choose M? Should it be two? Should it be two, five, six, seven, 24? Yes.
(49:51) I think it depends on how [Applause] to visualize. I agree with you. If I have one predictor, if you have four, five Gets confused, right? Gets confused, right? Pray. Okay. Pray. That's a good thing. We're going to pray to cross valid. We're going to pray to cross validation. Let me explain. But let me explain. Let me show. So this is degree one.
(50:38) Obviously is underfeeding. It's not good. This is degree. What's happening there? Right. What's happening there? Right bleachers there. You're in your computer. You're not paying attention. You're in your computer. You're not paying attention. Hello.
(50:56) So, what's happening on the right? So, what's happening on the right? Anyone? What is that? Anyone? What is that? What? What do we call that? Overfeeding. Thank you very much. Overfeeding. Underfeeding. Underfeeding. But there is some degree that is the right the right one and how and how we going to find that that's the the next thing. So fix scaling I'm just going to skip it because we have seen it.
(51:24) So finally too many predictors colinear and too many interaction there's a high degree of polar leads to overfitting. All right. So ant model selection leads to overfitting. These are all the students. Okay, let me go to the next one. Um um the scaling don't worry. The scaling don't worry. We have it in the previous slides.
(51:57) Oh, why did I take this out? Mod selection. Oh, by the way, I forgot to mention the beautiful photo we saw before. I apologize. I'm going to go back again. Um good. Yeah. This is good. Okay, next topic is model selection. And I'm going to uh now just because you found a model that minimize the MSE, the square error, it doesn't mean the model is good. Of course, we've seen this. You can investigate the R square too.
(53:01) And if the R square is high, you say, "Ha, I'm done." Are you? So in this case MSE here is high due to noise in the data. In these cases the MSE is high for other reasons. So just to sum up what I'm saying here MSE and R square just part of the story. Okay you need to do more.
(53:27) You need to actually look because as you see here okay the MSE is whatever number is because the data have noise. There's nothing we can do about it. But this one for example here has a throw out will throw up because you have remember the is the aggregation of the square error of every of every point. If one point is far away if one point is far far away for some reason huge square huge square error you average when you average it's going to push out MSE but everything so if I remove that there my MS would be good. Okay. uh in this case um the MS is high not because of the noise of the
(54:07) data it's because my model is not good enough and in this one uh whatever there is something strange going on in the domain that we uh merge okay so the idea here is and I think we have seen this before is to take my model that I train and I find some MSE Let's check it out. Check it out. Unseen data. There's many scenario many scenarios here.
(54:41) Your model trains well on your get a good performance training data. Then then you go into the unseen data validation test and your MSU shoots up. That means okay there is another scenario that you training data you get an MSE or some performance you go to your validation and it goes very down. that usually means you have some outlier in your training data. Okay, so that's just a thing.
(55:08) So here in this case, my uh training data has an outlier that kind of pushes the model to go up. You see this is my training points and I have one outlier. So the model said, okay, I'm going to catch it. But then when I see that unseen data, that outlier is not there. So my MSE on the NCO validation test data will be off will be different right okay so that's one scenario okay so that's one scenario there's more I want you to think more I want you to think more we want you to think more so I have this example here for budget and sales and this is the model I get
(55:45) y is equal to minus.5x + 6.2 two. Okay, it kind of fits the data. You do the visualization. Looks good. Any problem with that? And I'm going to go with that group of students now. I'm smiling at Look at that model. Tell me, are you okay with that? Yes. Go ahead. Yeah. Look at the The bulk y is equal to 0.
(56:35) 5 decreases decreases. Right? Huh? Yes. As you increase the value, your sales will decrease. Does that make sense? Exactly. So in this scenario it's to me I mean it's either the data wrong maybe we forgot something something pre-processing some normalization didn't go right it may be such that the the the TV advertisement is so bad that I know people and they say I'm not going to buy that could be a scenario but you see now I'm looking at the model trying to understand it now on the right hand side okay the the the slope is at least positive but we have a negative intercept. What negative intercept
(57:32) means? What does it mean to have negative intercept? mean I think fail means people okay I like that but it doesn't make sense that's the bottom line right so again either the data wrong or my model is wrong or we don't understand something the intuition is not right okay I'm not giving you answers but these are the kind of things you have to think once you do did fit to your model and you just don't say ah my R square is 0.9 here it is now you have to think a little more what are the coefficients telling us does it make sense okay
(58:24) all right so generalization error in general is how well the model is doing in unseen data we call that generalization error okay that's the term we're going to use now the goal of model selection is to reduce the generalization to reduce the generalization error. In other words, the goal of the selection all the selection is to reduce over. Okay, good.
(58:51) You I overfeitted you enough. Okay. So, every question I answer now, you said over fit. You're getting there. All right. So, that's the goal. So, our our goal is to find the model that will give us the smallest overfitting, right? decrease the generalization. So we're going to choose the model that will give us the best results in unseen data. It makes perfect sense to me.
(59:19) It's such a common sense, right? I have a bunch of models, different degrees of polomial, different models, neural networks, decision trees, logistic regression, whatever. Right? Which model should I use? the one that performs the best in unseen data.
(59:43) It's as simple as that, right? So model selection techniques help us to do that. So let's go into that. So before actually just before we go there one more thing, let's remind ourselves what contributes to high generalization error. What contributes to overfitting? What makes our over makes our model over fit? Let's just go through them because we see few of them.
(1:00:09) The fish space has high high dimension means I have too many predictors. We know that if I put many predictors eventually I'm going to overfeit. The polomial degree is too high. We saw it before. If I get degree 50 it will overfeit. Too many cross terms are considered. We have too many cross terms. Now I summarize this all with the following statement. We giving the model too much power.
(1:00:35) And what is the famous saying? Too much power comes with huh someone great responsibilities, right? So once we give the model too much power is going to overfeit. Okay. So that's kind of what we get. The more the complex the model is, the more the tendency to overfeit. Okay. So, what we're going to do, we're going to try to find the model that doesn't overfeit.
(1:01:12) Now, if I take away all the powers of the model, what's going to happen? It's going to underfeit. So, either underfeit or overfeit. We want something in the middle, right? Okay. There's one more that I haven't talked about. We're going to talk on Wednesday. So I haven't talked about that but it's gonna come right. So two three. All right.
(1:01:33) So you remember this thing train validation test. What do we use a train for? To train. We use this to train a model to find the coefficients to find the parameters of the model. That means minimizing the loss and with respect to the parameters and the loss to the parameters function of the loss. So it's a function of the data. So in other words, find the parameters given my data. Boom.
(1:02:00) Clear, right? Clear, right? Second validation. We use this to select the model. Okay. We're going to use a validation set to do that. And what is test? Test is only to uh is to report the model performance at day. And I have a favorite saying there's a special place in hell for people that use test data to choose the model.
(1:02:23) Do not use the test data for anything. Leave it there. Actually, the best way is to give it to me and then I give it to you at the end of the semester. Okay? And I do that with my graduate students. Believe it or not, some of my PhD students, I don't give them the test date.
(1:02:43) They have to give me the model and then I test it because there's a tendency to check on the test data, right? To do so. Leave the test on the side. Never leave the test on the side. Never ever touch only the last line of your report. model did and the model did so on so on the test day. Okay. All right. All right. So, here's model selection. Uh, okay. There's many ways to do model selection. Many. Let's start from the beginning.
(1:03:08) Exhaustive search means what? I'm going to try all possible models, train them, and once they're trained, I apply them on the validation set. Okay. Let's say you have this two models. Say you have two models. Let's say you have 10,000 models. Let's say you have 1,000. Not so easy peasy. Not so easy to train. I have to train,000 models and test validate validate each one.
(1:03:34) So exhaust exhaustive search if the model space is small. The model space is smallg is pretty cool. Oh, I forgot these people. Second one is called greedy algorithm. Greedy algorithms are greedy algorithms that every step at every step we can. We don't see the big picture but we do the best actually picture we do the best actually run our lives like kind of run our lives like that right we pretend we have pretend we have five years planned in reality we're just trying to make it trying to make it to the next week so that's called a greedy algorith so
(1:04:06) that's called a greedy al and then finetuning the hybrid parameters we're just going to get to it tuning the hybrid parameters then isization on Wednesday on all right when exhaustive sur all right you good exhaustive sur so let's talk about exhaustive circ what is exhaustive circle talk about exhaustive is let's say I have three predictors x1 x2 x3 x1 x2 x3 how many models can I make out of that think about let's think about I can have x1 I can have x1 only x2 only that's three models okay that's x1 and
(1:04:42) x2 x1 and x2 x1 and x3 or x2 and x3 I can all of them No, I have actually a model with no predictors. I have actually a model with no predictor with one predictor. I have I have models with one models with two predictors. I have two predictors. I have models with three predictors and I have models with three predictors. Eight models. I have to mod in reality.
(1:05:06) If I have reality, how many models? If I have J predictors, how many? Huh? Huh? Two. Two to the J. Two to the J. Yes. Yes. This is right. Is right. You said interaction terms. Start start including interaction terms. It's going to be bigger. But to J is already a big number, right? So, and you can prove this very nicely. Do it in the middle.
(1:05:45) I'm kidding. I said you can prove it very easily. Said two to the J. It's a nice little two to the J. It's a nice little proof. Not part of this class. Not part of this class are the ones that are the ones that say look I'm going to start somewhere.
(1:06:03) Let's say if I say if I want to test one predictor find the predictor result keep it keep it. And now only consider predictors with that one. Take the pair and then get the third. These are called greedy algorithms. Uh I think I have slides for that. So basically it's a little bit formal slides for that. So basically it's a little slides here which is fine formal but quite straightforward predictors.
(1:06:31) First I predict first I consider the predictor and I find the predictor that gives me the best validation. Keep that keep that and then I only when I do pairs then I get when I do pairs consider those then I get the pair and I only consider those. It is greedy because it's no guarantee. It is greedy because it's no guarantee it's going to give you the best results because you may not the pair may not include the one. So these are called greedy algorithms.
(1:06:55) So these are called greedy algorithms and we're going to see quite a lot of greedy algorithms and you're going to see a lot of greedy algorithms in many of your class and other ones that you see a lot greedy algorith. Okay. Uh and with degree algorithms you have only order of J² which is much much small to to the J. Okay. Good.
(1:07:24) Now let's finetune the hyperparameters now. So I show degree one under 50. I show 50. I show 50 over 50. How do I find one there? How do I find that? validization set remember so what I'm going to do so what I'm going to do I am going to do this plot choose the polomial degree find the validation validation error change the degree find the validation change it again okay so I'm going to have this plot here let's take on this plot for a sec here let's take on this plot for a Protobas I don't know why it says protobavas there should be so on
(1:08:09) the x on the x ais we have the degree okay okay 1 2 3 4 5 whatever okay five on the y ais we have on the y ais we have to choose training training and the validation the training training uh the training MSE will be going down more degrees we add more more degrees we add more complex the model better it's going to be cut is That clear? Is that clear? It's guaranteed it's guaranteed to go down or stay the same.
(1:08:42) Why? Because if I if I add an extra term, I can put the coefficient down to zero. So it gives me the same performance as the previous degree. Now I can change it up and down. But if I want, I can guarantee to be at the same level. So or the training will go down. Okay. So you see it here. Is that clear to everyone? Now the validation is a purple line.
(1:09:08) It starts with a small degree K here and what will happen? It goes down and then it shoots up. Now where it start shooting up that's the overfeeding region. This region here on the left is the underfitting because I can do better. This region here is over 50. So where I want to be right there. So if I have where is you say how I choose my my degree of polomial I make that plot I plot it and I find the degree that gives me the smallest validation error and that's model selection with validation.
(1:09:53) All right. All right. So, one one more final thing. I have five minutes because they were late. I was Yes. H2O. Yeah. Yeah. So, while we're finishing while we're finishing up and don't mind because we're going to repeat that on Wednesday, but I want to at least show it. secret work for the days too old. Right. Write down please. All right.
(1:10:53) So we're going to just go until um finish but I'll come back on Wednesday. All right. So far let's repeat what we've done. We had added interaction terms and then we went to polomial and we know how to do polomial and every step of the way we're seeing overfeeding and then we said well we're going to find the model that overfeits the list.
(1:11:16) Minimize the the generalization. That's all I said today pretty much. Okay, easy peasy. Now we found out that we can select the best model by looking at the validation set. That's all good except That's all good except one thing. I want you to look at this example carefully. Let's let's look at this example very carefully.
(1:11:48) Get out of the computer for a second. And get out of the computer for a second. And what I have I So what I have I have my training data which are which are the blue dots. I have I have my validation data which are the orange or whatever pinky thing is right. And I have three models degree one, degree two and degree three. Okay.
(1:12:10) So I fit each one of them right with my training data which are the blue dots. And now which of these model will be selected when I go to validation? This is the trickiest question I ask in this class but you can see it there. Degree one very good because it goes close to the validation there.
(1:12:40) Now what happened here? Obviously degree one is not the best model which is the best model by looking at it degree three right most likely. So what happened here? Yes. The validation is better representative of that or can I say a little bit differently? The validation data happens to be like that.
(1:13:15) Remember how do we choose the validation data? randomly and you remember lecture four I said someone here will say hey I'm not 100% sure I think it was Zoyek he was very suspicious of me right uh when I was getting the K was it you no any uh when I was choosing K someone was suspicious I'm not sure K3 is correct why because I'm not sure my validation data is correct this is what we see here we choose our validation randomly it's just going to be a random subset Now, let's say I try not all three models. I try 1,000 models.
(1:13:47) There's going to be one that is just going to be perfect for that validation set. And that's a problem. Okay. And that is called uh that is called overfeeding to the validation set. Okay. So, two overfeeding overfeed training and the other is over validation. Yes. But unlucky case is a unique and unlucky case. Very good.
(1:14:13) And can you make sure the distribution between the validation? So they said make sure your distribution is the same. How do I do that? Or what is the remedy of that? Histogram of the X. Make sure they match and do a tail diverg. Or what can I do otherwise? Yes. do it a bunch of times. And that's what we're going to be doing. So I can do it a bunch of times and average it out. Okay.
(1:14:46) In in the last 30 seconds, we're not going to do it exactly bunch of times randomly because that doesn't guarantee every training set to be every training point to be used. So we're going to do what is called cross validation which goes by and I will explain that on Wednesday. We're going to talk cross validation is what you said. Do it a bunch of time on steroids. Okay.
(1:15:10) So, wait. Don't forget the word H2O. Uh I'm 5 minutes. Five minutes behind. Five minutes behind. That's it. I'm Trisha. I emailed you on Friday about potentially