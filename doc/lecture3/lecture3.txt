109 day4 - YouTube
https://www.youtube.com/watch?v=GnVT5QFbNrk

Transcript:
(00:01) All good. All right. Test test test. I don't get anything. All right. A little bit that way. Chris, do you know how to reduce the volume up there? To reduce the volume up there. be no complaint. Oh, I think we have to end. I I can go up there and try to if you can. All right. Good morning everyone. Good morning.
(00:46) Um I feel a little bit echo. Is that true up there too? Yeah, a little bit echoing. We'll try to reduce. Okay. Uh good morning. Welcome, welcome, welcome students here up there online everywhere. Future data scientist I am going to be your lecture Pavlo Protoabas. Uh this is not the first time you see me. Some of you sign up later is the first time. All right.
(01:11) So today we're going to start with talking about modeling. A little bit change of pace. Um the first two lectures we gave you a lot to chew. Today we're going to tone it down. I'm going to be speaking a little bit slower. Uh, and I'm going to go a little bit slower today. Uh, if you heard me before, you know that I believe in ramping education.
(01:36) We start slow and every week we increase by five. It's progressive overloading. Everybody knows about exercising. So, every week we add a little bit more complexity and I'll be talking a little bit faster and we're going to add more. So, the first couple of weeks we're going to go a little bit easier. uh is just to make sure everybody is on the same page.
(01:54) Get used to my New Jersey accent and the likes. Uh right. So uh let's start. By the way, one more thing that before I start for every lecture, the background photo will be a photo that you send me. I'm going to put something on the ad and you could just put your photos there. Put it. So this is my first time and therefore I didn't have any photos from you but I have a nice photo from Asel who is one of RTF and this is cold chuck lake I believe is in Washington state.
(02:27) Uh the lights in this room are not the best. It's a beautiful picture but uh we're going to work on that later. Okay. Ready to rock and roll. Everybody settles down. All right. All right. So let's look at the outline for today. Uh this is what we're going to be covering.
(02:46) First we're going to be talking about statistical model just at the high level. I got Kevin's attention. He just turned out I said I hear statistics. Uh we're going to be talking in particular about K nearest neighbors. And then the second part we're going to be talking about model fitness. Uh we're going to talk about how do we know how the model perform in predicting and then how to choose from two different models. very basic things, very easy.
(03:12) Uh is an opportunity to ask questions as we go and to make it more fun, we have some games to play along the way. Okay. All right. So, um before we get anywhere, it will be a what I would like to do is to set the stage as here feel like dancing every now and then.
(03:39) uh is to set the stage meaning that we want to make sure we all uh speak the same language. What do we call things and so when someone is talking to me I know what they're talking so when I give lectures so you know what particular parts of the data what particular parts of the model I'm talking about. So the first thing is to set the stage as we said. All right. So now let's consider a scenario which we're trying to predict the values of one variable based on some other variables. This is an important uh distinction. This is an important thing to do. Uh so for example
(04:15) predicting the number of views that a Tik Tok video will receive next week based on factors such as video length, posting date and previous year. I think this Tik Tok deserves more views. I like it. Uh so uh here's another example. Imagine you're being uh hired by Netflix and your goal is to uh forecast which movies a Netflix user likely to rate highly considering the previous views u previous movie ratings and demographic data.
(04:51) Okay, so that's another thing we want to do. All you see we're doing prediction here and we're going to get into uh further later as the semester go. So I I want to get and concretize this example. Uh so instead of doing Tik Tok videos and Netflix movies, we're going to work with a very simple data set.
(05:14) Uh this data set is the advertising data. It contains 200 uh markets. For each market which is each row we have advertising data that they come in three forms TV, radio and TV, radio and newspapers all in 1,000 units and also we have the sales how much they sell uh in 1,000 units or whatever they're selling.
(05:46) Okay, is that clear? So uh this is my data set and I'm going to be working on that for the next couple of lectures. Okay, it's a simple data set. It has uh it has enough for us to chew on it and to work on it. Okay, now uh the problem is that many of these problems they have some asymmetry some of the variables here. So we have here four columns.
(06:12) Okay, so we have TV budget, radio budget, newspaper budget, and sales. Okay, so there's four columns. Uh, but in many of these prediction models and a lot of these, we're going to have this sort of asymmetry. Uh, the asymmetry here is going to correspond that some of the columns, some of the variables is harder to measure, are more significant or directly or indirectly influenced by other variables.
(06:40) So one of these columns here have this symmetry. Can someone guess what is that? Just shout. Is it the TV budget? No. Sales. Right. Sales has this symmetry. Is the one that I'm interested is the one I believe that directly or indirectly is influenced by the budget that we put into the advertisement. Is that clear? Yep.
(07:10) So we're going to classify our variables into two categories. The first one are variables whose values we aim to predict. Okay, sales for example and the v the variables used as an inputs to inform our prediction. These are the variables I'm going to use to predict the other. Easy peasy. Yeah. All right. Now let's look at the data.
(07:34) So I have four columns as I said on the right here on the Y the coolest pointer at Harvard. Okay. Uh so on the right we have prediction which is uh the Y is the one is the target is what I'm trying to predict and I'm going to be calling it the response variable the dependent variable. So I'm going to stick as much as possible to response variable.
(07:59) It is good to have consistent inductions. But if you read somewhere and they call it the dependent variable or the target, that's fine. It means the same one. Okay. Now, I have on the the other three columns that those are the variables I'm going to use to predict the sales and we're going to be calling the predictors, features, if you're having coariants or independent variables. All mean the same. It depends where you're coming from.
(08:26) And in these lectures I'm gonna be doing, I'm gonna try to stick to predictors, but very often we use the word features. Okay. All right. So, and I think Kevin likes coariants. No, no predtor. Predictors. Good. All right. So, we're together here. All right. So, uh, so here is this. Now, let's formalize it one step further. So far, everybody should be okay, right? Shake your head.
(08:50) Give me some smiles back there. Yes. Okay. All right. I need I need some feedback otherwise I don't know am I talking to the walls or right? All right. So the next thing let's formalize it a little bit more. So now we're putting a little bit of math. So what do we have? We have on the y uh instead of uh uh uh instead of y just y we're going to say y is a vector and the vector represents uh elements.
(09:20) Each element will represent one of these markets y1 y2 y3. All the y all the way to y where n I find it annoying. How about now? A little bit less echo. Can you hear me? Yeah. Okay. Foo foo testing. One, two, one, two. Is that good? Is it echoy up there? It shouldn't be a quick uh all right Chris just adjusted if you I think it's good now right okay let's continue so instead of just say y I'm going to call the y a vector vector has elements and it goes from y1 to yn where each one of them represents one of the rows in my data or one of the markets as I said or one of the observations. So if you notice here I have n observation every
(10:21) row in the data is an observation and every column is a predictor right so we have n observations 200 markets and p predictors or features okay is that clear to every okay so now I'm represented as a vector uh and of course my x equally now I have p vectors for every column I have a vector and every and every column Uhhuh.
(10:52) And every column X1 will have n elements. Okay. So therefore X is a it has columns and row. X is a matrix. Right? Of course. Okay. I'm going to be calling it the design matrix or the data matrix in a second. Here it is. This is coex also known as the design matrix. Sometimes you hear the word data matrix. This is a slight subtle difference.
(11:13) Data matrix is the data that you get. Design metrics is usually what I'd at least refer is something is from the data you do some dump processing some addition some uh feature extraction etc. Okay. And we'll see in a second I mean next on Wednesday that I'm going to add something to the X in order to make it work better for me and I'm going to call it design matrix.
(11:38) But there it doesn't matter. Call it design matrix. Call it data matrix. That's fine. X is a matrix. It has n rows and p columns. Cool. Okay. All right. So, and of course the response variable will have n elements. So, couple of other agreements we should have. Uh I'm going to try uh capital letters will mean matrices and lowercase letters will mean vectors. Okay.
(12:06) So again, we're trying to be consistent. If we're not, let us know. We fix it. Okay. It's hard to be consistent through every lecture because sometimes the lecture notes are came from a different course or we added it before or something but we're trying to be consistent. You should also try to be consistent.
(12:26) It's very very good idea to be consistent your notation and your nomenclature. All right. Now let's connect it back to pandas that Chris was talking last Wednesday. So if I do if I go to skarn or pandas and I do x do.shape shape I'm going to get the dimensions n comma p because x is symmetric.
(12:52) So if you connect it to the shape by the way dot shape look at it is going to be your best friend go moving forward as a future data scientist or AI or ML engineer dotshape is the command you're going to use the most and it's going to save you so many times. It's a question of stretching. Okay. Um, so if I do y dot shape on the y, it's going to give me either n or n comma one.
(13:15) What's the difference between these two? All right, I'm going to get to that. Don't worry. So again, if I have only one predictor and I do dot shape, I'm going to do n comma nothing or n comma one. What's the difference? I'm not going to ask Chris. Chris knows the answer. So is K. anyone. How do I get to the first one and how do I get to the second? So, this class we're going to go back and forth between implementation and theory and math and intuition. So, sometimes I'll get this kind of questions.
(13:51) What's the diff? I think it has to do with if it's a one list or a 2D. Okay. So, it has to do with the dimension. This the name is Panos. Panos. Yes. Uh, okay. Pano says it depends on the dimension. If it's uh if it's a vector or in pandas we call it what series it will do n comma nothing. Uh but if it's an array it's going to be n comma one because an array has two dimensions right a series has one dimension. Okay slight difference is important all good.
(14:32) All right, we have a game. I told you we're going to have games today. The game is the following. Who wants to be a data scientist? Of course. Let me explain the rules so you know how to play. First, if you volunteer, raise your hand to be the conttor, you get, if you get it right, you get one attendance for free.
(14:56) Okay? So, uh, so one day you don't have to come to class. I don't know why would you don't want to come to class. is fun. You get to meet us and of course your peers, your colleagues, your crushes or whatever. You come to class to meet them. But if you want, you can sleep one day much later. So, uh the rules are the following. You're going to put the question, you volunteer, you have to introduce yourself.
(15:20) I said the first couple of lectures we go a little bit slower to relax, right? To get used to things. uh and then you have to answer the question or use the audience or call a friend assuming you have a friend. So that's the assumption I'm making here. Okay. So who let me put the question up and see who wants to do it. So it is panda's review.
(15:45) Who was paying attention last Wednesday when Chris was talking? Me. Okay. Which of the statement below is correct? DF double bracket X or DF single bracket X. I'm looking for volunteer. The music creates a All right. You hold on hold on. We have I'll do it. Yeah. Yeah. Yeah. Your name, which school you are, what house, your hobbies, everything here.
(16:18) Hi, Jacob. Um, sophomore college in Adam's house. Adam's house. Is that the one that's under construction? It's done now. It's done. Good. Good for you. But it's only one of the one or the other. Okay. So, the one on the right. All right. So, Adam, you any hobbies you have? Oh, yeah. Um, I ski.
(16:39) You ski? Yeah. Downhill? Yes. Where? Everywhere. Okay. If you ever go to Canon Mountain, ping me. I'm usually there in the winter. Okay, ready. Jason, you said Jacob. Jacob. Jacob, you ready for your extra coin? You see there's a coin there. Uh, we're not giving physical coins. It's more like bitcoins. So, you can exchange it for one late attendance. Okay.
(17:06) Trump coin. Trump coin. Did you say Bitcoin? Did you say hundreds of thousands of dollars? What do you call it? Trump coin. You don't use that in front of All right. options. A df's double door returns a panda series uh object where df returns a data frame. DF is invalid operation.
(17:30) DF returns a data frame where DX returns a series and DF is invalid operation. Jacob. Okay, this is opportunity for everybody to think. I mean it's not just a game of course is pedagogically very deep things. I think I'm leaning towards C. Do you want to use the audience? No, you don't trust him. Is your final answer C? I think so.
(17:59) Are you sure? Not 100%, but okay. You don't want to use the audience? No. All right. All right. He doesn't trust you guys. All right. Let's see if is Jacob correct or not. And Jacob is correct. Yes. And Jacob, your last name. So we keep track of your love. Gross. Gross. Okay. Thank you. I'm sure I will remember. All right. So that was just for fun. So now let's go into what we call statistical model.
(18:28) That's where we you're here today. Um so statistical model. Um so we're going to start with the following statement saying that imagine and again I'm keeping it light. Don't worry, by the end of the semester, you're going to regret these days uh that you're going to miss them. Okay? So, imagine that you're having uh an ice cream cone. So, perfect. Everybody likes ice cream here.
(18:56) Yes. Very quiet. Do you guys don't like ice cream? So, I shouldn't bring ice cream. I understand. Okay. You don't like ice. She doesn't like ice cream. All right. So, it is the perfect ice cream, right? It's just perfect. Now, in reality, we can't just get the perfect ice cream. We have a lot of flavors. We're going to combine them.
(19:15) We're going to choose. Give me a little bit of that. And so, in reality, we don't have the perfect ice cream. So, let's translate this. This is what the statistical model is. In some ways, it's trying to find the mixture of flavors that will give you that ideal ice cream that nobody ever tried.
(19:34) Right? We have this idea that there is one combination of flavors that would make the perfect ice cream and we try until we get there. The same the same idea applies here. So let's now get a little more serious. So let's imagine there is some relation between X and Y. The perfect ice cream, the perfect relation.
(19:55) This is what generates Y given X. I'm going to call that f ofx. F is this function that represents the ideal connection between X and Y. Okay, ideal I say. Now what is this epsilon here? There is a Greek letter epsilon banos. You for sure you know what it is, right? Uh so what is that epsilon and why do we have it here? Now the epsilon will represent anything that makes the relation between the actual y we measure and the f of x not to match identically. Okay.
(20:37) So what what could be the reason to have that epsilon is noise. It's stoasticity. What could be that epsilon is there? Any ideas? Yeah, it doesn't. But what can generate? What are the reason? You can do my favorite emotion. Anything anything like I don't know interference with your measurement. Interference of the measurement. So hasticity all kind.
(21:16) If you make measurements with people, people are sort of random. They sometimes answer random. If you measure the temperature of the floor, there's going to be some measurement errors. The universe is full of stoasticity, right? We don't have a perfect word, right? Nothing is exactly perfect except if we do math, but we're not doing math here, right? So let's call this epsilon the catch it all.
(21:40) It catches everything that makes y not to be exactly as fx. We'll deal with that later. But for now, let's say we're not we don't know what it is. Just put it there. Okay. Under the so statistical model or modeling in general, I call it statistical to obsuse my statistician here. In general, modeling is the following. What is modeling? It is our attempt to find this f.
(22:06) Whatever we do to find this f, to find the relationship between x and y, to find this perfect ice cream, we're never going to get there most likely, but it is our attempt to get this f ofx, right? And we're just going to put oh, sorry. And we're going to put the hat on top of the f in order to make sure that we recognize it from the ideal one.
(22:31) All clear have on the bleaches there. Yeah, here and here. Let me see your faces. It's hard to see you, but I can see a little. All right, so this is what is a statistical. Let's try to find this f. All right, at this point I would like to just make some clarification. There's two types of models we're going to be doing.
(22:56) First one is called prediction versus estimation. So we have inference problems and what are inference problems? The primary focus is obtaining the f of hat is to understand actually f of hat right. Um the objective is to understand the form and the characteristics more like you're a detective here and I put it here because I know Kevin is very hot on these things and he's going to as later lecture he's going to talk more about interpolarability and understanding of the f okay the other side of the coin is what's called prediction problems prediction problems you only care about accuracy I don't care what f form is I
(23:39) don't care what it means. I don't care anything. I want to have good results. I want to make money, baby. So I don't care what the f means as long as I get good predictions. Okay. Um so the objective is to minimize the difference between predicted values and the observed values. So these are two different ways of doing right.
(24:00) Uh we're going to get more and more into that. Now on the left if inference problems usually are parametric models few interparable parameters. So examples for that will be linear regressions and logistic regression. We cover that in depth.
(24:21) And on the other hand we have prediction problems which are not usually nonparametric but there's parametric models that they are prediction models. For example, if you take 109B, we're going to do a lot of neural networks. And neural networks, they are parametric. They have parameters, but I wouldn't call them inference models. Okay. So, I said K nearest neighbors that we're going to cover today.
(24:40) Random force we're going to cover at the end of this semester and neural networks we're going to be covering next semester or you have used. Okay. So, these are the two different things that I want to emphasize at this point. uh some of these ideas will no this idea will come again and again and again I just wanted to bring it up there and ask the question in this lecture we're talking about K nearest neighbors and you may which is a prediction and not parametric and you may wonder why do I start with KN&N and the simple reason is because it's easier to understand
(25:12) doesn't mean we're stopping here huh by Wednesday I'm start talking about linear regression multilinear polomial regression and then we're going to keep going into parametric models until November. Okay. All right. So, let's go back to our goal.
(25:33) The our goal is to predict sales, right? Uh so, I want to build the model to predict sales based on TV budget. Why I only care about TV budget? Because I wanted to have things simple so I can explain them. And of course, then we're going to include the others. Okay? Starting with one predictor is easier easier to visualize and the likes. So the response variable y is the sales as we see and the predictor here x x you see became small lowerase y because it's a vector okay uh and I give you a bunch okay so there's a bunch of data I plot them there and again for the case of simplicity I'm just going to show few of them right just because real estate
(26:18) is more expensive than real estate. You should know that. So we should make sure. So I'm only choosing eight points just to demonstrate the point. So on the yaxis we have the TV budget in thousands of dollars and on the y axis we have sales in thousand of units. Okay. Now what is the goal here? The goal is to predict the egg the y the sales as some particular value of x. Okay. That's what we're trying to do here.
(26:47) Give me how much money you're going to put in the TV budget and I'll tell you how much sales to expect. Okay, cool. Yep. All right. Or here, right? What is the value of y or here? Right. So, so all this one. So, this is the gold. Okay. Now, ready a new game. Okay.
(27:14) I'm going to So, some of these games is just to give you a chance to think, right? So the question, who wants to play this time? Jacob won late day attendance day. Sorry, I have habits from last year. Anyone? All right, my friend. He's an Arsenal fan. And what do we call the game that Arsenal is playing? What do we call the game that Arsenal is playing? What's the name of the sport? Oh, football. Football. Thank you very much. You're my favorite.
(27:47) Now, this is a dispute here. Okay. Uh, your name. Uh, turn it on. There's a little button at the bottom. I I can do it. Hello. Yeah. My name is Yaya. I'm a junior at the college in Quincy House. Quincy. Yaya. Yeah. Okay. Yaya in Greek means grandmother, but that's cool. Uh, okay. Yeah. Yeah, I like it. Uh, any hobbies besides follow following Arsenal.
(28:20) No, Arsenal, I'd say, is my main. Okay, good. We won the game last weekend. I know. I know. I've seen it. Yeah. All right. So, here's the question. Uh, choose for the models. Which model would you use? Actually, there's multiple correct answers here, but I want to see your reasoning more than anything else. Okay. All right.
(28:45) So the first one is which of the following five methods could you be used to predict the value given X? I want everyone to be thinking I want it to be more interactive than just finding the right answer. So the first option is you utilize a convolutional neuron network. Second is use a linear regression model with slope three and intercept two.
(29:07) Third one, identify the examples that closely resembles the input data point and D. Consult a TF during office hours for the answer. And I think I have I think I have a fifth one here. Yeah. And the fire calculate the average value of Y from available data points. Okay. Yay. Yeah. So sort of um talking you through my line of reasoning. Yeah. I don't think that it's the first option. It's not the first option.
(29:37) Yeah, that seems too complicated and it would be very good. I like Yeah. Yeah. Yeah. Um I think option B makes sense because Do you want to see the data again? Maybe. Yeah. Or is it maybe that the slope and the intercept? Slope too. Slope is this divide by this. Yeah. Yeah. So never mind. And I changed what I said. I changed. Okay. Good. Good. All right.
(30:07) Uh, so B is out. A is out. How about C? C makes a lot of sense. C makes sense. Good. Dense. D, of course, is always an option. But, but you know, we're in this class to learn, so we shouldn't be Who is this kid? Oh my god, you're the best student ever. Love you. And E. and E is good, but I'd say C is better because the issue with E is that um like we can clearly see that it varies based on the X. So, okay, just by calculating the average would mean that we're sort of ignoring.
(30:42) We all agree with Yaya. Clap if you agree with Yaya. Clap if you don't agree with Yaya. Good. All right. So, you said C is most likely the correct answer, right? Okay. So very good. Yeah. Yeah. You got E could work too. And that's there's a point I'm making uh about that. Thank you very much. You can skip a day. I don't know why, but you can skip it. All right.
(31:13) So, um let's move on. So, the reason I put E there is because I want to create a very simple model. The simplest model that can I can I could think of, right? So the simplest model you could think of it is the following. You just take all the data, you average it and you give an answer.
(31:38) We do that all the time, right? If I tell you something, what is the grades of uh students at Quincy? You just give me a number, right? Is the average grade, right? Is not depending on the year da da da. Of course, that's more correct, but we do often get the average, the mean, right? and we just use it. Is a bad model? It's not incorrect, but it's just too naive, right? But I want to set it.
(32:03) This is the this is our base model, right? And why am I doing this? It's going to come later. But also, I want you to get into this habit, the habit of let's start with an most simple model and get better, get better, get better. Right? understand the simplest model and now we say how can we make it better boom how can we make it better boom okay and at each step we understand what we do so this should be easy to understand all clear to everyone right good now we're going to talk about k nearest neighbor which is what the is option C is find
(32:40) the other examples that is similar to the one I'm trying to predict right let me explain this so if do we have any doctors in the No, good. I can make fun of them. Oh, one. Damn it. Besides you, if you go to the doctor and you said, "I have a tummy ache." Uh, the doctors don't ask many questions. They do, but the first thing they think is like, "Oh, this week we have 10 patients with tummy aches, and it's because they all ate from some food trap, right?" Yep. Good. It's not wrong.
(33:17) It's just a way of thinking, right? We're not running a model there to figure out I mean if that doesn't work then we do some test etc etc good doctors of course like our uh friend here your name Alex will do you do the test anyways but in general what we do is just find out the nearest neighbor the most close examples we have in our memory right okay so that's what we do so we just say okay so this person has tummyach I saw 10 people with tam eggs this is what I concluded that just start with that right and that's the idea of 10 nearest neighbors and let's look at this I want to predict the y at some value of x
(33:57) right I have my examples my green examples that's my data right my training data and what do I ask which other of my examples in my training data has similar x okay so I measure the distance between my x and all the other examples I have those vertic Horizontal lines represent the distance and then I choose the one that is the closest which is going to be that one. Right? Correct.
(34:26) Okay. So that's the one and calculate the distance and say okay that's the closest example I have. What is the y for that example? It is something 12.7 and that will be my answer. Okay. Oops. Sorry. Is that clear? Simple. Extremely simple. Right. I just find the nearest neighbors and I predict it. Okay.
(34:50) So now I can do it for every X. Every possible X I find the nearest neighbors and I predict that. So if I do it for every X here I start from X0 whatever and then I go until 250. Remember when you do that you don't need you don't have to use your training data X's. You just what's what function are we using to get a bunch of X's lin space NP list or array.
(35:21) So you get a bunch of X's and for each one of them I am predicting the Y given what's the nearest neighbors. Okay. And the orange line is my prediction. Cool. Easy peasy. Yeah. Any questions? Any doubts? Is the pace good? Am I too fast? Too slow, too slow, right? Yeah. Okay. Now, let's move on.
(35:50) And some of you are already thinking, okay, actually I'm going to ask you and I give you 10 seconds. This is the f the first model. We take the average. The next one, we get the one nearest neighbor. What's the next one? Neighbor. K nearest. Let's take two, three, four. Right. Good. Jacob, you're on the road. So instead of using one nearest neighbor, let's take two of them and average them. Is that making sense? Yes. So that's what I'm going to do here.
(36:14) I take the three nearest neighbors. I average the values of y's of these three nearest neighbors and that will be my prediction. Let's see. I average them and I get the prediction based on the y. Okay. So the star is the average of the y's of this point, this point and this point. Good. All right. Okay. So now K.
(36:39) What is should be the K? All right. Let's try a different case. Here I have K is equal to 1 is the red line. K= to three is the orange dash line and K= to 8 is the K is equal to 8. So we have options here. We can do K2 5 25 whatever. Right? In this case, K8 is the highest because I only have eight points.
(37:10) So, the highest number of neighbors will be everything, right? And you notice K8 gives me back the the average the dumb model, right? The base model. Okay. All right. That's so far so good, right? Now, I can put all the data back. Why not? I mean, I just only have eight points just for uh simplicity, but I can put all the data back here. And I can do K1 10 and 70 in this case. Right? Okay. Let's summarize what is K nearest neighbors. K nearest neighbors is a decision making uh by similar examples.
(37:43) Is nonparametric learning algorithm. We don't have parameters. We just get the data and average them. Uh we're going to get back the comparison between nonparametric parametric in few more lectures. But for now it should be clear.
(38:03) uh it doesn't assume fixed form of the relationship between input and the output in simple words only rely on the data by the way I really rock on the gifs okay I have the best gifs ever I spend more time on the gifs than I think uh so we'll be looking at examples that we have parametric next next lecture okay uh the next is the is a keras algorithm here is a slide that I have the math uh we don't need this.
(38:29) I mean it just there's some people like myself I speak math better I speak English I actually more fluent in math than English some of you may be like that but this is the math you just start with a bunch of vectors uh find the k number observation in most similar these are my nearest neighbors and then I average over that okay nothing things but it's good to have the reference slide and for some of us we we kind of understand this better all right so Now K we call it a hybrid parameter and I'll explain in a sec. It's not a
(39:05) parameter. It doesn't change the form of the relationship but different case are kind of different models. So we call it a hybrid parameter and we're going to be choosing over that. So nonparametric model doesn't mean it doesn't have hybrid parameters and K here is a hybrid parameter of the mode. Okay.
(39:25) Okay. Now, that was part one. I have part two into the lectures while I'm switching between one and the other. Uh, you can stretch. Get up. It's a good idea to stretch. Yes. Uh, and we're going to put the the leg the word of the day. I really recommend to stretch. Stand up because it's good to stretch.
(39:49) Don't just stay there. Are you on Zoom? Are you on Zoom? Is it good? I think you're shorting me out. Uh, maybe. I don't know. Something just happened. Hold on. My camera just dropped. Hold on one second. My camera just turned off. He shorted. Hang on. You tripped the circuit. It must be the circuit, right? Okay.
(41:03) And it doesn't have the little Okay, I think I'm back up now. Okay. All right. Welcome back. We start part B. Uh now we're going to be talking about error evaluation and model comparison. Uh again the the background figure is from one of the TFS Daniel Moore and this is Sao Paulo Brazil like a beautiful building. I want to go there. Any Brazilian in the room? No.
(41:32) Anybody has been to Sao Paulo? Can you confirm is there or? Yeah. Okay. Good. All right. So um let's move on. So now we're going to move to the second part which is error evaluation and model comparison. First part how we evaluate our model. So the meme here it says simple use labels. Don't just put in your home any any any figure without labels. He gives he gives us the heavily gyps. Okay.
(42:04) So when I see it a figure with no labels I get very annoyed. Okay. Put labels. That's what the meme is. Okay. Now we're we were here right and of course we have choice of K the hyperparameter K and the question is uh which one shall we use and of course I'm going to play a games anybody who wants to play the game for a free attendance right it's on already be careful what you say your name.
(42:38) Hi, my name is Zara. I'm a junior in Quincy House, I guess. Quincy, we have two Queen. No. Was it Quincy House back there? No. Yeah. Yeah. Quincy. Two Quincy. The Quincy's here is winning. All right. And any hobbies, Zara? Coming to class. Yes. Yes. Of course. Pride too much. Right. Uh, by the way, if has anybody figured out where the bathrooms in this building are? Where basement? Okay, good. I know. All right.
(43:13) Which model do you think is the best? Zara. Yeah. Options. K= 1, K= 10, and K= to 70. I think that's the only three I have. Let me make sure. Don't reveal the answer. No, I have K= to 15. And there is a fifth option. I said that's my favorite emoji. Uh, okay. So, what do you think? You want to see the plot again or can I see it again? Yes, of course.
(43:50) The suspense is killing us. I am going to go for the last option because it would depend on the data. Um well, it's for this data like for the different points you don't I feel like they would need to be there's not a specific answer for what is best. I think what is best be arbitrary. So I'm going to go with there is no right answer.
(44:23) Okay, I'll summarize. Zara said it depends on the data. I said no for this data and they say well it depends. There's no is ambiguity here. What's the ambiguity? What you're not sure about it would one would way too much variation. One will have too much variation. I think for 10 it would get a little bit more specific but still yes um and then for 7D it seems to just average out which may simplify that's not so good. Yes.
(45:10) Um from these three models or for based on our question we want to answer. Yeah. Um we would have to examine that and other different factors. Okay. So you there is right in some way that look I you put me on the spot I don't want to give a specific I need to see more which is actually not bad all right let's play let's ask the audience um who thinks k is equal to one is the best model clap uh how about k is equal to 10 and what are the other options I have I forgot uh 15 and 70.
(45:50) And who agrees with Sarah? Sarah. Come on. Sarah. Sarah is right because I trick you. We didn't define what you mean best. Is it looking better? Is it smells better? Is it faster to compute? What do I mean by best? Thank you very much. You got your attendance. All right. So, in order to answer the question most precisely, we need to know more. That's what Zara said.
(46:29) I need to understand more. And one of the things you want to know is what do we mean by best? And how do what do we mean by best? Okay. So, here's my data and I'm going to feed a model. I'm going to have a model and I'm going to decide if it's best or not. But before we do that, we're going to do something which is kind of confusing.
(46:49) We're going to take some part of the data. I withhold it. And this part of the data, I'm not going to use it to make prediction. Okay? And this split is called train validation test split. Okay? Pay attention because I changed it from last year just to less confused. So the train set is what we use to make the prediction.
(47:14) Take the nearest neighbor in this example right on the train set. The validation set is what we're going to use to choose between different case and the test we just leave it on the side. We only use it at the very end to report performance of the model.
(47:38) And I keep saying there's a special place on in hell for people that use the test to choose a model. Remember that words. Okay? Do not use your test for choosing the model. Okay? That's it. We're going to use validation. All right? So there's a little class clarification here back tied it back to code. If you do this and you try to do first of all the way we choose that is randomly.
(48:04) We randomly choose some of the data to be the train and some of them to be the validation and some to be the test. What do I mean by randomly? Anyone? Someone else? Not J here. This I said I'm selecting some data randomly. What do I mean? Simple simple question. Don't overthink it. Yeah. It just takes some time. All right. So if you use sklearn, we only get train test split.
(48:42) That's the only library function that skarn does. So it is called train test split I believe. So it means that if I want to split three things, I have to do it twice. We do train split and then train split on the train or on the test. Okay. So then I have three things. Okay. Now here we are. We have our data.
(49:05) I'm going to uh estimate the y on the data points in the training set. That's what I got here. Now um this is my training for k equal to one. Okay. This is my model, right? K is equal to one. So now I'm going to go into the validation data which are shown in purple po points here. And I want to evaluate my error on my model on this data set. I want ideas.
(49:30) How do I measure the error? All right. If you answer, I give you an attempt. Yes. I get you going now. Okay. So let's start by building that. Yeah. So the first thing we're going to calculate is the distance between the prediction and the purple point. Okay. So that's one and we go this the residual. Residual is the difference between the true and the predicted. Okay.
(50:09) So that's vertical points vertical bars. You see this is the difference between the prediction and the actual actual the actual value and the prediction. Okay. Cool. The next thing I'm going to do either I'm going to average the sums or the squares. Why don't I average the residuals? Positive or negative? Because some of them will be positive, some will be negative and if they sum them up they may come to zero and I think I have no error but in reality do.
(50:43) So we're going to always consider any deviation positive or negative to be an error. Yes. Question. Why do you tend to do statistics on the squared residuals and not the actual residuals or the absolute residuals? Why? Sorry. Why would you choose to use the square residuals instead of absolute? A very good question. There's two types of professor good ones and awesome.
(51:02) The awesomes anticipate your question is in the next slide. Okay, it's a very good question. So to to quantify the performance, we first going to aggregate the errors. This aggregate value is commonly referred as the loss error or the objective or the cost function. Okay. So they has multiple names. What do you mean aggregate? So I'm going to take the squares and sum them up. Okay.
(51:25) So that's one which is called MSE. I love to say MSE. I don't know. It's my favorite loss function. I call it MSE. Sounds good. Sounds cool. Sounds like I'm smart. Uh and that's the MSE. we're going to be using it. Now a little note there there is different terminologies cause loss whatever error I use the term loss just to be simple uh in practical loss is the order of individual point and cost is the average but let's use loss okay all of us just to be clear all right so now I say MAC is not only is not the only
(52:06) valid or necessarily the best loss function for all scenarios depending of what you're doing. So we can use max absolute error, mean absolute error or hubber loss depending on your data, depending of your objective. If you for example calculate the prices of houses, maybe the best uh loss you want to do is the absolute value because is dollars or lo of the absolute value. Okay.
(52:35) Uh if you're for most of the problems we'll be doing, we're using MSE. And the question is why MSE? Why am we're so stuck? And I said we'll motivate MSE when we introduce probabilistic model. We're going to show you that actually MSE is not just a convenient loss function, but if you assume your data continues and you assume central limit theory, then the the the noise distribution is Gaussian and this is the best mode.
(53:01) Okay. We have that about lecture seven. We have the whole proof. Okay. All right. So sometimes we use the root mean square which is the square root of the MSE. All right. Now we have something that tell us what is best. So we're going to do the same exercise. Now I have on the x-axis the k nearest neighbors the k and the y axis I have the validation mess.
(53:28) Which is the best model? Now and there's no doubt Zara which one which is K= right she's very suspicious of me very suspicious of me K is equal to three right is the lowest validation MS yes you have any doubts not at all anybody has any doubts I The answer is correct. T is equal to three. We're happy.
(54:05) But maybe we should just spend at least 30 seconds, 10 seconds to 30 seconds to think if we have any doubts. It's good to do that. Anybody has any doubts? Something that doesn't sit well with upper level. So even though P= 3 gives you the lowest MS, um you also see that it has the highest variability in terms of the next closest points. So because you don't know the full training data set, you're not necessarily sure whether you know you have a radical increase in the loss when you switch a train as opposed to you say something like K is equal to 8 where you have a much more smooth loss
(54:55) function. So you can maybe be more confident that you know the loss you see is going to abruptly be the loss you expect to get into the train. All right, excellent answer. I'm going to summarize your name. Michael. Michael, I'm going to summarize and if you don't agree with your summary, please raise your hand. But it's it's there's two things here. Michael make very good points.
(55:18) But one thing just to make it intuitive is the following. I said we choose our training set randomly. You remember? The other question is if I do it again another random will I get the same K uh same MSE right so what Michael is saying is that maybe we should do it multiple times and see which one holds sort of right so the variation in the model may be higher on the case equal to three than for k= 28 and he's right smaller the k is more sensitive you are to your nearest neighbor is only three. Eight is eight. Even if you reshuffle things, you're
(55:58) going to get much more robust answer. Now, this is not important now, but we're going to be getting into this kind of questions. Looking at the variance of the model is going to be important, and we're going to be talking about overfeitting and everything, but I wanted to bring it up. Good summary. Good. I got the thumbs up. All right.
(56:20) So, now the question is, which of the model do you think is the best now? And I'm going to just say k= 3. But why don't we experiment with different train test sets? That's the same thing I did. So Zera was her intuition was good. She said yes, but she couldn't trust it fully and she couldn't articulate. But now that's what you had in mind, right? Yes. All right. So first part done.
(56:41) We know how to compare and find the best model. What's left? Okay. The example I like to give is the following. So let's say I have K equal to three. the MSE comes to be five and then K is equal to seven is worse. K7 and eight is worse. Okay, we have the model with the best K. So let give you the analogy.
(57:02) If I take the teaching team, all of us and I say I'm the best basketball player. Go with me. Okay. All right. I'm the best basketball player. I'm on the teaching, right? Um would you hire me in the NBA? Would you can you give me the million-dollar contract with the Boston Celtics? I see people are still doubting. No, you're not going to hire me for NBA. Look at me.
(57:35) I'm not an NBA material for the love of God. Okay. But so what I'm trying to say is being the best doesn't make me to be an NBA caliber player. Okay? So there's two different things here. one, which model is the best and the second one is the mo is the best model good enough, right? And let me put it another way.
(57:56) You're the CEO of a company. You hire me as a consultant. I give you a model that has MSE5. Would you give me my money that we agreed $20,000 to make this model? Would you or would you not give me my money? M5. MC5 is pretty good. Can it too excited? Anyone here? Would you give me the money? No. Why not? Suspicious. I look suspicious. I get it. I do get that.
(58:33) I never for control. I do have this suspicious looking. Anyone? Would you give me the money? You don't trust me. Okay. Why? Kind of nice. Not nice enough. Not nice enough. You're good. It's just a It's a single data point. You don't know how it compares to any other.
(58:59) Every other model I did was higher than five. What does five mean? What does five mean? Thank you very much. MS5. I'm selling you smoke mirrors here. I said they see five baby is good. And he said, "What does five mean? I can change the units. Instead of a thousand, a thousand sales, I can just do 10,000.
(59:28) " And what will happen to the MSE? We go down. So I can just sell snake oil here to you. Right? You have to be careful. Correct? What does it mean? So how shall we go about it in order to report the performance of the model that is actually Miss meaning. So I said I'm the best basketball player.
(59:55) What's would be a question that you'll ask before you sign me in your team? The error number. The error number five. [Music] Forget the error. So you hire me to work in your team and I can tell you I'm the best in the teaching stuff. What's would be your next question? How do you compare to LeBron? How you compare to I don't know Joic if you're as good as LeBron or Joic or whatever I'll hire you. Did you play oneonone? Huh? Giannis.
(1:00:25) I'm trying not to be very I'm trying not to be biased or Yiannis for that matter. Right. So Banos will ask me, have you ever played one- on-one with Yanis? And I said yes. What was the score? 100 to zero Yiannis. And he said and he was laughing. Right. But if I tell you, hey, I played with Yiannis, it was 12 of 12. Yan Panos, you hire me, right? Yes.
(1:00:46) Good. So that's the idea. We need to have some comparison models. And before I show you the slide, you remember I start with the dumb model. I called it dumb. I heard his feelings, but that's okay. Uh that model is the worst, right? Right. Is like the bottom. And the best model will be perfect prediction. So, if I put that the bottom and this one, put this zero and put this one, we're golden, baby.
(1:01:15) I have a measure that tells me I'm LeBron or I'm uh I don't want to name names or Pavlo. Okay, so let's see. Okay. Uh All right. So, that's what we're going to do here is the same things. Uh R square is our friend. Let's talk about R square. So, we have the worst possible model, which is the average model, and we have the best possible model, which is perfect prediction, right? I know we're never going to get there, but if we're close to that, it's perfect. So, I'm going to do the following formula. R
(1:01:51) square, which is where is it? Here on the bottom. So it's one minus the sum of the squares of my model minus the predictions. I should turn them around but it doesn't matter. It's square, right? So it's the difference between my prediction and the true. So if it's a perfect model, this one here, let me point with my thing. This one here will be zero. 1 - 0 1.
(1:02:25) So if I have the perfect model will give me R square of uno one. Okay. If my model is equivalent to the average model which I have as Y bar here this the numerator and then the numerator will be the same. That means the ratio is one one minus one zero. So if the model is as bad as the worst model it gets zero. If it's as good as the thing it gets one. Okay.
(1:02:55) So I have a note there. Uh the following things I got this question since R square it is not the square just no keep keep in mind R square is not the square of the quantity and therefore can't be negative. How is R square square of a number should not be negative just put the point there is not a square of the number that's the formula there but R square could be negative and when sounds sounds wrong how can you be negative well that means this number here is bigger than one for that to be bigger than one it means my model is
(1:03:37) worse than the average model. Yeah, if you make a mistake in your code, you're going to get negative R square. Or if you uh if your model is trained on the training and we go to validation and so happen we do much worse in the validation than the average model, which could happen, your R square will be negative. Okay.
(1:04:02) Uh I'm going to skip that because I want to finish how much time I have. Actually I can come back but but I want to finish these slides. Okay. So let's talk about highdimensional KN&N. By the way the secret word for today is earth. Okay. Just don't forget to put it. I give you. So I'm going to repeat it because I know half of you are not paying attention to what I say.
(1:04:31) The secret word for attendance today is earth. Come back to earth and write it down. Don't come to me and say I didn't hear you professor. I'm going to say again the secret word for attendance there is there. All right let's move on. Now we so far we do KN&N for one predictor right and we calculate the distance between my point and the nearest neighbor to be just the distance right the difference.
(1:04:58) Now what if we have more than one dimension? Let's say we have two predictors, three predictors, four predictors. What would you do? What is the distance in that case? So I have two predictors. I'm trying to find the nearest neighbor. What do you do to find the distance between two things that they have two dimensions or three dimensions? All right. So I have a point if it's one dimension is the distance between the students. Yes.
(1:05:37) So ukidian Pythagoras. Okay. So if let me look at the VN distance neighbor to Kenn of course uh I don't know your name. Sorry. Yeah. Uh but if we look at the nearest neighbor of 10 to two students, I need to actually get the distance to everybody else, right? And that's just the p the ukidian distance. Clear? Cool.
(1:06:06) So we're going to do that. So the distance between two points that they have more than one number is a vector or multi-dimensional. It is of course the ukidian distance. And what is the ukidian distance? We learn it at some point in well in Greece we learn it in kindergarten uh because the Pythagoras but somewhere in high school or middle school we learn what the Pythagoras distance is the difference index the difference the y square them add them up and take the square root. Okay, is that clear to everyone
(1:06:41) guys there? Are we good? Yeah, good. All right, so that's cool. We can do that. But I want to leave you with at least one warning slide. The warning slide says the following. If as the dimensions increase, the couple of problems will arise. The data becomes sparse.
(1:07:06) Meaning that it's hard to imagine this, but in if everybody was sitting in one line in this class, you'd be dense. When you go to dimension, you start having holes. Imagine now we actually we have a three-dimensional classroom because we have the second dimension. But imagine if you start having fourth and fifth and sixth dimension what will happen is that most of the seats will be empty okay because it's just the volume is big and that is called the curse of dimensionality because the distance between two points if you randomly put them in a high dimensional space will be far but the most difficult thing tends to even out because if you have many high dimensions it's a little
(1:07:44) bit hard to imagine But trust me on this u spend some time you have a very high dimensional space every point now will be almost equidistant from each other because the space for them to go into it so I said the canyon can struggle unless you have a lot of data use feature scaling uh slash selection what is feature scaling let me explain this is not in the slide I should add it is the following if in one dimension we have let's say TV budget thousand of dollars and in the other one we have newspaper budget in dollars
(1:08:22) which one will be always dominate the distance of course the dollars because those numbers will be much bigger right so we need to make sure that every dimension is scale and I I believe in the section we have some scale yeah uh so it's not in the slides but it's in the sections uh they will explain So take away Canon is intuitive but high dimensional space make it less effective. All right.
(1:08:51) Now for the last three minutes um I'm going to summarize and then we can play the game if you feel like but let's not. So I believe so for every lecture I'm going to put the learning objectives. It's a summary. If you know these things you're good. At least you have somewhere to go and look what is the professor is asking me to learn right and these learning objectives will be the driver for the quizzes exams etc.
(1:09:21) Okay so the first one by the end of this lecture I want you to be able to define response and predictor variable and represent them using design matrix and response vector. Yeah, you learned that. Good. You can yell. Yay. Okay. Explain the difference between inference and prediction. I don't hear anything.
(1:09:48) Describe the K nearest neighbors algorithm as a nonparametric that relies on data fixed parameter. Yeah. Implement KN in 1D. Maybe not yet, but with by the time the sections are done, you should be able to do it. uh find the nearest neighbor using distance predict output by average their values. I hope that's super clear and then in addition to that extend to multi-dimension that's compute distance to B dimensional using ukidian distance. Yeah.
(1:10:17) And then recognize scaling and feature important issues. So that last one was not very much covered in the slides but we have sections don't forget and we cover that even more. Okay, with that it is 3 minutes. Thank you very much. So I see you all on Wednesday.