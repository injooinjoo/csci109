109 day7 - YouTube
https://www.youtube.com/watch?v=HnPMpev5NxA

Transcript:
(00:01) So I run my multi- aent but it didn't. Good morning everyone. Audio good good morning everyone and welcome welcome welcome uh students online and students here. I am Pablo Protobabas and I'll be your lecturer for today and next Monday. After that, you I'm going to leave you to the good great hands of Kevin for a while before I come back to finish the semester together.
(00:44) A couple of quick announcements while everybody's settling in. Um homework one was due yesterday, right? Was it? It was yesterday. I saw a lot of ad post. Uh some of you have late days. Great. Because you come to class, you have late days. How cool is that? Uh also, um homework 2 was released. Uh pay attention to this. Get out of your computer. Actually, let's do every now and then. Let's do something.
(01:17) No computers for 2 minutes. Just get your computers out for 2 minutes. Good exercise to learn to focus. Uh, homework 2 is out. So, I'm suggesting to start as early as possible. It is a little bit of challenging long I would say not challenging. So, I do suggest to start early homeworks. Hello.
(01:49) What's going on? Let's start. Um, Homer 2 is out. Please start as early as possible is long because we make the homeworks a little bit longer because now you have a virtual assistant called CH GBT. Okay, so start early. Do not leave homework to the last two days is going to be long. So you have to do a lot of things. Next announcement.
(02:16) Uh next week we're going to have a mini quiz half an hour uh during sections. Okay, it's going to be about mainly multiple choice questions, maybe one or two uh open responses. Next week at section, don't miss section next week because you have to do the quiz. It is time for half an hour. Uh on Friday, I'm going to we're going to put bunch of practice exercise to see the level and how they look like.
(02:44) Okay, it's actually it's going to be a run. taking double side. Now, during the exam, it's going to be on a piece of paper uh and a pencil. Bring your pencils. We're not going to have pencils and pen. So, bring your pencil and pen. I'm saying I don't have a pencil or pencil in my backpack anymore. So, bring one. And you're allowed to bring one sheet of paper.
(03:09) You can write both sides. A small phone as you want. That's all you have. Okay. Any questions about that? Bring a calcul. Yes. Just one piece of anything else. Huh? Yes. Thanks. Yes. The letter, not the Don't bring a scroll. Right. Any other questions about that? I don't need to type it. That's what uh you can print it. Yes. Yes.
(03:52) What the frame like? Yeah. Anytime. Um as long as it's not connected to the internet. Yes. Guys, pay attention. What's going on? uh the the quiz will cover everything until Monday. Everything that been said in class or in section could be in the quiz. Okay? So please don't come and ask me is that covered is that covered is that cover whatever we cover whatever we talk about during lecture sections will be in the just so you don't stress out is not meant to be difficult. Yes, Chris.
(04:47) Everything is election to be the quiz. I said everything is a lecture. Just I we decide what is in the in the quiz. I'm not going to do conditional. The only condition the only thing I'm giving you the hint is that is not meant to be difficult. The quiz questions are not meant to be difficult. They're not meant to catch you. It is kind of a check.
(05:12) The go let's actually discuss for two minutes why we're doing assignments of that sort. The most important for me and I think for my co-structors here is that you study, right? So it's going to force you to study and you're going to force you to get through all these things we've done a little bit in with a little bit of time to see how everything falls together. That's for me the most important thing.
(05:42) So the quiz question, the quiz next week is not about us proving that we're better than you because there's no point to do that, right? Uh it is to see if you follow, if you studying and make sure you get the time to digest everything because we're going to move on and unless you get these things we've done so far nicely sorted and understood, when Kevin starts, you're going to be more confused. Okay.
(06:07) Well, we adding more complexity as we go. So, it's good to have some positive. All right. So, any other questions before I start. All right. So, today we're going to be talking about regularization, but I didn't finish last uh Monday. So, I have few things to do for Monday, which actually going to be useful for what we do today.
(06:45) what do you see? Uh, Chris, uh, anybody on the Zoom here? Now you see the slide right. Okay, great. Okay. Okay. All right. So we're here last time and we show this example that we have training data and validation data. We train the model on the on the training data of course and we train three models with three different degrees of polomial.
(07:47) And when you look at it uh and we said which one of these will be picked based on the validation error and for strange reasons we said it's degree one and it's you know one of you said this particular validation said it was because he was chosen randomly and when you choose things randomly could be a strange right so in this case as you can see the model that is going to be picked by validation will be the linear one because the validation data are the pink data. Okay.
(08:18) So you can see the pink data there. Uh so the validation this is not working today. Okay. I'm going to Okay. Start with Okay. Here it is. So the validation data are the pink points, the triangles. So now I have three models and I'm going to select the model that fits the validation data the best. So it's picking the straight line, the the green line, which is degree one.
(08:43) But obviously that's not correct, right? Because when you look at the data, the trend is most likely degree three. So what we said last time, this is a strange thing about when you pick up your validation randomly, there is a chance to get some particular validation. Now you can turn this around and say, okay, how often I'm going to get a validation set like that.
(09:07) Of course, I did that for pedagogical reason, but it's not going to be so common to get such a straight validation set. However, he's sleeping today. Uh, however, u imagine if I have a lot a lot of models, there's going to be one model that is going to be specific. I give my class okay to do this class before twice.
(09:35) Uh so you're gonna if you do a lot a lot of models you eventually going to find a model that just fits that validation set. So we call that fitting overfeeding to the validation set. And then I asked what shall we do about that and I think I think it was Hoy said do it multiple times right? Yep. So why don't we do it multiple times so to avoid the fact that one of these validations it's particularly good for this particular model. Okay. So that's where we left. So and I said yes that's great.
(10:06) We can do it multiple times and if you do it multiple times this randomness is just going to wash out. Is that clear to everyone? Yeah. All right. So and then I said and I left it there. I said there is maybe a better way than just doing it randomly many times because if I do it randomly many times uh there is a chance think about it that some of the training points do not make it into the training set right so if I get all of you and randomly select 30 every single time even if I do it 10 20 times some of you may not be ever included in the training set so in order
(10:44) to avoid that we're going to do what is called cross validation. Okay. So let's discuss cross validation. We started with that. You remember we have the training set, we have the validation set and we have the test. Train to train the parameters the betas of the model. Validation is to select between different models.
(11:08) And the test what did I say? Save it. Don't touch it. Right? Only at the very end when you report your results to someone you said that's my result. Okay. Now we're going to do slightly different. So we're going to take the training set and instead of split to train and validation, we're going to do the following. We're going to divide it into folds. In this particular case, I have five folds.
(11:32) I take the training data and I divide into five folds. One of these folds will be acting as the validation and the other f four folds will act as the training. Okay. So I take that. So that's my training and that's my validation. So when I validate the model in that validation fault, I get MSE validation one. Then I take the second fold to be the validation and the other four to be the training.
(12:04) I train again and then I use the other the second fold as a validation. I get another validation measurement for the MSE and I keep doing that. Okay. So at the end what I'm going to do I'm going to average all the validation MSE's and that will be my cross validation MSSE. That avoids the chance of having some training data that don't make it into the training set. Okay. Any questions about that? Yes sir.
(12:31) How do you do the reporting for display and the numbers that seems the same? seems only okay. Andrew asked a very good question which is perhaps usual is how do I choose the number of faults then he he also said how do you choose the proportion I think if you choose the number of faults and the proportions they will come together right you agree with yeah so it's a very good question and actually I take this opportunity to go back to the first one when we split the data into train validation and test let's just forget the test for now just for simplicity and
(13:12) we give you a training data and I ask you to split it into train and validation. What's the proportion? 80 to 20 is that 80 20%. Who said that the homework Chris? Okay, we said it in the homeworks and in labs and everything, right? But have you ever questioned that? Why? Why 8020? Why not 7030? Why not 50/50? Let's think about it for a moment.
(13:53) Right? So what we want to do is my goal is to have as big of a training set as possible. Okay? Because the training data is what helps me to find the parameters of the model. The validation is to validate how good it is. Now let's get the extreme cases.
(14:13) If my validation set is very small, what will be the problem? Forget the default Andrew for a moment. Let's think about that and that will come. Okay. What will be the extreme cases? If my validation set is very very small, what's the problem with that team? Yeah. in the individual organizing the distribution of the data. I'm just going to rephrase it a little bit and tell me if I if I have a very small data set and I do some estimate on it that estimate is could be wrong or you don't have confidence that what Timothy Yes.
(14:52) So if you you do an estimate you calculate a validation if your data set that you do that I'm not forgetting you guys. So uh if your data is very small that estimate is not very believable right imagine if your validation say one point would you believe that no depends on that point okay so we don't want it too small now if I do the validation very large then what's the problem there's no many data in the training set okay now what is the right proportion well I want the validation set to be large. Now if you
(15:31) ask Kevin who is a statistician at heart and I ask Kevin what is a large number 50 statisticians okay that was 50 so let's you have a th00and point right or 100 points or so 50 is a little bit too small let's say about 100 so that 20% comes from the fact that our training sets usually is about thousand you need 100 or 200 100 in the validation we put sometimes 8515 I tend to put larger in my valid in my training than validation but if I have a data set of 10,000 points I would never lose protoas put 20% in my validation it's just a waste of data points okay so keep that in mind
(16:19) there's one more trick that is going to come later now to your question with that in mind we can think what is this fold we want size and then we decid on the K. Now if all these I said sounds confusing 80 20%. And the K is about five. Okay you don't want K to be big. You don't want it to be too small. Okay.
(16:47) So I gave you two answers a little bit more kind of intuitive and one a little bit more uh rule of thumbs. Okay good. Uh and then of course the other question usually I sometimes ask do we do validation or cross validation to set up your K because you can do that right you see what I'm saying the K is a hyperparameter then I can do another cross validation to find the K which I'm using the cross validation to find the other hyper it's possible but let's not do that okay unless you're really into these kind of things.
(17:19) So pick up the K five five 3 five 10 maximum. But think about what I said if you have a large data set you don't want your faults to be too big the validation fault because you're wasting u data for training and the only thing it's going to do is going to improve your validation robustness by a little bit.
(17:39) It's actually goes up by square root of one over square root of n. All right so we did that. Now we have the validation and this is just a reference slides every now and then I put reference slides if you want to go and study a little more in details but it says exactly the same thing as I said before exactly the same a little bit more formal a little more mathematic uh all right and then there is leave one out we're not going to do now I'm going to talk about this because this is kind of important and it ties it up with the
(18:10) sklearn uh there's one two things I want to talk. This is the this is the code we're going to be using. You're using skare model selection cross validation. The CV result is a dictionary. Pay attention. Dictionary is not a number. It's a dictionary. Okay. Uh do we know what a dictionary is? It has a key and the value.
(18:35) Right? That one is going to be the estimator. The model used to fit the data. So I'm giving the model. I gave you the data to fit the targets variable to predict. It's going to do the cross validation inside means it's going to do the folding inside. You don't have to fold it. You just give here's my training data. It's going to do the the the folding and it's going to do everything. Then the scoring.
(18:59) Uh here is one thing that you have to pay attention. You have to use the negative mean square error for regression. Okay? because otherwise it's going to use a different scoring. The default is different scoring and it's not going to find the model that gives you the minimum MSE. It's going to do something else. So always use negative mean square error.
(19:25) CV is the number of false and set to true to include train scores. Okay? Because it's going to give you a dictionary. If you put that true, this the dictionary is going to give you the scores for the training tool. Okay? So, we're going to cover this again in section, but there's couple of things I want you to pay attention if you don't get into section.
(19:42) Make sure you set I think I have it here. Make sure you set negative mean square error for regression for regression. When we do uh logistic regression, you have to put something else, but make sure you have that. It's a section. I said this in the section, but never hurts to say that twice. Oh, three times. Okay. Uh right, we're done with this.
(20:08) Now is we're going to move to uh regularization. We're going to start with bias variance things. Uh take a second before I get everything ready first with our slice reach. Okay. General. Yep. Okay. All right. So, now we're starting officially the lecture for today. So, the first part and actually this image I don't forget is by Greg. This is the Bryce Canyon.
(21:00) I got a bunch of new figures, uh, images yesterday, so I'm good for until the end of the semester. No, for a few more weeks, so keep sending us photos. Um, is Greg here? Of course not. All right, the nice picture. So, now we're going to be talking first about generalization errors, error, and bias variance trade-offs. You must have heard that or you haven't. You're going to hear it now.
(21:28) Okay, so here's the outline. We're going to be talking about model selection. I'm not going to do recap because we've done it and we're going to talk about error bias variance tradeoff and then we're going to get to two regularization techniques, lasso and reach. Okay, let's start with model selection. I think we've done this.
(21:48) No need to talk about it because we've done it. It's all fresh. Okay, let's start with error bias trade-off. Okay, so we know that just taking the training data, run your model and get the performance is half of the story, right? We need to check how the model is doing on unseen data, right? Uh so there are at least three ways a model can have a high test error or regularization error.
(22:20) Uh the first one if the data have error, right? So you get a data set that is very noisy. Okay. The other one is underfitting. Okay. And the other thing is overfeitting. Because let's look at all three together again. If you get a data set that has a lot of noise. No matter what you do, you're going to have a large generalization error.
(22:47) Validation error is going to be high. Test error is going to be high. Are we good with that? Yeah. The second scenario is that you get a data set and you feeding a very weak model. No matter what you do, you're not going to get it right. Simple. And the third one is overfeitting that we already talked before, right? So let's start with the first one.
(23:12) So the first one we call irreducible error or alotoric error. These are the two terms we use. What does it mean? It means basically is irreducible. There's nothing you can do about that. You can remove the noise from the data. Sort of. I'm not going to get into that, but in general, if you have a noisy data, there's nothing you can do.
(23:36) Your model, no matter how hard you try, you're not going to get good results. Okay? That's part of life. Okay? So, that's called irreducible error. It's going to be with you for the rest of your career. It's been with me all the time, right? No matter what you do, there's always going to be that error. So it cannot be reduced is always present. Cannot be eliminated.
(23:59) So reducible error comes in two forms. One is underfeeding and the other is overfeitting. Right? That's meaning the problem comes for your choice of model. Right? Underfeitting means your model is not strong enough. Overfeeding we've seen it before. You just throw too much on the on the data.
(24:18) It overfits the training but it doesn't do very well in the unseen data. So the regularization error will be high. All right. Now let's look at this. So we said irreducible error is something nothing we can do. So what we do? Forget it. We're not going to bother about that. Just leave it there.
(24:38) Now let's talk about the things we could do something. So as the model complexity increases, we're going to go from the underfeitting section region to the overfeitting region. Okay? And as a decrease decreasing the complexity of the model, we go from overfeitting to underfitting. And I think we saw that last uh Monday.
(25:04) So think about what is more complex a polomial of five or a polomial of one? Five. What is more complex? A model that includes 25 predictors or one that includes five 25, right? So more complexity you have, more you're going to overfeit. If you make it too simple, you're on under fit. All right? So with great model complexity comes great risk of overfeit. Now here's where interesting things happen and this is where computers out.
(25:34) Pay attention to this. It's one of the moments that you're going to remember for the rest of the day. Maybe all right. So I take a bunch of data. Uh now the data have some error right. So when you make a measurement if there is an error it means you just see one what is called realization of the error.
(25:57) Right? So if the expected value is here and you make a measurement it can be here can be here it can be here it can be anywhere. Right? an hour round expectation. We're going to talk a little more next week a little more about that.
(26:17) But in general, when I make a measurement, I just see one instance of this error, right? So that's what you see on the top left. Okay? And I fit a I take the orange data, which is one sample, and I feed the linear model. Okay? So the linear model is the orange line. Okay? So the blue one is the true model, right? Which I in principle I don't know. Okay, is that clear? So I just took some measurements and I feed the model. Okay, now let's do another measurement.
(26:45) So and I feed another model. Okay, and then another one, etc. Now let's do it about I don't remember 100 times, 2,000 times. Right? And what I drew, I threw away the point so it doesn't confuse us. So what I'm showing there is the 2,000 regression lines I got by fitting 2,000 models on 2,000 sample points.
(27:10) Sample it means the whole thing, not just one point, right? It's so many points again and again and again. Feed the line. Feed the line. Feed the line. Feed the line. So I have 2,000 of them. Right? Cool. All right. Now let's do the same one with a complex model. That was a simple model. Right? Now what do we see is that all these lines here all the lines here are close to each other right now now let's do the with a complex model so here I fit a polomial degree 10 right so this is one data set I fit a polomial of 10 and then I do it again and again let's do it 2,000 times what do you expect to Hey,
(27:59) spaghetti noodles noodles. It's a mess, right? So, you see all this? It's just lines, speeded lines. All right. Now, compare this to this. This is nice and easy. Nice and tight. This is a giant mess, right? Okay. That means on the left we have low bias but I got got his attention or her attention.
(28:37) Uh on the left what we have we have low variance meaning the models don't change from one to the other. On the other hand, we have high variance means the model is changing. The prediction of the model is changing from one data set to the other. Okay. So if you think about it, overfeitting is that you're very sensitive to your training data.
(29:02) Meaning that if you're very very sensitive to train data, that means that if I change the training data, the model will change. And that's what we see there. That's overfeitting high variance. That's underfeeding low variance. Is that clear? All right. So, so bias here refers to how far off a model's prediction is and from the actual truth. So, as we increase complexity and decrease complexity, the error of or the bias will go down.
(29:28) Meaning that as we increase the complexity of the model, we're going to be able to get to exact thing. But we increase the variance. Right? So let's look at that. Now as we do that the model we went for low variance to high variance. So this is the bas bias variance trade-off we'll be talking about. As the model complexity increases the error will go down.
(29:54) The bias will go down but the variance will go up. Okay? So as the model becomes more complex it's going to have more variance but it's going to predict better on the training. Is that clear? So where we want to be in the tradeoff somewhere in between. Okay. So another way of seeing that if I have high v high bias means I'm making a mistake is not accurate.
(30:18) It's like throwing darts and here I have low variance. I'm precise. Not accurate but precise. I'm just shooting the darts always off but they all at the same point. So that's the linear mode. Here I have high variance but pre not precise but um low bias right so I'm getting in the center but I have a high variance which is the polomial that we talked before now the other two corners is that we want this we want low variance and low bias and we definitely don't want to be there we don't want a model that has high variance and high bas Okay, so that's the the game we're going
(31:05) to be playing now. So it's so overfeitting happens when the model learns training data too far too and when did this happen too many parameters the degree of the polomial is too large too many interaction terms and then remember I have one more thing here which is that and we see that I had it last time where is it that the the coefficients of the model become too large and that's what we're going to see next and that's where the region and lasso regression will fix for us. Okay, so let's recap what we've done so far. What we did, we
(31:42) just investigate. We said if the model is too complex, it's going to fit the training data well, but it's not going to do very well on the validation on the seen data. We call that high variance. The model fluctuates a lot if you change the training data. On the other hand, if we underfeit, the model has low variance.
(32:02) It just gives us almost the same result, but it doesn't predict very well. Hope this is super clear. If not, this is the time to ask questions while I'm switching to the next section of today's lecture. So, it's easy. Yes, you have question. No. All right. Now, we getting into rich and lasso.
(32:49) The moment you're all waiting for, right? Mhm. I'm getting the heck of this. Okay. Picture of the day is by Lane King's Canyon, California. It looks much better on my laptop. The the lights are not so good here. Very nice photo. Is Laney here? No. Okay, good photo. You made it. Yeah. Can take a picture. All right. So, we're going to be talking about region lasso uh and then the hybrid parameters to that.
(33:32) And I have a surprise new slides today. All right, let's talk about regularization techniques and how it fits into the whole story. Okay, so you notice I'm telling a story here. These the new characters coming into the play. See how they fit into that. This is the f this is the figures I show you before which I want you to pay attention again.
(34:00) On the left is linear model robust low variance doesn't get the details of the model. You see the model here has some curvy thing. It doesn't get them. Uh high degree polomial gets those things but it has the high variance. Okay. Now let's look one thing. I'm going to look at the coefficients. Okay. So here is the coefficients of beta 0 and beta 1.
(34:26) Remember I have 2,000 models, right? I kept doing the reme-measuring the data, refitting the model. Okay. So here what I have is beta 0 and beta 1 for 2,00 models. And these things here are called the violin plots. I think uh Kevin talk about them. My favorite plots. Put violin plots wherever you can because I love them. Not when they don't belong, by the way.
(34:51) And just don't say, "Oh, I put violin because Pablo loves it." You're not going to get your grade like that high. But I like violin plots. They look beautiful. Okay. So what it says here is that this is the average value of beta 1 over the 2000 and this kind of gives us a distribution of the values of beta 1 because every time I get a new data set I get a different value of beta 0 and beta 1. Is that clear to everyone? Let's see back there we good? Yep.
(35:17) On the right on the right here I'm looking at the polomial 10 coefficients. Every polomial term will have a coefficient. So now what do you see here? Which one has the most standard deviation? The largest standard deviation H the B9. Yeah. Let's see who is the most keen observer in here. There's one detail here you have to see.
(35:54) Ooh is 10 to the 9. The variation on the coefficients in the polomial are huge. Right? Here my coefficient vary I don't know 2.1. Here they vary a lot. Right? Okay. So I think we're on to something. I think we found a way to crack the problem. If I have high degree polomial or I have a complex model that's amazing you got the right moment to wake up uh the polomials the coefficients sorry the coefficients have high values and when my model is low variance these coefficients have low values okay now what can we do about that I'm going to use that to create a new model
(36:52) such as avoid large coefficient values but also do well in training. So I'm going to take these coefficients. I'm going to shrink them down as much as I can. Okay, are we cool with that? So that's my my trick here. So yeah, here it is. This is where I said the coefficient values are 2x3. Now let's let's examine that.
(37:19) So how do we discourage extreme values in the model parameters? Here's a question. I said I want a model that is going to do well but the values should not be extreme and I'm going to translate that into the following statement. I want a model that does well but doesn't overfit. I make a leap here because I know high variance will means overfitting. It means extreme values of the coefficient. Okay.
(37:44) So how do you do this? Anyone suggestions? I want what is the expression? I want the pie full and the dog full. I want to eat the pie and have I want both. I want to go to a party and I want to finish the homework. Uh I want to go away this weekend and have the lectures ready. Yes. And the penalty. Yes. Excellent. So I have a game time actually.
(38:11) Let's play the game. I forgot we have Who wants to play now? just to you've done once right let's give a chance to someone else who wants to skip a class you want your name you I am Marcosa um I live in Lever house and what else you want uh fun fact about me is that I can juggle three balls cool all right now I have one observation none of our data science master students students ever raise their hand.
(38:46) So I'm keeping my eyes on you. Okay. Your name again? Marcos. Marcos. Okay. So here's the question. How would you discourage extreme values in the model parameters? I kind of gave the answer. Divide all model parameters by large numbers. Make sure the causal relation between predictors and response variable is true. Discard any model with model parameters value larger than one.
(39:12) penalize the model with penalty that is proportional to the values parameters. Don't listen to him. He doesn't know what he's talking about. I think I would go with D. You're going to go with D. But let's think why A is bad. I don't want large values of the coefficients. Why don't I divide by that number? Because it's just scaling the data. And we say when you scale the data, nothing would change.
(39:42) Make sure the causal relation between predictors and response value is true. I have no idea what I'm saying there. I just make fancy words just to confuse you. Uh discard any model with model parameters value larger than one. That's going to bias the model. So this is your final answer. Yeah, I think you're right.
(40:01) You can be excused starting on Wednesday. Thank you. We got it. All right. So we're going to penalize. That's the main idea here. Penalize. So what we want is to penalize. How do we penalize? It's actually it's actually easy peasy. So what we have we want to when we train what we minimize what do we do minimize the error right whatsoever the MC right that guy what is that called the loss function the MSE right yes remember I love the MSE I like to single in the MSE all right so MSE See that's what we get our parameters better minimize that right now
(40:59) we want to discourage extreme values in the model parameters. So what shall we minimize? How do I'm going to write the loss function that make sure that the parameters are not large. Anyone wants to write, anyone who hasn't, let me put it, anyone who hasn't seen this before, because I suspect you've seen it, uh, wants to take a crack at this, what will be a term that will be large if the parameters are large and will be small the parameters are small.
(41:46) of it's yes the sum of the coefficients just the sum sum is good so the coefficient is good but shall I take the absolute value or or the square okay let's start with the square boom you got it and the absolute value boom you got it okay either of the two will work right so now I have two things. I want this to be small and I want this to be small.
(42:25) What shall I do? Put them together. Add them up. So, I didn't put this there. I'm telling you, sometimes people go and put things on my just to surprise. So, we add them up, right? Are you okay with that? Anything else you want to do? Just add them up. I'm letting you think a little. Well, there is a scale problem here.
(42:59) MS is one scale. The other is just adding them up. Maybe not exactly. Maybe we should put a coefficient in front of the two just to control it. So, we're going to put a lambda there. Okay. So, that will control let let's think about this lambda. And I have more slides about that actually. Let's see. This is called the regularization parameter.
(43:16) It controls the relative importance between the model error and the regularization. Let me explain. If lambda is zero, no regularization. Back to our MSE. If lambda is huge, I don't care about MSE. I want to minimize the other one because that becomes important. And then all the coefficients would become zero.
(43:41) Right? Because if I want to minimize the sum of the squares, the easy the only the best way to do that is just put them all to zero. Right? So if I don't care about the the MSE the first term here, then just if the large if lambda is large, so that means I don't pay attention to that, I make the other zero. Okay, I hope that's clear. I don't know what happened here.
(44:05) Okay, so that's that's the regularization parameter. Okay, now let's go to uh how we determine lambda. I don't think I have just to to get you thinking. We're going to get into that. Did you see that? How do we determine the lambda? It's a hyperparameter of the model. What's the answer? Cross validation. Okay, everything is cross valid.
(44:34) So, we're going to do cross validation. I'll show you more details in a sec. All right. The same one applies by the way this is rich. This is called rich regression. U if it no if if is the absolute value we call it lasso and if we call it if it's square we call it reach. But one detail here I want to say so.
(45:01) So if if the absolute value we call it L1 norm and it's called the lasso. Right? That's fine. And one detail I want to point out is that we actually don't regularize the bias. Meaning the bias the beta zero is not included in that term. And why is that? Should I include it? Let me remind you what I said. Overfeeding is when I'm sensitive to the fluctuations of the input data.
(45:38) But beta zoro is not involved with the x's and the y's right just the bias right so we don't have to worry about that right it's just the bias it's the overall value okay clear we don't regularize the beta zero so this is lasso we minimize the lasso with respect to betas and that's going to be our new model called lasso regression okay so if we do reach we just put the term there to be beta square So lasso absolute value reach square that's it. All right.
(46:11) Uh the reach actually let me talk about the reach is the L2 norm which is the square of the coefficients is called L2. If you see it I want you to know and again we don't regularize the beta zero. Yes. Bas and lasso. Very good question. I have slides for them for now. You can try both of them and see which one works. Uh there's some advantage of bridge and some advantage of lasso. I have slides.
(46:38) Okay. But good question. All right. So let's talk about how we choose lambda first and then we talk about how to choose the two. I said cross validation. Let's first do it for validation. I have very stepbystep explanation on how to do that. Okay.
(47:04) So maybe it look easy but let's just do it once by step by step because in the last three four years when I was showing this students got a little bit confused. So last night I got a motivation to redo this slide. So let's see how it's going to work. So the goal is to find lambda based on validation right cross validation I do next just now one validation set. Yeah.
(47:23) All right. So we have the train validation test. We use this to train. We use this to select and do that. Okay. So let's start with the beginning. So the first step is to select a range of possible values we want to examine. So we get from the minimum to the maximum. You choose that range. Okay.
(47:51) So you say I'm going to check lambda from 0.001 to 100. Okay. So these are the values. The values are not fixed. You can choose your range yourself based on the problem. Okay. So we choose a bunch of lambdas we want to examine next. And here's my values of lambda. I go from 10 to the minus 5 all the way to 10. Okay.
(48:14) It is typical when we do uh hyperparameter the for the regularization to use orders of 10 0.1.2 uh sorry 10 to the minus 5 10 the minus4 and keep going like that. All right. So once we have that what we going to do we just going to find the values of beta that minimize the loss that includes the penalty term. Now in a linear regression without the regularization term you remember there's a formula x t transpose x inverse xrpose y.
(48:52) You remember that right? Would you look something like that without this lambda? Okay. Now when you do regularization for reach, there is a closed form solution to find the betas. And here is how I find my betas. Meaning I take a value of lambda. I plug it in there. I find the betas. These are my betas here. I take a second value of lambda. I put it there. I calculate the betas. Here are my betas here. Okay.
(49:21) So for every lambda I find the coefficients better that minimize the regularization loss function. Okay. Now we take those and this is an important thing. Computers down. Computers down. This is where I want you to pay attention because you're not when I say down it doesn't mean you just move it 10 degrees. Just close the computers for five minutes. Thank you.
(49:52) Thank you. I done this so many times. Close the computer. They do it 10 degrees. I say close the computer another 10°. Just close the computer. It's not going to hurt. Unless you're running a a big program and it's going to kill it, right? Thank you. So now we're going to take the beta values we found and we're going to calculate the loss, but only the MSE loss. only the MSE loss.
(50:24) Note the regularization term is not going to be included. Okay? Because most of the time most students make a mistake. Why do we do that? Why don't we include the regularization term? Yes, because you actually care what power is doing on the data, right? Not an extra turn. Thank you. and that we for that we get the MSE on the validation data without the regularization term.
(50:54) These are my values. Okay, what's the next step? I'm going to find the minimum value. So I have my remember I have my lambda values here. I did the whole thing. Now I have the MSE on the validation data or for every lambda I found the betas.
(51:17) Using those pedals I calculate the validation MSE and here are the MSE values. So let's visualize this MSE values. Lambda is a bunch of values, right? And which one is I'm going to select? Which lambda I'm going to select? Well, the one with the lowest value, right? That's that guy. That's the lambda I need. That's the lambda that minimizes the validation MSE. And that's my lambda.
(51:46) That's my regularization parameter. Yep. Good. All right. So, next step, we take the train, we take the validation, we combine it. That's an extra step that uh we like in this class. Not always um done in practice, but this is how 109 we like it. SK likes to too. They learn from us. All right.
(52:23) So we take the train and the validation, we put it back together, we retrain the model, and that's my betas from my rich regression. Okay, so that's the whole story. So let's recap. Choose the lambdas. From the lambdas, I find the betas for each value of lambda. I take these beds. From those, I calculate the validation MSE. MSE only without the regularization term. Then I get a bunch of values for the validation MSE.
(52:48) I find the minimum. That's the lambda. The lambda that gives me the minimum value of the validation MSE. It is the correct one. I use that. I recombine the data, refit the model, and boom, I'm done. Okay. Now, we can do the same and then record MSU on the test set. That's what I usually do. Right? That's your final report. You do the same thing for test set and that's what you report.
(53:13) Okay? Nice and clear all of these steps. Now in the case of lasso is the same procedure except one little thing. There is not closed form solution for minimizing the lasso loss to give me the exact values of beta. I don't have an xt xrpose x blah blah blah for the lasso. Okay, there's no closed form solution.
(53:39) There is tricks to do and that's what we do. So again how do we determine we start with the data we find the lambdas again the same thing we have the values of lambda we choose and then we instead of using the formula here we use something called the solver solver is tricks heristics uh that we do to find the values of beta that they minimize the lasso loss. Okay.
(54:09) Now, as I said, linear regression, enrich regression and very rare cases that you have a closed form solution. We'll be using solvers very often, right? Okay. So, we use a solver. Some trick we have to find the values of beta. Once I have the values of beta, I now calculate again the validation. Now, I'm in the validation thing.
(54:32) MSE and only the MSE, not the extra term. I get a bunch of values of validation MSE. Same thing as before. We visualize it. We find the minimum and that's my lambda for thing. Then I put the data back together and done. And then report the last one. Okay, that's it. The same story.
(54:55) The only difference between reach and lasso is that in rich and I think it's it's kind of give you hints where I'm the difference in rich I have an exact solution for minimizing the full loss where in lasso I have to use solver and therefore lass is a little bit slower okay nowadays it doesn't matter because computers are so much faster all right now I'm going to do the same thing with cross validation it actually is the same procedure but I'm going to do it multiple times. So I have my faults. I take the training the validation as before.
(55:28) Again I select the range of lambdas the same exactly way. Um I take those I use reach or lasso regularization depending which one you do. And keep in mind that each lambda values give us a different vector. So beta that's what we said. Now I'm going to use that and calculate here is for each I calculate the MSE on the validation for each set of betas which comes for each individual value of lambda and again I have that thing it's the same as before so far now the only difference is in cross validation I'm going to repeat this with a different
(56:09) fault I'm going to repeat this multiple times and for each time I'm going to get a list of validation MSE for every lambda. So what I'm going to have I'm going to have this this this and this. So what I end up having is now instead of one set of MSE validation for every for every for the set of lambdas I have multiple ones because I keep repeating in K times and what I'm going to do I'm going to take the average of that and I'm going to find the minimum of the average. So let me visualize that for you.
(56:44) So what we have is we have the average validation MSE which is here with L values and I'm going to find the minimum of that right so if you visualize this uh we have for one fault we get that for another fault another one another one and then we get the average which is the black line and I'm getting the minimum of the average validation loss Okay. And then again, we put it back together.
(57:19) We train again. We find the new betas and that's going to use that to report on the test set. So, okay, that's the story. I have some reference slides again, but that's the story. Now, I have time for questions. I have one or two points to add. So, let me let me start. Let me give you a little bit of time to think 10 30 seconds. Let the questions bubble up.
(57:47) Yes. Would it be the same first the minimum for each and then take the average of those instead of finding the average value? Yes. Yeah. Uh the question is uh will it be the same to find the minimum and then average the minimums or the average the lambdas? Okay, let's step. So I have I find the minimum value of the validation loss and I have a lambda value.
(58:28) Then I have another one, another one, another one. Do you average the lambdas at then? Yeah, that's okay. So actually I think if you look at it from this graph I think what Panos is saying look I have one minimum here one minimum here one minimum here I mean if you take the average of those is not going to be the same or I think so okay other questions good that's a good question yeah why validation setting into the model but not the question is why is the validation okay to put in the training and not the test okay I got it uh so Andre is asking
(59:19) why don't you put the test set into your to train again right or rather like what's the difference that's I thought both ways I think I understand the question so let me repeat so what I said is that at the end I'm going to put the validation and the training together and retrain.
(59:42) So first question why am I doing that? The second question which you didn't ask but the second question is why and I'm not including the test there. Okay first the second question is kind of simple because I don't want to report a result that included that data set. Okay. Now I use the validation in the training in the last step because I'm done selecting the model.
(1:00:06) So I'm going to take advantage of that data to train the model again because I'm done now selecting the model. I already selected the lambda. Okay. So I'm done with that. Okay. That step as I said is not considered a great scene if you don't do it. And I think in the homework we don't ask them to do right.
(1:00:30) We do well SK9 will do it but we don't explicitly so if you don't do it that's I'm fine but it's a good practice to have I'm done selecting the model I don't need that validation anymore I'm going to put it back to training and train again did I answer yeah yeah the MSC without the regularization functions or or appropriate then how would you know which is actually minimizing.
(1:01:00) The question is when I trying to find the value of lambda that is ideal. I want it to be the one that minimizes the the MSE on the validation because I didn't use the validation to find the coefficients better. Right now your question is for when you're calculating the MSSE on your data you're saying you want to leave like lambda out of that yes because I use I use the lambda to find the betas right so the betas there are found because I have the lambda first but when I decide which model to use I want to use the valid only the MSE
(1:01:49) because that's what I care. I want a model that is going to do very well on the unseen data in this case which is a validation that is going to give me the best results. So think if you think about let's say the ME is dollars right what I want at the end then is the model is going to give me the most profit right the fact that I put this regularization term is to avoid overfitting but at the end I want the model that gives me on the unseen data the best performance is that clear yeah one more question yes what is the solar for
(1:02:27) what is what is a solar what is the solar Lasso asked um there's two ways of doing it. One is some kind of gradient descent and the other using subm modular gradients long discussion. Yeah. So you basically huh so gives you which solver is using but the the gist of it is instead of using regular gradients use the special gradients we basically say if it's above zero use that well use that it's not as simple as just if statement it's called some modular gradients and you can do some math and I have slides if you want
(1:03:13) uh and you can find it Of course online. Yes. Going back to the cross validation process definitely expose the valet any other cross valid station from just looking at the entire training like so. Okay. The question is actually you have a very you may not realize but there's a little leakage there.
(1:03:45) I'm not going to get into it but there is a little leakage there but uh we get to that when we do trees and in trees we use out of back error but remember in November I'll come back to a little bit of that but now you're asking how is that different that if I do kfold every data point is using the training but not directly I'm always leaving one out right so I never validate using the data I use for training never I do it multiple times but I never do that so is not is not the same because if I use the training and then validate on the same data is it's going to overfit right good questions guys Chris
(1:04:35) all the data again have crossed choose land with cable balls. Once I do that, I choose my best lands. Which one you want to do? Yes. I mean, the good part that last step that is is optional. When you do cross validation, it becomes less optional. And the reason is because once I find the lambda, which of the models do I use now? Because I have cave models, right? So by putting them all together and doing another fit, you eliminate that problem, right? Good point.
(1:05:22) Um, anything else before I move on? All right, so that's finished this. We have the last uh part which kind of addresses one of the questions before actually before I do that can we put the word and I wanted to address one more I think what's the word Chris.
(1:06:32) All right. While we get in the word of the day, I wanted to to address one thing at least on the board. Can you all see here? All right. So, let's imagine you have the following scenario. This is my validation MSE and this is my lambda. Okay. So, let's say you see the following thing. So in the slides I have a nice minimum.
(1:07:01) What if I see something like that? This is my maximum lambda and this is my minimum lambda. What is the lambda you choose? Anyone? This one? This is the lambda that gives me the minimum validation MSE. Are you okay with that? Why not? I would say no. Yes. If we just choose more later valently. So if I see a thing like that it means yes this is the minimum but I is because I restricted my maximum to be there.
(1:08:00) So what I will do just take that and put it here and then wait until I see something like that. Right? So if your optimal value of lambda is at the edges it means you haven't explored enough range. So increase the range and do it again. again. And the water, the word is fire. Huh? No, we did earth, water, fire. It is fire.
(1:08:35) You can do it in Spanish, too. That's your word of the day. You can live now. All right, the picture for this section is Chicago. I only have five, six minutes to go. Yes. Yes. Question. Very good question. Is there a chance to find
(1:09:48) um I I'm gonna draw it. What if I have this scenario? That's what you say, right? And let's say actually, let's say this is my L max and I choose this, right? Yeah, I know. I think I know what you're saying. So let's say I have let's say I have it's a good question good question.
(1:10:28) So let's say I have this right uh and I'm no I'm going to exactly what you have in mind. This is my range and this is the minimum I found. Right? And the question is how do I know there's not another minimum there? If I continue, maybe I get something like that, right? You won't get negative, but I think that's what you mean, right? Yeah.
(1:10:54) Uh this is a convex function. So in some ways you don't have that. If you the next question you should ask, so there's no local minima, right? Uh the next question I'm sure you want to ask is that well, if it's convex, why don't we just minimize it, right? It's not exactly convex because at every step I'm changing my betas, right? So, it's very unlikely to have a thing like that.
(1:11:18) I think they may exist because we're changing the betas every time. So, it's not exactly uh a convex function. However, it's very unlikely and if it happens, that's fine. We can't do anything about you can't ex you can't search the whole space, right? Yes, Sandra. Can we far down you can go? Very bad. I want Okay.
(1:11:52) So, this is a good opportunity to also say the follow uh this data science is I would say 60% science and well maybe 70% scientist science and 30% art. Okay, it is get used to it. Not everything has proofs. Not everything is clearly stated. So this is one of these things how how deep to the rabbit hole I'm going to go.
(1:12:24) I said before the K I can cross validate on K too, right? And then cross validate on that. I could just keep going, right? And keep uh trying to find answers. uh this is something you kind of learn by doing right uh especially when it gets to hyperparameters if I have one hyperparameter which is the regularization coefficient is one if I have the polomial and the regularization now we have two you have polomial and regularization and multi multi- predictors now I have three now you add couple of other which one of the two shall I use region lasso another hyperparameter and that can go on and on and if you do this exhaustive research
(1:13:06) grid search that becomes impossible right at one point you're going to have to say this is good enough for me okay um there's certain things we always do like regularization coefficients we usually do find it but if you go even to neural networks you see people do uh regularize the coefficients of the neural networks very rarely I've seen anybody actually doing even search for that you use the default.
(1:13:34) I can say from the people in this room that they use neural networks they never have even know that there is uh weight decay with the regularization parameter just use just the default right so it's a interplay between have some intuition what values work and some things regularization parameter will can give you very different results if you change it uh the k between three and 10 I guarantee you it's going to be okay any one of them it won't change your result. Okay. All right. Let's um let's wrap up.
(1:14:06) So, in this section, I'm just going to compare between reach and lasso. Um there's different ways of deciding. What time is it? I have two more minutes. Done it. H there. Hold on. Hold on. I start two minutes late. So I think this is the slide I want to show so we don't come short today.
(1:14:35) So all lasso regression the the actually no I I need to show some more. Yeah I let me go through this rich regression is good and is fast. Okay, lasso regression is slower but it has one advantage and I'm going to repeat on Monday since you all rushing to the but let me finish it. Lasso regression has one advantage when minimize the sum of the absolute values some of them will become zero.
(1:15:06) Can you be quiet please? You want to leave fine just be quiet for me to finish. I need one minute. Uh so when you do lasso for some reason the coe some of the coefficients will become zero. So you sum a bunch of absolute values and you want it to be small you can put some zero and add some not so big with the reach you can make all of them small.
(1:15:39) So the big difference is that lasso naturally we do some parameters predictor selection. Okay, the advantage of rich is fast. The advantage of lasso, it can just eliminate some of your predictors. We'll come back on that on Monday and with the game next time because you're in a rush. Okay, now look at this. Thank you.