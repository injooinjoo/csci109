109 day5 - YouTube
https://www.youtube.com/watch?v=M1IneUdkSYY

Transcript:
(00:01) Yeah, this um a little bit of We don't have any We don't have any announcements, right? No, other than homework due next Tuesday. Uh test test fu. Are we good? All right. Okay. Welcome, welcome, welcome. Can you hear me up there? Welcome, welcome, welcome. All right, settling down slowly. The expensive seats are taken. Cheap seats are still available. Uh, okay. Welcome
(01:08) students here. Welcome students online in the sea. Maybe someone is on a cruise. Welcome everyone. I will be your lecturer again. My name is Pablo Protoabas and today we're going to be doing linear regression. one of the most fundamental models in data science. Uh some of you may wonder why do we do linear regression is simple. We know it.
(01:35) Uh my opinion and the way I think actually even the most complicated models I start learning I always go back to linear regression. I make a column this is linear regression which I perfectly understand. Now let's go to a much more complex model and let's just do the mapping between them. It does help me and I bet Kevin and Chris to always go back to linear regression because it helps you uh ground your thoughts and the way you model.
(02:01) Right? So you will see that we even do that when we do logistic regression. Sometimes we're going to say okay linear regression we've done that we understand it. Let's see how we map that to uh logistic regression. Okay. Uh no announcements from the team. So we're going to go straight in. Uh by the way the the the photo today is by Chrissy.
(02:20) Where is Chrissy? I met him yesterday. Uh, yeah. Where is he from? It's right. Juliana, the Swiss Alps, right? Yeah. All right. Thank you, Chrissy. Um, some of you sent photos. We're going to be putting them. Don't worry, your photos will show up. Except if you have a pet. I don't put pets. Only nice landscape.
(02:49) No bet is anything wrong with your pet, but I don't put pets in my All right, let's look at the road map for today. So the first one we're going to get into simple linear regression. Simple linear regression is we only consider one predictor. Okay, just one predictor. We start always with the simplest thing and then we keep going more and more complex.
(03:09) Now then we're going to expand this to multilinear regression where we're going to consider more than one predictor. We're going to have multiple predictors. Once we do that, we're going to go and interpret the model. Uh we said that we're going to be doing a lot of interpretation this semester, especially with Kevin this semester.
(03:28) We is going to be doing much more interpretation, but we're going to be talking about interpretation of the model. And actually, that's what is linear regression so good is because you can understand the model. Uh once we do that, we're going to go into some practical. I hear myself and I don't like Oh, there. Can you hear me up there? Good.
(03:53) So, then we're going to go to some practical things. For example, scaling uh your predictors. We're going to talk about that. And actually, we're going to talk about scaling even in the sections this week about scaling for KN&N, but we're going to come and talk about scaling here too. And some pitfalls like colinearity. We're going to address that.
(04:12) We're going to actually address colinearity again and again a few more times but we're going to touch on it today. And then finally how we include predictors. They are not continuous categorical qualitative predictors. That's the road map. Let's see how much we can get. And in the meantime don't be discouraged.
(04:32) We have a lot of games today later part of the games and we have some surprises. Okay. All right. So start with the simple linear regression. So this is a uh this is a figure that we got from last time with the KN&N. KN&N is the K nearest neighbors. Uh we discussed about K1 K3 whatever we did all these things. When we do KN&N we have a nice prediction.
(04:56) However you see this stepwise kind of prediction. The KNN is nonparametric. No parametric means it doesn't have a nice closed form u expression for our f hat. Okay, that means that if I want to ask questions a little bit more complicated questions in the form how much more in sales can we expect if we double the TV advertisement budget to do it with KN&N because we don't have a closed form solution. We don't have a formula to double it. It's a little bit more complicated.
(05:31) I have to find where I am, double it, see what's the value. Right? So it cannon would not allow us to do this kind of things. Okay? So to do that we have linear. Did you see that? That was cool. To do that we need to do linear regression. Linear regression it will be the answer to this kind of models. Okay.
(05:57) So in linear regression we're going to make the following assumption. uh we're going to assume that the functional form of my f remember f is this perfect ice cream f is the model f is the actual relationship between x and y which I don't know the goal of modeling is to find f which we call f hat right so now I'm going to make though one assumption the assumption is that this function f relates the predictor with the response through a a linear relationship.
(06:32) So f of x is beta 0 plus beta 1 x. Beta 0 and beta 1 are numbers. X is my data. Right? So what is beta 0? Beta 0 is the intercept and beta 1 is the slope. I'm sure we learned that at one point in our lives, right? So, so what follows is that since f ofx we make the assumption that for all the possible relationship between the input and the output I'm going to constrain it into a very simple form.
(07:03) So at this point I want to step for aside a little bit there are infinite number of functions that can relate x to the y. Right now what I did I constrain it. I make the assumption that only consider linear relationship between X and the Y. So my relationship I assume is that is beta 0 plus beta 1 X. So by as a consequence to that my estimate for f F hat should have the same form.
(07:33) So it's beta 0 plus beta 1 X. Now I put hats on it because F is the truth answer. Beta 0 and beta 1 are the true coefficients. Now I'm estimating beta 0 and beta 1 to find my estimate of the function. Okay. So the way we're going to do estimate this beta 0 and beta 1 is through data. Okay. So let me recap infinite number of functions that could be true.
(08:02) I make the assumption I constrain the possibilities to only linear. There is some beta 0 and beta 1 which can be anything. And I'm going to find them. and how I'm going to find them. I'm going to use data to do that. Okay, so this process um let's actually get a little concrete example.
(08:24) We start again with TV budget on the x-axis in thousand dollars and sales in thousand of of units in the y-axis and we plot eight points again seven this time. Okay, seven points. And the question is for all these linear functions that could be which one is the best. So I can draw a line that one it gets the trend. Is it good? I don't know. Let's try another one. Well, this one seems to go closer to the points. Maybe this is better.
(08:54) Or it could be that one. Right? So there's no way we can do that by drawing by eye. Right? They all look kind of similar, especially the last ones. And I can keep drawing this until the end of the lecture. Right? So what do we do about that? How we going to find the best line? Yeah. Then you minimize the sum of the squares. To minimize the sum of the square of the residuals. First of all, let's redefine the residuals.
(09:24) We did it last lecture. Let's just review it. Right? So we're gonna introduce the residuals. The residual is the distance between the data and my prediction. My prediction is the green line. Correct? So the vertical lines there represents the difference between my prediction and the actual data.
(09:43) And those are my residuals. Right? So I want to minimize the square of the residuals or minimize let's talk about put it more general. I want to minimize the overall error on average. And to do that I'm going to use a loss function. And what's my favorite loss function? MSE.
(10:05) So I'm going to minimize the MSE mean square error which is the difference between the prediction yhat and the true one y square. And why do we square it? Because otherwise you get zero. Otherwise if you have a sign if you have the positive and negative that can cancel out. You said on average I have no error but you have plenty of error.
(10:29) So either we take the absolute or the square or the fourth power or the sixth power and anything but we're sticking with MSE and why is that because I love MSE and I think someone asked we're going to get in two three four lectures later we're going to come back and say we're going to motivate it we're going to show why MSE it is the appropriate loss function for this kind of data not for every data for this kind of data okay so we get to that but for now let's just take the MSE Cool. All right.
(10:56) Now remember the Y had now is not like before with KN&N. Yhat has some functional form. Yhat is beta 0 plus beta 1 X. Right? Okay. So now I have an expression for my MSE. So as Jacob no Jacob Jacob said we're going to minimize that. Meaning I'm going to try different beta values.
(11:24) I'm going to find the the beta 0 and beta 1 that will yield the smallest MSE. Okay, just make sure we understand this. If you notice, I'm finding the beta 0, beta 1 that yields the smallest MSE, but I'm using data, right? The first part I make a random assumption that the relation is linear. The second part, I'm using the data. Okay? So, I'm going to minimize the beta 0 and beta 1.
(11:48) So, I'm going to take the loss function. If you see the loss function on the top is this expression but the loss functions does depend on beta 0 and beta 1. Meaning that if I change beta 0 beta 1 I'm going to have more error or less error. So I'm going to minimize that this arc meaning over beta 0 and beta 1 tells me find the beta 0 and beta 1 that will yield this the the smallest error. So far so good right? Okay. All right.
(12:15) Now this process of finding the minimum is what we call training or fitting. You hear the term fit, you hear the term train. That's what it means. We get the data, we make the assumption, we have the model, we get the data and we find the parameters of the model in this case beta 0 and beta 1. That gives us the smallest error. Okay.
(12:36) Now you look at me and you said come on this is too simple. Is that what I sign up? And I said progressive over logic. Wait, don't don't think that whole semester will be so easy. But there is a very very important lesson right here. And if you hear me out, you demystify machine learning a lot. I said I define my model. I assume is linear.
(13:03) I wrote the loss function and I minimize the loss function with respect to the parameter. That's pretty much all the machine learning. Okay, a lot of machine learning is that find your model. Decide what's the model is linear, it's quadratic, is neural network, whatever you want. Define the loss, minimize the loss. These three steps for most of the problems you do follow this thing.
(13:29) Even chbt starts with that. You define the model which is a giant transformers. define the loss which is going to be some binary crossentry categorical cross entroproy minimize it with back propagation. Okay, we're not doing back propagation now but that's the three steps. Okay, simple but gives us the foundation for a lot of other things.
(13:54) All right, now how do we minimize the beta 0 and beta loss with respect to beta 0 beta 1? To do that, we're gonna use one of the most popular and powerful libraries in the data science ecosystem, sklearn. Okay. So, you're going to do it if you haven't. Have we done that? Not yet. We're going to do it this uh in this uh week section.
(14:13) So, you're going to see it. But I wanted to show a few things as we go ahead. So, it's quite straightforward, right? The first thing we do, first line, we import the library linear regression from skarn linear models. Okay. So the second thing we're going to do, we read the data. So it's the second line here.
(14:39) We read the data into a pandas data frame. Okay, so that's the second line. Then we take some of the columns from the pandas data frame and we call one to be the predictor X and the other to be the response Y. So I take the TV values to be the predictor. I call it X and I take the Y and I call it response variable Y. Okay. Now notice the double brackets there.
(15:03) Do you remember why is that? Have we told you why is that? Because Skarn expects that your predictor is always a matrix, right? So if you do a single bracket, you get a series and it's going to throw an error. It's going to complain. So if you see the error like that, just put double brackets onto your X.
(15:21) if you have one column because then it's a matrix with one column and skarn wants the X to be a matrix an array I should say. All right. So the next thing we do we uh instantiate the class linear regression. We start the model and that's where you should be a little bit familiar with classes.
(15:46) If you're not go and read in bedrock classes we have a whole fun lecture there. What is a class? we instantiate a instance of the class linear regression and then there is this magic thing which is thought fit that's where all the magic happens that's where the beta the values of beta 0 beta 1 come out that's where I'm finding the values of beta 0 and beta 1 that minimizes the loss okay so that's the dot fit and we give x and y meaning we give the data and tells it hey go and find the values of beta 0 and beta 1 that will minimize the loss of this linear regression. Okay. All right. Once we do that and we
(16:22) and we fit the model, we can actually access the values that he found. So rec coof this is not a typo. That's the way you do it. It will give you the value of the intercept that you found during the process of dot fit. And if you do recincept will give you the value. I'm sorry.
(16:45) This is the beta one and this is the intercept. Okay. So that we can access the values of the fit and then now we have a model we know the values of the coefficients we can also predict give some value here I want to know what will be the sales predicted for 100,000 TV advertisement and boom it gives you 11,000 sales units okay all right any questions so far we'll see this kind of things in the in the sections this week sections start today correct yeah so to see that we do some exercise. We're going to make you do some of this. Cool. Now, are we happy with it?
(17:26) You said I'm happy all the time. Yeah. So, all right. So, we did the fit. Everything is good. You're ready to go home. You're ready to do your homework. You're ready to get your internship or job because you know how to feed a model, right? Anybody feels uncomfortable with this thing? Nobody. You're all happy with that.
(17:58) You're done. Don't fit. It's all I need. Magically, he's done it for us. You shake your head. Why? Very good. Doesn't know what it does. Your name John. John says I don't know what it does. The same with me. I don't come to this class to learn dot fit. Do fit is easy. Everybody can do it. Get a tutorial.
(18:25) Run it. Do fit. Put the data and done. No, we want to know what's going on, right? Aha. Let's understand this. What happened there? Right. We put everything under the rack and just in case I wrote in Spanish. I don't know how to read that and I don't know how to read that. Okay, is in various language. I want to know what happened in dot fit.
(18:52) What going on? How does sklearn finds the values of beta 0 and beta 1. Okay, so that's what we're going to do the next five minutes. In order to do that, I want to introduce two concepts which I'm sure most of you are familiar. But just for the case to get everybody on the same page, just a very quick review.
(19:12) First idea we want to know is what is a derivative? Okay, so everybody knows what a derivative is. Yeah, I don't hear a loud guess. So, let me just at least spend one minute. The derivative is the instantaneous slope of a curve. Right? So, a derivative of a function as you see there is that at every given point in my curve is the slope at that point.
(19:37) Okay? Is the rate of change is on that point in my curve. Okay? Good. You said I know that. Yes, I now let's make it a little more fun. If I have two variables, now I'm have to introduce something called the partial derivative. Nothing fancy. It just means I find the sustenous rate of change of one variable by holding everything else constant. Okay. So now I have two variables, beta 0 and beta 1.
(20:04) Right? So my goal here is to put the beta 0 and beta 1. We put the data. Did you see that? It gives me the loss right given the value of beta 0 and beta 1 and the data give me the loss right so but the L depends on both the beta 0 and beta 1 and therefore I want the partial derivatives of that so what I do I calculate the partial derivatives of the loss with respect to beta 0 and beta 1 two derivatives with respect to beta 0 and with respect to beta 1 holding one constant to the other one holding the other constant to that okay so far so
(20:37) good So the second idea I want to introduce and this idea you're going to need it if you're doing neural networks but let's introduce it here is called the chain rule. The chain rule tells me if I have a function of this form so beta the loss we said is y minus the yhat which is beta 1 x plus beta 0 all square.
(21:00) Now notice there's two functions here. One is inside the parenthesis and then I take whatever is in the parenthesis and I square it, right? So the chain rule tells me that if I need the derivative of the loss with respect to beta, I need to multiply two partial derivatives. The first one with respect to the function inside the parenthesis and one with the square.
(21:25) Okay. All right. So let's do that. This is my f. So this is my f². So I do that. Now the derivative of f² will be 2f of course. and the derivative of the f with respect to beta 0 will be just minus one. I can do it for uh so now I can multiply the two and I got my partial derivative of the loss with respect to beta 0.
(21:52) Okay, step by step I know you're smiling. We do it once we get more. All right, so we have the derivative of of the loss with respect to beta 0. And I'm going to do exactly the same for beta 1. I'm not going to go step by step for that. So I have the two partial derivatives, right? So far we said we're going to minimize the loss. To minimize the loss, I introduced this idea of partial derivatives.
(22:14) To do that, I needed the chain rule. Okay. So they said this the third idea here is that the minimum of a function will occur when the slope is zero when the partial derivatives are zero. Okay. So that's the second idea. That means these two derivatives both of them should be equal to zero. That's going to happen at the bottom of the ball here. Okay.
(22:38) So I'm trying to minimize and I'm saying that at the minimum the loss derivative with respect to any of my variable should be zero. We all agree on that? We getting it? Okay. So that means okay by the way I'm introducing this symbol which is an upside down triangle which is the nambla it's just a simple it's called the nambla symbol that means this vector okay instead of writing all this I'm just writing like that we don't need it now in order to find the beta 0 and beta 1 that gives me zero derivatives there's three ways we can do it first
(23:18) one the brute force brute force means But try every possible combination. Okay, why not? I try every possible beta 0 and beta 1 values can imagine until I hit one pair value that gives me zero derivative. Cool. Any problem with that? Try eventually thousands of potentially thousand this is only two imagine if I have 10 dimensions I have to try all combination no good right okay so brute force is not for us let's go to the next one the next one is called greedy algorithm uh the most famous one is gradient descent which we're going to cover later in the semester and in B and you any machine
(24:16) learning course you're going to do gradient descent is going to be there uh we touching it a little bit later when we do logistic regression. The final one which is very special for linear regression is actually that I can algebraically solve for those two equations. Okay, what does it mean? I have two equations.
(24:42) The first is the equation of the first derivative with beta 0 equal to zero. The second one is the loss derivative with respect to beta 1 is equal to one. Two equations, two unknowns. Yeah, we can do that, right? All right, let's do that. So I have my two. This is one of the equations on the left, right? Is the derivative with respect to 0 is equal to zero.
(25:06) And this is the derivative of the other with respect to zero. Two equations, two unknowns. How many of you know what to do here? I solve for beta zero and I substitute to the other ones. Right? Okay. So, I'm not going to go step by step, but I got my little Wait, did you see my little machine? It did it for me. And then he got it. Okay. So, this is the solution.
(25:28) It's an exact solution given the data. These values of beta 0 and beta 1 will give you the minimum loss. Okay. Now, this equation for some, not all of you, may look a little bit scary, but let's go step by step. I'm gonna unwrap it just to make sure it's clear.
(25:47) So the first one, we sum, we sum over all our data, right? The second one, this is my data, X data. The Y is my Y data, predictors and sales. The bar means the average of my X. The Y bar means the average of the Y. The rest are the same. Okay, a lot of symbols, but it's very straightforward. Okay.
(26:12) Now it means that I can find the values of beta 0 beta 1 that minimize the loss given that it's a linear relationship and given my data ta not impressed I know that's fine all right so here's my now the whole story summary I start with the assumption of linearity I wrote my loss function I express y as a linear expression and minimize it And I do this and I found the smaller this is the training and this is my answer.
(26:46) Now the value this line on the bottom yhat is equal to beta 0 plus beta 1 x beta 0 hat and beta 1 hat are the values I got from that equation on the top. Yes specific the expected specific numbers. Yes. Okay. So look at the separation right here. This is values of your data. So I sum all my data. You're going to get a beta 0 and beta 1 3.2 and 1.0 or something numbers.
(27:18) Just keep in mind these values are the best values of beta 0 beta one given your data is the slope that will get closer to all your points. Okay. Any other question? Right. That's what dot fit is going to do for you. Um, warning, this idea of find the exact solution only works for rare cases. It's cool. That's why we love linear regression, but it won't work for logistic regression. It won't work for trees.
(27:50) It won't work for neural networks. It won't for vector machine. It will not work for pretty much every other model you're going to touch except for linear regression. Okay? It's the only one. And that's why we love it's easy. We don't need to do optimization. You run this, you find the answer.
(28:09) You don't have to worry about did I find the correct answer or not. Okay. All right. Are we good? That's it. Part to one done. Linear regression done. Let's up the ante now. Right. So remember what I said is progressive overloading. What's the next one? We get a little more complicated. All right, it's time to get up. Talk to your neighbors. Uh, while I'm getting the next one.
(29:06) What's happening? The the bubbles that pop up cover some of the the I usually have two. I usually do have but not all of them. I think there's only one or two. Usually I move it. Power point is on the We're back. Um, part two. We start with Yellowstone. Is Jenny here? No. Jenny. Ah, good picture. Yellowstone. All right. Uh, I like this.
(29:55) Kind of appropriate. What's going to happen in this section? It's like buffaloos in the wild. Okay. Uh, all right. Let's uh continue our story. Um multilinear regression. And to do that, of course, we're going to start with a little game. Okay. I like games. Um hey, pay attention. So the I need a volunteer.
(30:20) You know, you get an extra free day if you if you do that. Actually, where do I control this out? Okay. I should be able to control the sound. It's a little bit too loud. No. Oh, whatever. You can live with. All right. Volunteer to play my first game. We have three today.
(30:48) Some is it you want to play or you just want to skip one lecture? Both. Both. All right. Introduce yourself. Is it? My name is Na. I'm a junior at the college living in Quincy House. Is that the third Queeny we got? Yeah, it's old Queen. Okay. So, name Ada? No. What's your name? Nana. Nana. Any hobbies? Any interesting facts? Um, I like to knit. You have to knit. Okay, great.
(31:15) All right, Nana. So, you get a free lecture, right? I see the motivation there, but then you won't be able to play games. That's fine. All right. So, this is an easy one, man. Uh the first is if you have to guess someone's height, would you rather be told their weight only their weight and biological sex? The weight, biological sex and income, their weight, biological sex, income and favor number.
(31:50) I would have to say B because those are genetic biological things. income is not like genetic, favorite number is not genetic and while weight is helpful, why not have two factors? So, okay. So, Nana says the first two, okay, I think most most of you agree the first two should be there, right? But does anybody has an idea about C income? Don't expect it to be linear.
(32:19) It could be step function nutrition could be shorted. We have any Dutch in this from Holland? No. Anyways, the Dutch were the shortest people in in Europe 100 years ago. Now go to Holland. I feel like I mean a short guy. Everybody's two bitters or six feet. So maybe we should include the third one. I don't know. You don't know. You're not sure. You still with the B. How about the favorite number? That's very helpful. Not help.
(32:52) My favorite number is 179, which is my height. But is that relevant to your height? My favorite number is my height. Actual correlation, though. Well, you don't know. You're right. But here's the problem, Nana. You allow your biases, your assumptions to go into your model. Definitely. You allow your biases into their model.
(33:17) Do we want that? No. No. Okay. Maybe we should throw everything in there and let the model intelligently find out what is appropriate. Actually, nana, there's no right answer. So, just all right. But you volunteer. You show your interest of not skipping a lecture uh between lecture 8 and 12. That's the ones you should meet.
(33:44) Thank you. That's Kevin's. He got it. All right. So, I don't think this is the right answer here. I just wanted to make couple of points and make you think is that yes, is a dangerous here to allow our biases into the model. Right? We just say, "Oh, income is not important.
(34:08) Maybe biological sex, I don't believe biological sex is those are personal biases we have, right? So, let them on the side and let them all take care of." Of course, that's not so simple. So, uh, of course, we always, by the way, this is by far my favorite GIF of all times. I'm going to let you admire it for a second. It's brilliant. CS 109 students, baby. All right. I really love it.
(34:40) Okay. So of course we want as many things in our model and we want the model to figure it out. Of course it's going to be a big part of this course is to figure out which ones are there and I said the models will be able to take lots of data and make the prediction. And of course this approach brings a few questions. The first one is data noise.
(35:00) If we add a lot of predictors we adding a lot of irrelevant information. Okay. So this I'm not going to explain this now. We will get to it over feeding and all this kind of colinearity and all these things but we I want to start with the following thing let's just put everything in and then we figure out what we don't need okay but let's not allow our biases to do that now on the other hand you I see your face I get you on the other hand putting everything else is not always right if you're trying for example to predict
(35:37) loans who to give loans phones we remove maybe ethnicity. Should we should we put ethnicity in? No. Right. Should be obvious guys. You should say yes professor. No, don't put ethnicity into who is getting loan or not. Right? But it gets more complicated. Maybe the zip code is correlated with ethnicity. Right? So what I'm so I'm confusing you on purpose.
(36:06) I'm saying put all the data in with few caveats. The first one putting every data it may make the model suffer and second we should also not just put every data because we have it. We should think about some of the ethical considerations that we should be thinking.
(36:26) Sometimes we're not going to put some predictors not because it doesn't have signal because it's not right. also also is illegal too and more of a ask for forgiveness than permission. Okay. All right. No, but I agree with Chris. Okay. It's also illegal. All right. So simple linear regression we assume a simple basic form of f beta 0 plus beta 1. But no at least she wanted two predictors, right? So how do we go from here to two predictors? This is my one predictor. It's a straight line. Good.
(36:58) Now the second part is to include more than one predictor and we're going to make the same assumption that our response variable is related to every predictor linearly. Okay, as simple as that. So here's my Y. Y is a vector remember because for every observation we have one sales and X is a metric.
(37:23) So it has columns. This is the first prediction. The second prediction predict predictor not prediction predict x1 column x2 column x3. So we have height and biological sex two predictors two x's two columns. Yeah. Good. So I'm making the following assumption which is down there that the response value that the f now relates to every one of these predictors linearly. Okay that's it.
(37:55) And now instead of having a line, if I want to plot it, I have a hyper plane. Okay, a flat hyper plane. I can't draw more than three dimensions because it's impossible. But you can imagine if I have 20 predictors, I'm going to have a hyper plane in the 10 dimensional space. Okay, question. No. All right. Now, let's go back to our data.
(38:19) We have one column for the design metrics and one for the response, of course. Now I'm going to make it multiple. So now I'm considering all different uh advertisement budgets, TV, radio and newspaper. Okay. So in the next five six slides I'm going to do since some of you said, "Oh, this is too boring. We want more math." Right? I'm going to go into a little bit of the math.
(38:45) Don't be afraid um because I'm going to go step by step. Uh we're not going to ask derivations in the exam. So anxiety level goes down but at least once you in your career in your journey you should see couple of the math right some of you are math oriented so let's do it yeah okay we're going to go step by step and explain this so in linear algebra I'm going to change now to something called linear algebra because if I have more than one predictor first of all I have a lot of rows I don't want to write y1 y2 y3 right I want to have one symbol that pred shows everything. So I use a vector
(39:24) notation. So y is my vector notation that has all the all the things and x is a matrix. So I use a metric notation. Now beta becomes a vector itself. I want you to notice something. I added one in my data matrix. You see these ones here? I added them. Why? Because it makes everything look so much more beautiful. Okay, let's do it. So, here's what I have.
(39:53) Sales equals to some bias times some coefficients, the TV budgets plus some other coefficient, uh, radius, etc. Okay, so let's take the first row here that I wrote and I just look at it. So, I have sales is equal to the first row. You remember multiplication of matrix with a vector.
(40:15) You take the first row of the vector of the matrix and you multiply it with the vector. take the second one, you multiply and you add, right? Okay, if you're not familiar with that bedrock lecture seven, I have the whole linear algebra you need for this these kind of things. But the idea here is a matrix times a a vector.
(40:34) I'm going to take the first row and multiply and add them up. So if you do that, multiply a row by a column, you add them up, I get that. Cool. That means I can write the whole thing like that. Okay, so instead of writing complicate formulas, Y there is a vector, X is a matrix, beta is a vector with beta 0 included.
(40:58) Now, why did I put the one is because I don't need to write the intercept differently. Nice. Neat, right? Just three symbols. Y is equal to X beta. Cool. All right. So, we have that. So, again, I repeat that. So, Y is that, X is that? All looks good. So in order for me to go to the next step which is again find the values of beta 0 beta 1 beta 2 beta 3 or beta p that minimizes the loss.
(41:29) I'm going to introduce two three concepts the rest is algebra. Okay. So the first concept which I'm sure is obvious if you don't know it I'm sure you get it is that if I have a matrix which is look like that the transpose of the matrix is make every row a column and every column a row. Okay. So you take it and you just turn it 90. Cool. Yeah.
(41:59) The second idea I need and by the way you can get that with numpy and numpy is one of the also powerful libraries in the data science ecosystem and in science in general. So we sooner or later we going to be introducing napion in your section. Do we have Jonathan this week show? is going to show pretty much somewhere or not.
(42:25) is a library that allow us to do numerical uh operations and it has a whole part for u arrays tensors and the like again if you're not familiar with numpy bedrock section we have numpy all right so now the second idea I want to introduce is called the inverse the inverse is the following if you have a number and you multiply by the one over the number you get one Right? The same with the matrix.
(42:57) If I have a matrix and I multiply by the inverse of the matrix, we get the identity matrix. Okay, these two concepts will help us. And here I verify that you can do it with again numpy linear algebra. All right, so y is equal to x * beta. Now my MSE changes a little bit form. I don't need to introduce summations or nothing.
(43:23) I just say if I take this part and put it to the left hand side that should be equal to zero. Okay, the square of that. Let me show you again. So y is equal to xp is my model. Right? So if I put it in that side then that should be zero or in other words my prediction which is xp should be equal to the true values which is y. So this says y which is the true value minus x beta which is the prediction is equal to zero that's m right square of that right yeah let me scan faces back there yeah good good all right all right I can write this way if you like a little more cumbersome but this is equivalent to this okay
(44:10) so instead of writing sum of blah blah blah I in this one that should not be cut off. It's one that escaped me. So the y there should be lower case. Don't worry about this. I'll fix it. All right. So here is what we have. We have this and I said well I have this I can expand the sum. This is a sum over all my data points.
(44:38) Is the error of the first point plus the error of the second point squared plus the third. all the data point. So look instead of writing a sum I'm going to expand the whole thing. Why? Because it's going to make things easy. So expanding that I have prediction for sales one min prediction of sales one sorry true value of sales one minus the prediction of sales one plus squared plus sales of observation two minus the prediction of sales two squared plus all of them. Okay. All right. So cool. And that one over there annoys me.
(45:11) So I remove it away. It doesn't matter. It won't change things at all. All right. So we have that. And our goal, you remember, is to find the derivatives of these things. So I went there and I just took the derivative with beta 0. So and there is a square. I get the two out these and I get a minus because beta 0 is negative there. Then I take the derivative with respect to x1.
(45:36) I'm going to get the two with respect to beta 1. I have x11 plus the whole thing etc. Okay, I took the derivative the partial derivative. Okay, those should be equal to zero, right? So let's do it. So now those three half is equal to zero. Now I can do this little magic here because I'm smart.
(46:05) I can look at this and I can decompose this thing into a matrix times a vector. Cool. That makes a little bit nicer. But now notice this is y - something y minus something. So I can do this because this is a vector and I got this. Now this x1 x2 again I get into a vector and then I go one more step and things will look fantastic. Now remember this thing should be equal to zero. Now we have two.
(46:34) What is this purpley thing here? Let's see if anybody got transpose of what? Right. Xrpose. Right here it is. What is this thing here is a y. This is x and this is better. Okay. That thing should be equal to zero. Let's do it. So I have 2xt - y = to 0. The two doesn't matter. The minus doesn't matter. I have that. All right. Now I can solve this equation for beta. Right? Easy peasy.
(47:16) I what is it called? You multiply each term. I calculate x with the y. And then here with that I open it up. And now I have xrpose x beta is equal to xrpose y. Okay. And now multiply both sides. Remember the inverse of xrpose x. So this going to go away. And here we are. Okay. This thing that I did can be done in two steps if you know multivariable calculus. But I didn't want to get that. All right.
(47:49) Now it's over. It's done. You saw it. I unwraith. I took the derivative. I wrap it back into vectors. Now, that equation there. I want you to stare it. Stare it. Not your computer. Stare the equation. I want you to be imprinted into your retina because you're going to see it very often. Okay. Now, let's look at it.
(48:15) What is telling me is that I can find the better coefficients by doing this simple algebraic operation. And we're going to see in the sections today today or this week that we actually don't don't fit with that. We don't need we don't need scikitler. We do it by hand. We do that right. We don't do this. Okay. I thought the originals stateful. Okay. It was there. We're not going to do it. Sorry. False advertisement.
(48:41) Okay. But this equation is what skarn.feit does. This exact thing. Okay. All right. So what we did, we went from a single predictor to multiple predictors. To do that, we have too many equations to solve by hand. We went into a linear algebra. We express everything as vectors, matrices. I show you that you can get to that.
(49:04) Again, anyone here has done multivariable calculus, raise your hand. So you could do that in two lines, right? You don't need all this unwrapping around. But because we don't require multivaried calculus, I wanted to show you how to do. Okay. All right. So, it was a little bit of a uh messy. But let's move on to the next thing.
(49:24) So, we said we do that. We write the loss function this way. And this is my final formula which is the minimum of the loss function with respect to beta. That equation is a closed form solution because x is data, y is data. I don't need anything else. Just plug it in and happy to go. Okay. All right. So, what else we have? And I no I have it here. Not in the section.
(49:53) That's where is it? It is in the section notebook. You see not dreaming. All right. So you can do the in numpy in few operations since in the section I will go. All right. So so far let's recap what we have done. Simple linear regression. Find the coefficients with some algebra. Multilinear regression.
(50:16) We assume linearity with every predictor. A little bit of algebra. We get that formula you remember by now, right? Yes. Good. And so now let's move on. So the next one is interpreting the model parameters. Okay. And to do that, of course, I'm going to play a game the thing. Let's see if I remember correctly.
(50:41) All right. So, we do have a game. So, the game is who wants to play? Who wants to play that doesn't want to skip class? Here is your name, sir. Where is Yayat? Arsenal one yesterday. Exactly. Yeah. Uh my name is On. I'm a junior in Quincy House. On Yeah. Quincy House. Quincy House. The best house. Yeah. House Rocks, baby. It's my favorite house. Represent. Um Oh, like information about myself.
(51:22) Uh little bit. You don't have to. I uh I love basketball. Um, I have the same birthday as LeBron James, actually. That's who? LeBron James. You really? Cool. Um, so, uh, do you play at that level, too? Of course. Of course. Of course. So, you you relate to my joke last week about me playing NBA, right? Oh, yeah, I did. I did. I was listening very carefully. Yeah. Okay.
(51:45) Um, who is your favorite player? SGA. She better. Oh, yeah. SGA. Oh, yeah. Yes, sir. Yes, sir. Okay. Um, you have inside information. Yeah, you do. Okay. All right. Here's a question. Uh, in a simple linear regression model, you have the equation y = 5 + 3x. What does the coefficient three represent? Option number one, the predicted value of y when x is zero. The change in y for a one unit change in x.
(52:21) the amount by which Y varies randomly around that line and none of the above. Um, damn. I think I'm going to cross out A. You're going to cross out. Okay. Because I feel like that's what five would represent. Mhm. Right. Um, I think I'm going to cross out C as well. Um, and then growing up, I was always told to never pick none of the above. So, I'm going to cross out D, which means that B is left.
(52:52) So, I'm going to say it's B. The change in Y for a one unit change in X. Your final answer going with B. If you go correct, you don't have to come between lecture eight and 14. Eight and 14. Okay, I'll take that. I'm kidding. You cannot come into any lecture. That's Kevin's lectures.
(53:13) But if you get it right, if you get it wrong, you still have to come. Okay. Confident. All right. The correct answer is B. Yeah. All right. Nice meeting you too. All right. So the idea here is that when we have simple linear regression, the interpretation of the beta one coefficient is very straightforward.
(53:37) It's how much the response variable will change if I change my y my x by one value. Nice interpretation because now I know how the y responds to changes in x. Right? So what if X changed by two units? How much will the Y change? Two times beta one. Yep. Good. So however if we in if the number of predictors is very large this kind of interpretation explanation does not is not so convenient. So instead of doing that we're going to do slightly different. You can still do that.
(54:17) But there is one more thing we want to do which is to look at what is called the feature importance which is shown here. Let's see what we have in this particular model. We're looking at vegetation I believe given some predictions predictors. So we have rainfall percentage of irrigation total grow days temperature humidity etc.
(54:41) And what do we plot in these bars is the value of the coefficient after we fit the model. Yeah. Okay. So if we look at that, so you say which one of these predictors is the most important. Anyway, rainfall as expected. Now I'm going to ask another question. If the coefficient of some predictor is zero, what does it mean? Can you speak louder? It's not correlated.
(55:24) In other words, it doesn't matter, right? That predictor is good for nothing for us, right? You can just throw it away, right? Because it doesn't matter if you change your response variable. It does. It has no predicting power. Is not correlated. All mean the same. Now if the predictor value if the coefficient is negative what does it mean? Some of them there in red means is negative.
(55:51) What is the interpretation? We can draw from soil pH. If the soil pH is high means it's going to be the opposite is negative in so 11 it is it gives us the negative effect right now negative effect doesn't always mean uncorrelated it just mean the opposite right so the way we plot it here is that we put them we rank them as the length of the bar positive or negative the absolute but then we plot it with the negative if you See the soil pH is rank here number eight whatever nine but it's because it's negative. So if you put it into the positive because it does affect
(56:34) vegetation but in the opposite in the negative way right. Okay. So this is something we want to do at this point. You may start wondering well it depends on the values on the units right meaning that if I um I don't know daily radiation in which units we have if we change the units this would change right so we need to in some ways make sure that the scale the units are not what we see right shall I repeat that or is that clear yeah okay So in order to avoid that, we need to do something else. What is that something else?
(57:23) You're dying to answer picture scaling. Thank you. You'll be very good student, Chris. All right. And to before we do, we have one more free lecture. Who wants to take it? Oh, he's dying there. He's He was before you. Sorry. Next is you. I think I have one more. And your name? Your Quincy's house, right? No. I don't know. You don't know.
(57:53) See? All right. Hi, I'm Jordan. I'm a junior in Quincy House. Just like everyone else who volunteers. Okay. Raise your hand. Say hi if you're from Quincy House. It's quite a lot. Maybe we should move the lecture in the Quincy House. All right. Go Jordan. Jordan. Yeah. Yeah. Go ahead. Except you want me to say something fun a fun fact? Yeah. Okay.
(58:17) I also like basketball just like a nin but my favorite player is Jokic instead. Not Yani instead of SGA. Uhhuh. I wanted to boo you too. Anyway, yeah. Let's not get into All right. So the options we have first the question in a multiple linear regression. How does the scaling the predictor variables affect the interpretation of feature important based on the beta coefficients? Okay, so we're talking about interpretation.
(58:47) Number one, scaling the predictors make it easier to directly compare the importance of each feature based on their beta coefficient. Number two, scaling the predictors makes all the features equally important. Scaling the predictors increase the magnitude of better coefficients for less important features and is scaling the predictors eliminate the need for better coefficients for future important.
(59:07) I let you read a little more. By the way, don't say growing up I I was told never to choose D because that's called overfeitting. And we're going to get to you're overfeed later. Okay. So, what do you think Jordan? Um A seems pretty enticing to me. I think that it just puts all the um on a level playing field. So we don't care about the units. Um so that's why we scale them.
(59:35) So you go with a Yeah. I mean it doesn't make them equally important that you know it just scales them to make it so you can compare them is saying um and scaling doesn't increase the magnitude for less important features. It's just based on like the units. has nothing to do with the importance of scaling and then well um let me say I need for beta coefficients for feature importance I guess it it's kind of d as well if I'm interpreting that correctly [Music] we still did right I I just I'm interpret that as saying like we don't care about the absolute values of them exactly but we're scaling there's one correct answer. Okay, then
(1:00:20) I'll go with A. He's going with A. Final answer. Yeah, A is the correct. Don't Don't worry about that thing. PowerPoint works in mysterious ways sometimes. It work perfectly fine on my lap in the morning. Good job, Quincy House. Let's see if the other houses or some of the master students will have up their thing. So, scaling, there were many different ways of scaling.
(1:00:50) Um the first one is we subtract the mean and divide by standard deviation and if you come from physics you call that normalization but in this word we call it standardization or zcore. Okay. So that's what we mean. You just subtract the mean. So there's no mean and we divide by standard deviation. Uh normalization which is all mean max scaling.
(1:01:14) We subtract the mean and divide by range. that guarantees you everything to be between zero and one. So this and the question is makes alg sensitive to future scale performance. But one important thing that Jordan said the results in linear regression will not change if you scale. Why? Because think about if you scale your y scale them linearly you can scale the coefficients the same way. So you get exactly the same answer.
(1:01:45) Okay? So your results in linear regression will not change. You should always get the same MSE same performance no matter of the scale. What will change? Two things. You will change the interpretation of your coefficients. Now you put them on the same scale and we're going to see next month day for polomial regression.
(1:02:05) You have to be careful because you get to some numerical instabilities. But these are the things things has easier interpretation in more depth. Check my notes and examples on STM. I have from last year put it quite extensive when to use one, when to use the other one, examples and everything. Okay, how am I doing with time? Coline I'm going to finish exactly on time. Watch me. Colinearity.
(1:02:33) So what is colinearity? You may have heard is in other words you can call it um correlation between two coefficients. the two coefficients. Think about no delete what I said correlation between two predictors. Imagine you take your data set, you get a one column and you copy it. Now you have an extra predictor. Does it tell you anything more? No. Does it create problems? Yes.
(1:03:04) So let's go through that. So while colinearity doesn't violate the assumptions of linear regression at this point you may say what are the assumptions of linear regression well we'll get to that colinearity will affect our confidence in the estimate coefficients I think let me explain this intuitive if you have two cor two predictors two columns in your thing that tells you exactly the same thing and they're correlated if you change one the other one would change at the same time now you want to put some value how important is that predictor since they
(1:03:40) both change that means that you're going to should get half and half right right so if I change this by and you change half and but this change it will change that by a half okay is that clear to everyone it just confuses the importance of the predictors because if they are correlated they are actually giving the same response to my response variable Okay. So I said we'll discuss assumption of linear regression soon.
(1:04:11) I didn't put it officially. I mean formally but we get to that. We'll discuss confident estimation of the parameters also soon. We'll get to that. For now I'm keeping it a little bit. And here said we delve deeper into the implication of in the context of overfeitting. And I said delve chibiti took over my slides. I don't know who put the delve there but it wasn't me. It's chibi for sure. Okay.
(1:04:35) Um, all right. So, let's talk about colinearity. So, let's imagine we have this data set that we're trying to predict credit card balances. So, my response variable is balance of the credit card. And we have a bunch of predictors. Income, limit, rating, number of cards, age, and education. Okay. So, I'm not doing the models here. I'm just showing you the example.
(1:04:59) Now, what do we plot here? Is scatter. Well, the diagonal is the histogram for income, limit, rating, cards, etc. On all the off diagonal blocks there, what I'm plotting is the scatter plot between the two predictors. Okay, so what you see for example here is the limit and cards. Okay, here rating and balance. Okay, so these are called scatter plots.
(1:05:26) So for every one of these subplots what we plot is the value of one and the value of the other as one though. Is that clear? Yep. Now now if you look here there is this box here the limit and the rating are strongly correlate. Why? Because most likely your rating comes from your limit or the limit comes from your rating.
(1:05:54) maybe the same algorithm that they used to do that okay but those things tells me the same so now if I predict the balance based on all of them they will be a little bit confused so let's do that so what do I have here is the coefficients on a model that I have all of them so here the limit and the rating the limit has a coefficient 02 and the rating has a coefficient 1 0 now when I remove the limit the coefficients of rating jump up that means now if I don't have the limit into my model the rating has gone up is more important because before they both
(1:06:36) contribute to that clear any question about that yes okay So the next thing you see is is is this one. You see the cards here. It was 17 and it became four. That's what you're observing, right? Good observation. Now why? There's two reasons. Now two possible scenarios. One is colinearity can be complex.
(1:07:15) The colinearity can be cascading between one and the other and the other and the other is not may not be as obvious as looking at the correlation the scatter plot and finding it may be because is you know the car depends on something else which depends on something else. Okay. So that's the first options. Okay.
(1:07:35) So he went from 13 from 17 that's what from 17 to four and the question is like why do you change those don't correlated with the others right but it may be some complex colinearity that's option number one what's the other option who said that it was an error I rerun it and it was an error was an error in my code I didn't scale But nevertheless, the observation was very good and you may get this kind of things.
(1:08:12) You may not see the correlation clearly on this on the scatter plots, but it may be a cascading correlation, a more complex correlation. Okay. So again, we're not getting into very many details of colinearity. Few things to remember. Colinearity does not break our assumptions for linear regression, which is we're going to get into next lecture.
(1:08:35) It does change the interpretation of my coefficients. And third, it does screw up our confidence on the values of the coefficient. First one, we're going to explain. Second one, we kind of saw it. The third one, the confidence, we haven't talked about how confident we are about the values we get from beta 0 and beta 1. I'm just going to throw it in there.
(1:09:01) If you remember, beta is xtx inverse. when you have correlated things that inverse kind of gets out of whack. We get to that in much more details. Okay, so remember colinearity is okay for linear regression is going to change the interpretation of your coefficients and it will change the way how confident you are for the vgea.
(1:09:29) All right, finally qualitative predictors may not be able to finish any. So, so far we've been talking about predictors that they have numerical values 3.2 6.7. Okay. So, we have here again the same data set. We're trying to predict the balance of individual given a certain number of predictors. So, income is a continuous variable, right? It has numbers. Uh limit has rating has age has education years of education.
(1:09:57) But if you look at these four here, biological sex, student, marital and ethnicity are not numerical values are categorical values. And what should we do for that? Damicod. Dami cod is that what is dam first of all why use some bad language in my class? Well, I know for R you could just do it like it would be zero or one zero or one variable. Okay, very good.
(1:10:34) For R the same here doesn't matter of the coding but let's go step by step. I'm not going to play the game because it's very easy and actually this is not correct. Uh so the question is if we have a binary variable yes or a no would you put no zero actually and yes one no one guess two no one and guess zero and no n and yes n uh obviously the last one is wrong because then what do I multiply my n with what n is not a number right so any of the other three actually will work we don't like number two but all the others does work. You can put zero or one.
(1:11:14) Never mind. Okay. So I think the idea is the following. So if I have a a binary categorical variable, I set a new variable the value one if it's one category and the value zero if it's the other category. So in this case I put one if the person is a female and zero if the person is a male. Now my model gets very simple. Let's say I only have that predictor.
(1:11:47) What I have is y is equal to the intercept beta 0 plus the coefficient beta 1 times the value of that dummy variable which is going to be 0 and one. So if the if the person is a female x is one. So it's beta 0 plus beta 1. And if the individual is a male x is zero. So the the y is equal to beta 0.
(1:12:13) The beauty of this is that we get some very very nice interpretation of beta 0ero and beta 1 which I think I'm going to close with that game. So anybody wants to play the last game for the day over there. All right. And then u we finish next. By the way, I'm never going to finish on the hour because you all come late. So, I'm gonna steal one minute at them.
(1:12:42) All right. Your name? Hi, I'm Robert. I'm a junior in Cabbat House. I know. What's the name again? Rupert. Rupert. Okay. So, here's what is the interpretation of beta 0 and beta 1. This may need a little thick. Select that. All that apply. I give you a hint. Beta represents the expected value of balance for males.
(1:13:09) Beta 0 represents the difference in balance between males and females. Beta 0 plus beta 1 represent the expected imbalance for female and beta 1 is the average difference in balance between females and males. So given that um X is a dummy variable here that differs between um if the person is female and male, I think that D comes out immediately as um as a kind of intuitive answer D.
(1:13:48) Okay, let's start with the the the the A. So if X if the person is male all the X's will be zero. So my answer for all of them should be beta 0 right? Yes. So beta 0 represent expected value of balance for males right? Yes. So that sounds that sounds yeah good sounds right. All right. So, A seems right.
(1:14:19) B beta represents difference in balance between males and females because the expected value for millies will be beta 0 plus beta 1, right? Okay. So, C seems correct. C seems correct. Okay. So, beta 0 is the difference. Beta 0ero doesn't seem like it could be a difference because it doesn't it doesn't have a different explanation for each case, male or female. Okay.
(1:14:54) Um so B doesn't seem right, but then maybe all of the others could be. So what's your answer? I think A and C. A and C we all agree, right? Yeah. Let's ask the audience, what do you think about B? Answer B. Clap if you think it's correct. Clap if you think it's incorrect. Clap if you have no idea what's up. All right. So we have A, B, and C seems to be all where is.
(1:15:39) So A and C we're sure, right? Well, and if A and C are right, then D also seems like it kind of follows from A and C. Okay. A, C, and D. Do we all agree? Yeah. Chris, do you agree? You lose one. Okay. So the correct is A, C, and D. Um, okay. What's the time? It's 46. Okay, I need to stop here. I only have three slides. Okay, but I see you all very tired.
(1:16:26) We cover them all in uh on By the way, you you're correct. You got your pre-day. Uh we're done for two days. Only three four slides left. We cover it next time a little bit fresher to capture that. Thank you very very much. I'll do my thing and uh I'll see you all next Monday.